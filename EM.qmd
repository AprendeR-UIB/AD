## Análisis Multivariante

En general, los datos que se recogen en experimentos son multidimensionales: miden diversas variables aleatorias sobre una misma muestra de individuos. Esta información se organiza en tablas de datos con filas que representan a los individuos observados y cada columna corresponde a una variable diferente. Lo que hacemos es evaluar un vector de variables aleatorias (que llamaremos **vector aleatorio**) sobre los individuos de una población. 

En esta parte vamos a introducir algunos conceptos sobre vectores de variables aleatorias y tablas multidimensionales de datos cuantitativos.

## Poblaciones: vectores aleatorios

Un **vector aleatorio de dimensión $p$** es un vector formado por $p$ variables aleatorias 
$$
\underline{X}=(X_1,X_2,\ldots,X_p).
$$
Una **realización** de $\underline{X}$ es un vector $(x_1,\ldots,x_p)$ formado por los valores $X_1,\ldots,X_p$ sobre un individuo. 

Una **muestra** de $\underline{X}$ es un conjunto de realizaciones. 

Usualmente, organizamos una muestra de $\underline{X}$ por medio de una tabla de datos con las columnas definidas por las variables $X_1,\ldots,X_p$ y donde cada fila es una realización de estas variables, es decir, un vector formado por los valores de $X_1,\ldots,X_p$ sobre un individuo de la muestra.


[*Ejemplo*]{style="color: blue;"}:
Sea $\textit{BMI}$ la variable aleatoria que registra el índice de masa corporal (BMI) de una persona, $C$ la variable aleatoria que registra el nivel de colesterol en mg/dl de una persona y $E$  la variable aleatoria que da la edad de una persona en años, entonces
$$
\underline{X}=(\textit{BMI},C,E)
$$
es un **vector aleatorio** de dimensión 3. Cada vez que tomamos una persona y anotamos el BMI, el nivel de colesterol y la edad y organizamos estas medidas en este orden en un vector, obtenemos una **realitzación** de este vector aleatorio $\underline{X}$.  Entonces,una **muestra** de $\underline{X}$ será un conjunto de vectores con el BMI, el nivel de colesterol y la edad de un grupo de personas de la población. Por ejemplo 

```{r,echo=FALSE}
library(kableExtra)
```


```{r,echo=FALSE}
kable(data.frame(BMI=c(18.3,24.4,24.6,24.4,22.2,19.5), C=c(170,202,215,218,210,210),E=c(49,39,50,44,40,36)))
```


Sean $\underline{X}=(X_1,X_2,\ldots,X_p)$ un vector aleatorio y,  $\mu_i$ y $\sigma_i$ la media y la desviación típica, respectivamente, de cada $X_i$. 

* El **valor esperado**, o **vector de medias**,  de  $\underline{X}$ es el vector formado por los valores esperados, o medias de sus componentes:
$$
E(\underline{X})=(\mu_1,\ldots,\mu_p).
$$
Para abreviar, a veces indicaremos este vector simplemente con $\boldsymbol\mu$.

* El **vector de varianzas**  de  $\underline{X}$ es el vector formado por las varianzas de sus componentes:
$$
\sigma^2(\underline{X})=(\sigma_1^2,\ldots,\sigma_p^2).
$$

* El **vector de desviaciones típicas**  de  $\underline{X}$ es el vector formado por las desviaciones típicas de sus componentes:
$$
\sigma(\underline{X})=(\sigma_1,\ldots,\sigma_p).
$$

## Covarianza 

La **covarianza** de dos variables $X$ e $Y$ es una medida del comportamiento conjunto de estas dos variables. 

Formalmente, dadas dos variables aleatorias $X,Y$ con medias $\mu_X$ y $\mu_Y$, respectivamente, su **covarianza** es 
$$
\sigma_{X,Y}=E\left((X-\mu_X)\cdot ( Y-\mu_Y)\right).
$$
Es fácil comprobar que
$$
\sigma_{X,Y}=E(X\cdot Y) -\mu_X\cdot \mu_Y.
$$


En efecto,
$$
\begin{array}{rl}
\sigma_{X,Y} & =E((X-\mu_X) ( Y-\mu_Y))=
E(XY-\mu_XY-\mu_YX+\mu_X\mu_Y)\\ &
=E(XY)-\mu_XE(Y)-\mu_YE(X)+\mu_X\mu_Y\\ &=
E(XY)-\mu_X\mu_Y-\mu_Y\mu_X+\mu_X\mu_Y
=E(XY)-\mu_X\mu_Y
\end{array}
$$


La covarianza de  $X$ e $Y$ puede tomar cualquier valor real (no como la varianza, que siempre es positiva), y  mide el grado de variación conjunta de las variables en el sentido sigüiente:

* $\sigma_{X,Y}>0$ significa que cuando $X$ es más grande en un individuo 1 que en un individuo 2, $Y$  tiende  también a ser más gran en el individuo 1 que en el individuo 2.


```{r, echo=FALSE}
set.seed(100)
x=rnorm(50,5,1)
y=x+rnorm(50,0,0.5)
z=8-x+rnorm(50,0,0.5)
set.seed(300)
t=rnorm(50,6,2)
plot(x,y,pch=20,xlab="",ylab="",main="Covarianza > 0",cex=1.5)
```


* $\sigma_{X,Y}<0$ significa que cuando $X$ es más grande en un individuo 1 que en un individuo 2, $Y$ tiende a ser más pequeña en el individuo 1 que en el individuo 2.



```{r, echo=FALSE}
plot(x,z,pch=20,xlab="",ylab="",main="Covarianza < 0",cex=1.5)
```

* $\sigma_{X,Y}=0$ significa que no hay ninguna tendencia en este sentido.


```{r, echo=FALSE}
plot(x,t,pch=20,xlab="",ylab="",main="Covarianza = 0",cex=1.5)
par(mfrow=c(1,1))
```



El signo de la covarianza  refleja la "tendencia del crecimiento conjunto" de las variables:

* Covarianza positiva significa la misma tendencia: Si $X$ aumenta, $Y$ tiende a aumentar. Esto suele expresarse diciendo que hay **asociación positiva** entre $X$ e $Y$.

* Covarianza negativa significa la tendencia inversa: Si $X$ aumenta, $Y$ tiende a disminuir.  Esto suele expresarse diciendo que hay **asociación negativa** entre $X$ e $Y$.


 
**Si $X$ e $Y$ son variables independientes, su covarianza es 0**, porque en este caso $E(X\cdot Y) =\mu_X\mu_Y$ y por tanto
$$
\sigma_{X,Y}=E(X\cdot Y) -\mu_X\cdot \mu_Y=\mu_X\cdot \mu_Y-\mu_X\cdot \mu_Y=0.
$$
Intuitivamente, si $X$ e $Y$ son independientes, significa que el hecho de que el valor de $X$ aumente de un individuo a otro no tiene ningún efecto sobre el valor de $Y$.


¿Por qué, si $X$ e $Y$ son independientes, $E(X\cdot Y) =\mu_X\mu_Y$? 

Os lo demostraremos en el caso discreto; el argumento en el caso continuo es lo mismo cambiando sumatorios por integrales.
$$
\begin{array}{rl}
E(X\cdot Y)\!\!\! &\displaystyle =\sum_{x\in D_X,y\in D_Y} xyP(X=x,Y=y)\\
&\displaystyle =\sum_{x\in D_X,y\in D_Y} xyP(X=x)P(Y=y)\\
&\text{(por la independencia de $X$ e $Y$)}\\
&\displaystyle =\Big(\sum_{x\in D_X}xP(X=x)\Big)\Big(\sum_{y\in D_Y} yP(Y=y)\Big)=E(X)E(Y)
\end{array}
$$


Es importante remarcar que la igualdad $E(X\cdot Y) =\mu_X\mu_Y$ es equivalente a la igualdad $\sigma(X+Y)^2 =\sigma(X)^2+\sigma(Y)^2$ que decíamos que satisfacen las variables independientes. En efecto
$$
\begin{array}{l}
\sigma(X+Y)^2 -(\sigma(X)^2+\sigma(Y)^2)\\
\quad = E((X+Y)^2)-E(X+Y)^2-(E(X^2)-E(X)^2+E(Y^2)-E(Y)^2)\\
\quad = E(X^2+2XY+Y^2)-(E(X)+E(Y))^2\\
\qquad\qquad -E(X^2)+E(X)^2-E(Y^2)+E(Y)^2\\
\quad = E(X^2)+2E(XY)+E(Y^2)-E(X)^2-2E(X)E(Y)-E(Y)^2\\
\qquad\qquad -E(X^2)+E(X)^2-E(Y^2)+E(Y)^2\\
\quad = 2E(XY)-2E(X)E(Y)=2(E(XY)-\mu_X\mu_Y)
\end{array}
$$
y por tanto
$$
\sigma^2(X+Y) -(\sigma^2(X)+\sigma^2(Y))=0 \Longleftrightarrow E(XY)-\mu_X\mu_Y=0
$$




**Si $X$ e $Y$ son variables independendientes, su covarianza es 0**, pero la implicación al contrario es falsa: **Dos variables aleatorias pueden tener covarianza 0 y no ser independientes**.

Veamos un ejemplo de este hecho:




Supongamos que tenemos un dado tetraédrico no trucado con las caras marcadas con los valores -2, -1, 1 y 2. Sean $X$ la variable aleatoria que consiste en llanzar el dado y anotar el resultado (la cara que queda en tierra), e $Y$ la variable aleatoria que consite en lanzar el dado y anotar el *cuadrado* del resultado obtenido. Como las cuatro caras del dado son equiprobables, 
$$
\begin{array}{l}
\displaystyle P(X=-2)=P(X=-1)=P(X=1)=P(X=2)=\frac{1}{4}\\
\displaystyle P(Y=1)=P(Y=4)=\frac{1}{2}
\end{array}
$$
  
  

Comp $Y$ es función de $X$, ya que $Y=X^2$, $X$ e $Y$ no pueden ser independientes. Veamos que, en efecto, no lo son. Observad que los únicos posibles valores para el vector $(X,Y)$ en una tirada del dado son (-2,4), (-1,1), (1,1) y (2,4), cada uno con probabilidad 1/4. Entonces, por ejemplo, la probabilidad de obtener en una tirada $X=-1$ i $Y=4$ es 0, porque es imposible, mientras que
$$
P(X=-1)\cdot P(Y=4)=\frac{1}{4}\cdot\frac{1}{2}=\frac{1}{8}\neq 0.
$$

Veamos ahora que la covarianza de  $X$ e $Y$ es 0. Para calcularla, primero necesitamos calcular los valores esperados de las variables:
$$
\begin{array}{l}
\displaystyle \mu_X=(-2)\cdot \frac{1}{4}+(-1)\cdot \frac{1}{4}+1\cdot \frac{1}{4}+2\cdot \frac{1}{4}=0\\
\displaystyle \mu_Y=1\cdot \frac{1}{2}+4\cdot \frac{1}{2}=2.5
\end{array}
$$
Por tanto
$$
\begin{array}{l}
\sigma_{X,Y}=E\big(X\cdot Y\big)-\mu_X\cdot \mu_Y=E\big(X\cdot Y\big)-0\cdot 2.5=E\big(X\cdot Y\big)\\
\qquad =P\big(X=-2,Y=4\big)\cdot (-2\cdot 4)+P\big(X=-1,Y=1\big)\cdot (-1\cdot 1)\\
\qquad\qquad\qquad +P\big(X=1,Y=1\big)\cdot (1\cdot 1)+P\big(X=2,Y=4\big)\cdot (2\cdot 4)\\
\qquad =\displaystyle \frac{1}{4}\cdot (-8)+\frac{1}{4}\cdot (-1)+\frac{1}{4}\cdot 1+\frac{1}{4}\cdot 8=0.
\end{array}
$$
Así pues, $X$ e $Y$ son variables dependientes, pero su covarianza es 0.

Dos propiedades importantes más de la covarianza:

* La covarianza es simétrica: 
$$
\begin{array}{rl}
\sigma_{X,Y}\!\!\! & =E((X-\mu_X)\cdot ( Y-\mu_Y))\\
& =E(( Y-\mu_Y)\cdot (X-\mu_X))=\sigma_{Y,X}
\end{array}
$$

* La covarianza de una variable aleatoria con ella misma es su varianza:
$$
\sigma_{X,X}=E((X-\mu_X)^2)=\sigma^2(X)
$$

La **matriz de covarianzas** de un vector aleatorio $\underline{X}=(X_1,\ldots,X_p)$ es la matriz formada por las covarianzas de los pares de variables que la formen:
$$
\sigma_{\underline{X},\underline{X}}=\begin{pmatrix} \sigma_{X_1,X_1} & \sigma_{X_1,X_2} & \ldots & \sigma_{X_1,X_p}\\
\sigma_{X_2,X_1} & \sigma_{X_2,X_2} & \ldots & \sigma_{X_2,X_p}\\
\vdots & \vdots &\ddots  & \vdots\\
\sigma_{X_p,X_1} & \sigma_{X_p,X_2} & \ldots & \sigma_{X_p,X_p}\\
 \end{pmatrix}
$$

Esta matriz es simétrica y las entradas de la diagonal son las varianzas de las variables del vector, porque $\sigma_{X_i,X_i}=\sigma^2_{X_i}$.

## Correlación

Como hemos dicho, el signo de la covarianza tiene una interpretación sencilla, puesto que refleja la tendencia del crecimiento conjunto de las variables. Pero, su magnitud no tiene una interpretación sencilla. 

Como alternativa, se puede medir la tendencia de que haya una *relación lineal* entre dos variables aleatorias **continuas** empleando el llamado *coeficiente de correlación lineal de Pearson* (o, para abreviar, la **correlación**), que viene a ser una versión normalizada de la covarianza. 

En concreto, la **correlación** de las variables $X$ e $Y$ se define como el cociente de su covarianza entre el producto de sus desviaciones típicas:
$$
\rho_{X,Y}=\frac{\sigma_{X,Y}}{\sigma_{X} \sigma_{Y}}
$$

La correlación tiene las propiedades importantes siguientes:

1. No tiene unidades (porque las unidades de $\sigma_X$ son las de $X$, las unidades de $\sigma_Y$ son las de $Y$, y las unidades de $\sigma_{X,Y}$ son las de $X$ por las de $Y$)

2. Toma valores entre -1 y 1: $-1\leqslant \rho_{X,Y}\leqslant 1$

3. Es simétrica, $\rho_{X,Y}= \rho_{Y,X}$

4. La correlación de una variable con ella misma es 1: $\rho_{X,X}=1$

5. $\rho_{X,Y}=\pm 1$ si, y solo si, las variables $X,Y$ tienen una *relación lineal perfecta*. Es decir, $\rho_{X,Y}=\pm 1$ si, y solo si, existen $a,b\in \mathbb{R}$ con $a\neq 0$ y tales que $Y= X+b$. La pendiente $a$ de esta recta tiene el mismo signo que $\rho_{X,Y}$.

6. Cuanto más se acerca $|\rho_{X,Y}|$ a 1, más se acerca $Y$ a ser función lineal de $X$.

* Si $\rho_{X,Y}>0$, la función es creciente
* Si $\rho_{X,Y}<0$, la función es decreciente

6. Si $\rho_{X,Y}=0$, decimos que las variables $X$ e $Y$ son **incorreladas**. Notamos que la correlación es 0 si, y solo si, la covarianza es 0. Por lo tanto, *si $X$ y $Y$ son independientes, también son incorreladas. El recíproco en general es falso*.



Si una de las dos variables tiene desviación típica 0, en la fórmula de la correlación aparece un 0 en el denominador y no la podemos calcular. En este caso, se toma $\rho_{X,Y}=0$. El motivo intuitivo es que una variable constante es siempre independiente de cualquier otra variable (haga lo que haga la otra, siempre toma el mismo valor) y hemos quedado que las variables independientes son incorreladas.


La *matriz de correlaciones* de un vector aleatorio $\underline{X}=(X_1,\ldots,X_p)$ es la matriz formada por las correlaciones de pares de sus variables:
$$
\rho(\underline{X})
=\begin{pmatrix} 1 & \rho_{X_1,X_2} & \ldots & \rho_{X_1,X_p}\\
\rho_{X_2,X_1} & 1 & \ldots & \rho_{X_2,X_p}\\
\vdots & \vdots & \ddots & \vdots\\
\rho_{X_p,X_1} & \rho_{X_p,X_2} & \ldots & 1\\
 \end{pmatrix}.
$$
Esta matriz es simétrica por la simetría de la correlación.


La correlación de Pearson de dos variables continuas mide la tendencia de las variables a variar conjuntamente de manera lineal. En particular, por ejemplo, si $\rho_{X,Y}>0$, $Y$ tiende a crecer cuando $X$ crece. Pero esto no significa que un aumento del valor de $X$ cause que el valor de $Y$ tienda a aumentar:

> **Correlación no implica causalidad!** 

La tendencia al crecimiento simultáneo de $X$ y $Y$ se puede deber a una tercera variable que las haga crecer las dos, o puede ser puramente espuria.




Si entráis a la página web [*Spurious Correlations*](https://tylervigen.com/spurious-correlations) podréis explorar un montón de correlaciones espurias. Nuestra preferida es una correlación de 0.947 entre la variable $X$= "Tomo un año y anoto el consumo per capita* de queso en los EE. UU." e $Y$= "Tomo un año y anoto el número de muertes por estrangulamiento accidental con las sábanas de la cama en los EE. UU.".

```{r, echo=FALSE, out.width="80%", fig.align='center'}
knitr::include_graphics("Figuras/spurcorr1.png")
```

Hay un ejemplo de correlación negativa que es importante tener presente para no dejarse engañar.

**Teorema**:
Si $X_1$, $X_2$  son dos copias independientes de una misma variable aleatoria $X$,
$$
\rho_{X_1,X_2-X_1}=-\frac{1}{\sqrt{2}}\approx -0.71
$$


Esto nos dice que, sea cual sea la variable $X$, si la medimos en dos momentos independientes o sobre dos individuos elegidos de manera independiente, la diferencia entre los dos valores *tiene una tendencia destacada a decrecer linealmente* en el primer valor. Por ejemplo:

* Hacéis un test y sacáis una nota muy baja ($X_1$). El día siguiente hacéis otro test similar ($X_2$) sin haber estudiado más. Lo más probable es que, por puro azar, saques una nota más alta (que $X_2-X_1$ sea grande, por lo tanto positivo).

* Hacéis un test y sacáis una nota muy alta ($X_1$). El día siguiente hacéis otro test ($X_2$) sin haber estudiado más. Lo más probable es que, por puro azar, saques una nota más baja (que $X_2-X_1$ sea pequeño, por lo tanto negativo).

Por si alguno necesita una demostración del teorema anterior, recordamos que
$$
\rho_{X_1,X_2-X_1}=\dfrac{\sigma_{X_1,X_2-X_1}}{\sigma_{X_1}\sigma_{X_2-X_1}}
$$
Ahora
$$
\begin{array}{l}
\sigma_{X_2-X_1}=\sqrt{\sigma^2_{X_2-X_1}}\\[2ex]
\quad =\sqrt{\sigma^2_{X_1}+\sigma^2_{X_2}}\ \text{(porque son independientes)}\\[2ex]
\quad =\sqrt{\sigma^2_{X}+\sigma^2_{X}}\ \text{(porque $X_1,X_2$ son copias de $X$)}\\[2ex]
\quad =\sqrt{2\sigma^2_{X}}=\sigma_{X}\sqrt{2}
\end{array}
$$
Luego,
$$
\begin{array}{l}
\sigma_{X_1,X_2-X_1}=E(X_1(X_2-X_1))-E(X_1)E(X_2-X_1)\\[1ex]
\quad =E(X_1X_2-X_1^2)-E(X_1)(E(X_2-E(X_1))\\[1ex]
\quad =E(X_1X_2)-E(X_1^2)-E(X_1)E(X_2)+E(X_1)E(X_1)\\[1ex]
\quad =E(X_1)E(X_2)-E(X_1^2)-E(X_1)E(X_2)+E(X_1)E(X_1)\\[1ex]
\quad \text{(porque  $X_1,X_2$ son independents)}\\[1ex]
\quad =-E(X_1^2)+E(X_1)E(X_1)=-\sigma^2_{X_1}=-\sigma^2_{X}
\end{array}
$$

Combinando lo anterior:
$$
\rho_{X_1,X_2-X_1}=\dfrac{\sigma_{X_1,X_2-X_1}}{\sigma_{X_1}\sigma_{X_2-X_1}}=\dfrac{-\sigma^2_{X}}{\sigma_{X}\cdot \sigma_{X}\sqrt{2}} =-\frac{1}{\sqrt{2}}
$$


Como ejemplo, generaremos una muestra $X$ de 101 "notas" aleatorias entre 0 y 100 con distribución binomial $B(100,0.5)$. Tomaremos como $X_1$ el vector de las primeras 100 notas,
$$
X_1=(x_1,x_2,\ldots,x_{100})
$$
y como $X_2-X_1$ el vector de las diferencias de cada nota $x_y$, $y\geqslant 2$, con la anterior:
$$
X_2-X_1=(x_2-x_1,x_3-x_2,\ldots,x_{101}-x_{100}).
$$
Calcularemos la correlación entre $X_1$ y $X_2-X_1$, y lo ilustraremos con un gráfico.



```{r,echo=FALSE}
set.seed(42)
```
```{r}
X=rbinom(101,100,0.5)
X1=X[-101]
X2.menos.X1=diff(X)
plot(X1,X2.menos.X1,pch=20,xlab=expression(X[1]),
     ylab=expression(X[2]-X[1]))
cor(X1,X2.menos.X1)
```
La correlación predicha por el teorema anterior es
```{r}
-1/sqrt(2) 
```

## Estadística descriptiva: Muestras

### Covarianza

Sean $X=(x_1,\ldots,x_n)$ y $Y=(y_1,\ldots,y_n)$ dos vectores obtenidos de mediciones de dos variables aleatorias cuantitativas sobre una misma muestra ordenada de individuos de tamño $n$ de una población. Sean $\overline{X}$ y $\overline{Y}$ sus medias muestrales. Entonces su *covarianza muestral* es
$$
\widetilde{S}_{X,Y} =\frac{1}{n-1} \sum_{i =1}^n\big((x_{i}-\overline{{X}})(y_i-\overline{Y})\big)
$$
y su **covarianza** (**a secas**) es
$$
{S}_{X,Y} =\frac{1}{n} \sum_{i =1}^n\big((x_{i}-\overline{{X}})(y_i-\overline{Y})\big)=\frac{n-1}{n}\widetilde{S}_{X,Y}.
$$
Es decir, la diferencia entre la versión "muestral" y la versión "a secas" recae en el denominador, $n-1$ y $n$ respectivamente.


> La covarianza de dos vectores solo tiene sentido cuando estos vectores representan los valores de dos variables cuantitativas sobre los mismos individuos, o sobre dos muestras emparejadas, y en el mismo orden. En particular, los dos vectores tienen que tener la misma longitud.


Como en el caso poblacional, la **covarianza** entre dos vectores mide la tendencia que tienen los datos a variar conjuntamente:


* Cuando $\widetilde{S}_{X,Y}>0$, si $x_i>x_j$ entonces $y_i$ tiende a ser más grande que $y_j$

* Cuando $\widetilde{S}_{X,Y}<0$, si $x_i>x_j$ entonces $y_i$ tiende a ser más pequeño que $y_j$

* Cuando $\widetilde{S}=0$, no hay ninguna tendencia en este sentido


Es fácil comprobar que:

* Las dos covarianzas son simétricas
$$
\widetilde{S}_{X,Y}=\widetilde{S}_{Y,X},\ {S}_{X,Y}={S}_{Y,X}
$$
* La varianza de un vector es su covarianza con él mismo
$$
\widetilde{S}_{X,X}=\widetilde{S}^2_{X},\ {S}_{X,X}={S}^2_{X}.
$$


[*Ejemplo*]{style="color: blue;"}

Hemos medido el índice de masa corporal, BMI, y el nivel de colesterol en 5 individuos sanos. Guardamos los resultados en un *dataframe* y calculamos las medias:


```{r}
BMI= c(18.3,24.4,24.6,24.4,22.2,19.5)
Chol=c(170,202,215,218,210,210)
DF=data.frame(BMI,Chol)
mean(BMI)
mean(Chol)
```


Entonces la covarianza muestral de estos dos vectores es
$$
\begin{array}{l}
\dfrac{1}{5}\Big((18.3-22.23)(170-204.17)+(24.4-22.23)(202-204.17)\\
\qquad +(24.6-22.23)(215-204.17)+(24.4-22.23)(218-204.17)\\
\qquad +(22.2-22.23)(210-204.17)+(19.5-22.23)(210-204.17)\Big)=`r round(cov(BMI,Chol),4)`
\end{array}
$$

Entonces la covarianza muestral de estos dos vectores es
$$
\begin{array}{l}
\dfrac{1}{6}\Big((18.3-22.23)(170-204.17)+(24.4-22.23)(202-204.17)\\
\qquad +(24.6-22.23)(215-204.17) +(24.4-22.23)(218-204.17)\\
\qquad +(22.2-22.23)(210-204.17)+(19.5-22.23)(210-204.17)\Big)=`r round(5*cov(BMI,Chol)/6,4)`
\end{array}
$$



La covarianza **muestral** de dos vectores numéricos de la misma longitud $n$ se calcula en R con la función `cov`.

```{r}
cov(BMI,Chol)
```

Para obtener su covarianza a secas, hay que multiplicar el resultado de `cov` por $(n-1)/n$.

```{r}
n=length(BMI)
cov(BMI,Chol)*(n-1)/n
```

Consideramos una tabla de datos cuantitativos de la forma
$$
\begin{array}{cccc}
X_1 & X_2 & \ldots & X_p\\ \hline
x_{1 1} & x_{1 2} &\ldots & x_{1 p}\\
x_{2 1} & x_{2 2} &\ldots & x_{2 p}\\
\vdots & \vdots   &   \ddots    &\vdots\\ 
x_{n 1} & x_{n 2} &\ldots & x_{n p}
\end{array}
$$
donde cada columna representa los valores de cierta variable $X_i$ y cada fila un individuo de una muestra de la población, de forma que la entrada $x_{ij}$ de esta tabla es el valor de $X_j$ sobre el individuo $i$-ésimo de la muestra.

La *matriz de covarianzas muestrales* de esta tabla es la matriz
$$
\widetilde{{S}}=
\begin{pmatrix}  
 \widetilde{S}^2_{X_1} & \widetilde{S}_{X_1,X_2} & \ldots & \widetilde{S}_{X_1,X_p}\\
 \widetilde{S}_{X_2,X_1} &  \widetilde{S}^2_{X_2} & \ldots & \widetilde{S}_{X_2,X_p}\\
  \vdots & \vdots  &  \ddots      & \vdots\\
\widetilde{S}_{X_p,X_1} & \widetilde{S}_{X_p,X_2} & \ldots &  \widetilde{S}^2_{X_p}
\end{pmatrix}
$$
y la *matriz de covarianzas* (*a secas*) se define de manera similar, pero con las covarianzas a secas:
$$
{S}=
\begin{pmatrix}  
 S^2_{X_1} & S_{X_1,X_2} & \ldots & S_{X_1,X_p}\\
 S_{X_2,X_1} & S^2_{X_2} & \ldots & S_{X_2,X_p}\\
  \vdots & \vdots  &  \ddots      & \vdots\\
S_{X_p,X_1} & S_{X_p,X_2} & \ldots & S^2_{X_p}
\end{pmatrix}
$$
Las dos son simétricas.


La matriz de covarianzas muestrales se calcula con la función `cov` aplicada a la matriz o al data frame de variables numéricas que almacena la tabla de datos. Para calcular la matriz de covarianzas a secas, se multiplica el resultado de `cov` por $(n-1)/n$, donde $n$ es el número de filas de la tabla.

[*Ejemplo*]{style="color: blue;"}

Añadiremos a los datos del ejemplo anterior una tercera variable con las edades de los 5 individuos.



```{r}
DF$Edad=c(49,39,50,44,40,36)
DF
```

La matriz de covarianzas muestrales de esta tabla es
```{r}
cov(DF)
```

Podréis observar que es simétrica, que la entrada (2,1) coincide con la covarianza de `BMI` y `Chol` que hemos calculado antes, y que en la diagonal obtenemos las varianzas muestrales de las variables de la tabla:
```{r}
apply(DF,MARGIN=2,FUN=var)
```

### Correlación de Pearson

Sean $X=(x_1,\ldots,x_n)$ e $Y=(y_1,\ldots,y_n)$ dos vectores obtenidos midiendo dos variables aleatorias continuas sobre una misma muestra de individuos de medida $n$ de una población.

La *correlación de Pearson* de $X$ e $Y$ es su covarianza muestral dividida por el producto de sus desviaciones típicas muestrales:
$$
R_{X,Y}=\frac{\widetilde{S}_{X,Y}}{\widetilde{S}_X\cdot \widetilde{S}_Y}.
$$

La *correlación de Pearson* de $X$ e $Y$ también es igual a su covarianza a secas dividida por el producto de sus desviaciones típicas a secas, porque los cambios de denominador se cancelan:
$$
R_{X,Y}=\frac{\widetilde{S}_{X,Y}}{\widetilde{S}_X\cdot \widetilde{S}_Y}=
\frac{\frac{n}{n-1}\cdot {S}_{X,Y}}{\sqrt{\frac{n}{n-1}}\cdot {S}_X \cdot\sqrt{\frac{n}{n-1}}\cdot{S}_Y}=
\frac{S_{X,Y}}{S_X \cdot S_Y}=R_{X,Y}.
$$

[*Ejemplo*]{style="color: blue;"}

Volvemos a la situación del ejemplo anterior. La covarianza muestral y las desviaciones típicas muestrales de los vectores `BMI` y `Chol` son
```{r}
cov(BMI,Chol)
sd(BMI)
sd(Chol)
```
y por tanto su correlación de Pearson es
$$
R_{BMI,Chol}=\frac{`r round(cov(BMI,Chol),3)`}{`r round(sd(BMI),3)`\cdot `r round(sd(Chol),3)`}= `r round(round(cov(BMI,Chol),3)/(round(sd(BMI),3)* round(sd(Chol),3)),3)`
$$


Algunas propiedades importantes de la correlación de Pearson:

* La correlación de Pearson es simétrica:
$$
R_{X,Y}=R_{Y,X}
$$

* La correlación de Pearson toma valores solo entre -1 y 1:
$$
-1\leqslant R_{X,Y}\leqslant 1
$$

* La correlación de Pearson de un vector con él mismo es 1:
$$
R_{X,X}=1
$$

* $R_{X,Y}$ tiene el mismo signo que $S_{X,Y}$, y por tanto este signo tiene el mismo significado que a la covarianza:

* Si $R_{X,Y}>0$ y si $x_i>x_j$, $y_i$ tiende a ser más grande que $y_j$

* Si $R_{X,Y}<0$ y si $x_i>x_j$, $y_i$ tiende a ser más pequeño que $y_j$

* Si $R=0$, no hay ninguna tendencia en este sentido


* $R_{X,Y}=\pm 1$ si, y solo si, todos los puntos $(x_i,y_i)$ están sobre una recta $y=ax+b$ con $a\neq 0$. La pendiente $a$ de esta relación lineal tiene el mismo signo que $R_{X,Y}$. Por lo tanto, la recta es creciente si $R_{X,Y}=1$ y decreciendo si $R_{X,Y}=- 1$.

* El coeficiente de determinación $R^2$ de la regresión lineal por mínimos cuadrados de $Y$ respecto de $X$ es igual al cuadrado de su correlación de Pearson:
$$
R^2=R_{X,Y}^2
$$


>Por lo tanto, cuanto más se acerca la correlación de Pearson de $X$ e $Y$ a 1 o a -1, más se acercan los puntos $(x_i,y_i)$ a estar sobre una recta. El signo de $R_{X,Y}$ indica si esta recta es creciente ($R_{X,Y}>0$) o decreciendo ($R_{X,Y}<0$).


Como en el caso poblacional, cuando uno de los vectores es constante, la correlación es igual a 0.



Con R, la correlación de Pearson de dos vectores se puede calcular con la función `cor`. Por ejemplo, la correlación del Pearson de los vectores `BMI` y `Chol` se obtiene con
```{r}
cor(BMI,Chol)
```


Veamos que su cuadrado es igual al $R^2$ de la regresión lineal de `Chol` en función de `BMI`:
```{r}
cor(BMI,Chol)^2
summary(lm(Chol~BMI))$r.squared
```

Para hacernos una idea de qué representa este valor de la correlación, veamos el gráfico de los puntos (BMI,Chol) con su recta de regresión lineal:

```{r}
plot(BMI,Chol,pch=20)
abline(lm(Chol~BMI),col="red",lwd=1.5)
```

Podemos observar como `Chol` tiende a crecer cuando `BMI` crece, pero los puntos (BMI,Chol) no tienden a estar sobre una recta.

[*Cuidado*]{style="color: red;"}
Es conveniente acompañar el cálculo de correlación o covarianza de dos vectores $x,y$ con un gráfico de los puntos $(x_i,y_i)$, porque conjuntos muy diferentes de puntos pueden dar lugar a la misma correlación.


Un ejemplo clásico de este hecho son los cuatro conjuntos de datos $(x_{1,i},y_{1,i})_{i=1,\ldots,11}$, $(x_{2,i},y_{2,i})_{i=1,\ldots,11}$,  $(x_{3,i},y_{3,i})_{i=1,\ldots,11}$,  $(x_{4,i},y_{4,i})_{i=1,\ldots,11}$ que forman el *dataframe* `anscombe` de R:

```{r}
str(anscombe)
```

Las correlaciones de los cuatro pares de vectores son muy parecidas:

```{r}
cor(anscombe$x1,anscombe$y1)
cor(anscombe$x2,anscombe$y2)
cor(anscombe$x3,anscombe$y3)
cor(anscombe$x4,anscombe$y4)
```


Pero si los dibujamos veréis que los cuatro conjuntos de puntos son muy diferentes:

```{r,eval=FALSE}
plot(anscombe$x1,anscombe$y1,pch=20,main="Conjunto de datos 1",cex=1.25)
abline(lm(y1~x1,data=anscombe),col="red",lwd=1.5)
plot(anscombe$x2,anscombe$y2,pch=20,main="Conjunto de datos 2",cex=1.25)
abline(lm(y2~x2,data=anscombe),col="red",lwd=1.5)
plot(anscombe$x3,anscombe$y3,pch=20,main="Conjunto de datos 3",cex=1.25)
abline(lm(y3~x3,data=anscombe),col="red",lwd=1.5)
plot(anscombe$x4,anscombe$y4,pch=20,main="Conjunto de datos 4",cex=1.25)
abline(lm(y4~x4,data=anscombe),col="red",lwd=1.5)
```
```{r,echo=FALSE,fig.width=10,out.width="90%"}
par(mfrow=c(2,2))
plot(anscombe$x1,anscombe$y1,pch=20,main="Conjunto de datos 1",cex=1.25)
abline(lm(y1~x1,data=anscombe),col="red",lwd=1.5)
plot(anscombe$x2,anscombe$y2,pch=20,main="Conjunto de datos 2",cex=1.25)
abline(lm(y2~x2,data=anscombe),col="red",lwd=1.5)
plot(anscombe$x3,anscombe$y3,pch=20,main="Conjunto de datos 3",cex=1.25)
abline(lm(y3~x3,data=anscombe),col="red",lwd=1.5)
plot(anscombe$x4,anscombe$y4,pch=20,main="Conjunto de datos 4",cex=1.25)
abline(lm(y4~x4,data=anscombe),col="red",lwd=1.5)
par(mfrow=c(1,1))
```

Ejemplos más espectaculares se pueden obtener con las funciones del paquete **datasaurus**, que permiten crear conjuntos de puntos de "formas" diferentes con los mismos estadísticos, y en particular la misma correlación. Empleando este paquete, hemos creado dos pares de vectores de datos `dino` y `star`, que hemos recogido en la siguiente [tabla de datos](https://github.com/AprendeR-UIB/MatesII/blob/master/Dades/Datasaurus.txt). Esta tabla de datos tiene tres variables: una variable `dataset` que indica el conjunto de datos, y las variables `x` e `y` que dan las coordenadas de los puntos que forman cada conjunto de datos. Comprobaremos que los dos pares de vectores de datos tienen el mismo coeficiente de correlación (al menos hasta la séptima cifra decimal) y los dibujaremos.


```{r}
datasaure=read.table("https://raw.githubusercontent.com/AprendeR-UIB/MatesII/master/Dades/Datasaurus.txt",header=TRUE,sep="\t")
str(datasaure)
dino=datasaure[datasaure$dataset=="dino",2:3]
star=datasaure[datasaure$dataset=="star",2:3]
cor(dino$x,dino$y)
cor(star$x,star$y)
```

```{r,eval=FALSE}
plot(dino,pch=20,cex=1.25)
plot(star,pch=20,cex=1.25)
```

```{r,echo=FALSE,fig.width=10,out.width="90%",fig.asp=0.5}
par(mfrow=c(1,2))
plot(dino,pch=20,cex=1.25)
plot(star,pch=20,cex=1.25)
par(mfrow=c(1,1))
```


Ahora supongamos que tenemos una tabla de datos numéricos de la forma
$$
\begin{array}{cccc}
X_1 & X_2 & \ldots & X_p\\ \hline
x_{1 1} & x_{1 2} &\ldots & x_{1 p}\\
x_{2 1} & x_{2 2} &\ldots & x_{2 p}\\
\vdots & \vdots   &   \ddots    &\vdots\\ 
x_{n 1} & x_{n 2} &\ldots & x_{n p}
\end{array}
$$

donde cada columna representa los valores de cierta variable $X_i$ y cada fila un individuo de una muestra de la población, de forma que la entrada $x_{ij}$ de esta tabla es el valor de $X_j$ sobre el individuo $i$-ésimo de la muestra.

Su *matriz de correlaciones de Pearson* es la matriz simétrica
$$
\begin{pmatrix}  
 1 & R_{X_1,X_2} & \ldots & R_{X_1,X_p}\\
 R_{X_2,X_1} & 1 & \ldots & R_{X_2,X_p}\\
  \vdots & \vdots  &  \ddots      & \vdots\\
R_{X_p,X_1} & R_{X_p,X_2} & \ldots & 1
\end{pmatrix}
$$


Esta matriz de correlaciones se calcula con la función `cor` aplicada a la matriz o el data frame de variables numéricas que almacena la tabla de datos. Por ejemplo, la matriz de correlaciones de Pearson de la tabla de datos `DF` del Ejemplo que venimos trabajando es

```{r}
cor(DF)
```

### Estimación

Las covarianzas de dos vectores obtenidos midiendo dos variables $X,Y$ sobre una muestra aleatoria simple de sujetos de una población estiman la covarianza poblacional de las variables $X,Y$ que han producido los vectores:

* La covarianza muestral $\widetilde{S}_{X,Y}$ siempre es un estimador no sesgado de la covarianza poblacional $\sigma_{X,Y}$

* La covarianza ${S}_{X,Y}=\frac{n-1}{n}\widetilde{S}_{X,Y}$ es un estimador sesgado de la covarianza poblacional $\sigma_{X,Y}$, con sesgo que tiende a 0, y es más eficiente que $\widetilde{S}_{X,Y}$

* La covarianza $S_{X,Y}$ es el estimador máximo verosímil de $\sigma_{X,Y}$ cuando la distribución conjunta de las variables $X,Y$ es la [normal bivariante](https://es.wikipedia.org/wiki/Distribuci%C3%B3n_normal_multivariada) que estudiaremos próximamente.


La correlación de Pearson de dos vectores obtenidos midiendo dos variables continuas $X,Y$ sobre una muestra aleatoria simple de sujetos de una población estima la correlación poblacional de las variables $X,Y$. En concreto:

* $R_{X,Y}$ es un estimador máximo verosímil de $\rho_{X,Y}$ cuando la distribución conjunta de $X,Y$ se normal bivariante. se un estimador sesgado, pero su sesgo tiende a 0.


### Correlación de Spearman


La correlación de Pearson mide específicamente la tendencia de dos variables continuas a depender linealmente la una de la otra. Si no esperamos que esta dependencia lineal exista, o si nuestras variables son discretas o simplemente ordinales, emplear la correlación de Pearson para analizar la relación entre dos variables no es lo más adecuado. Entre las propuestas alternativas, la más popular es la **correlación de Spearman**.

Intuitivamente, la correlación de Spearman mide la tendencia que si $x_i>x_j$, pase que $y_i>y_j$. Su valor es 1 si, para todos $y,j$,
$$
x_i>x_j\Longleftrightarrow y_i>y_j
$$
y su valor es  -1 si, para todo $i,j$, 
$$
x_i>x_j\Longleftrightarrow y_i<y_j
$$
Cuanto más se acerca la correlación de Spearman a 1 (o -1), para más parejas de índices $(i,j)$ tales que $x_i>x_j$ se tiene que $y_i>y_j$ ($y_i<y_j$, si se acerca a -1).



Formalmente, la correlación de Spearman de dos vectores $X$ e $Y$ se define como la correlación de Pearson de los vectores de **rangos** de $X$ e $Y$. El vector de rangos de un vector $X$ se obtiene sustituyendo cada valor de $X$ por su posición en el vector ordenado de menor a mayor, y en caso de empates asignando a grupos de valores empatados la media de las posiciones que ocuparían. Por ejemplo, el vector de rangos de

$$
x=(4,5,1,5,1,3,4,4)
$$
es
$$
(5,7.5,1.5,7.5,1.5,3,5,5)
$$
¿Cómo hemos calculado este vector?

1. Primero asignamos a cada valor del vector su posición si estuvieran ordenados de menor a mayor, y en caso de empate por ahora los ordenaremos de izquierda a derecha:
$$
\begin{array}{r|cccccccc}
x & 4& 5 & 1 & 5 & 1 & 3 & 4 & 4\\ \hline
\text{Posició} & 4 & 7 & 1 & 8 & 2 & 3 & 5 & 6\\
\end{array}
$$
2. Ahora, para asignar los rangos finales:

* El rango de los dos elementos 1 de $x$ es la media de las posiciones 1, 2 del vector ordenado: 1.5.
* Como que solo hay un 3 en $x$, su rango es su posición en el vector ordenado: 3.
* El rango de los tres elementos 4 es la media de las posiciones 4, 5 y 6 del vector ordenado: 5.
* Finalmente, el rango de los dos elementos 5 es la media de las posiciones 7, 8 del vector ordenado: 7.5.
    
$$
\begin{array}{r|cccccccc}
x & 4& 5 & 1 & 5 & 1 & 3 & 4 & 4\\ \hline
\text{Posició} & 4 & 7 & 1 & 8 & 2 & 3 & 5 & 6\\ \hline
\text{Rang} & 5 & 7.5 & 1.5 & 7.5 & 1.5 & 3 & 5 & 5
\end{array}
$$    


Por cierto, con R el vector de rangos se calcula con la función `rank`:


```{r}
rank(c(4,5,1,5,1,3,4,4))
```



Con R, la correlación de Spearman se calcula directamente con la función `cor` especificando el parámetro `method="spearman"`. (El valor por defecto del parámetro `method` es `"pearson"` y por eso no lo indicamos cuando calculamos la correlación de Pearson.)

[*Ejemplo*]{style="color: blue;"}
Consideremos los vectores `BMI` y `Chol` del ejemplo que venimos trabajando. Lo primero que haremos será calcular sus vectores de rangos:
  
  

$$
\begin{array}{|c|c||c|c|}
\hline
 BMI & \text{Rangs} & Chol&  \text{Rangs}
\\\hline\hline
`r DF[1,1]`& `r rank(DF$BMI)[1]` & `r DF[1,2]`& `r rank(DF$Chol)[1]` \\
`r DF[2,1]`&`r rank(DF$BMI)[2]` & `r DF[2,2]` & `r rank(DF$Chol)[2]`\\
`r DF[3,1]`&`r rank(DF$BMI)[3]` & `r DF[3,2]`& `r rank(DF$Chol)[3]` \\
`r DF[4,1]`&`r rank(DF$BMI)[4]` & `r DF[4,2]`&  `r rank(DF$Chol)[4]`\\ 
`r DF[5,1]`&`r rank(DF$BMI)[5]` & `r DF[5,2]`&  `r rank(DF$Chol)[5]`\\
`r DF[6,1]`&`r rank(DF$BMI)[6]` & `r DF[6,2]`&  `r rank(DF$Chol)[6]`\\\hline
\end{array}
$$


Por tanto, la correlación de Spearman de 
$$
\mathit{BMI}=(`r BMI`)\mbox{ i }\mathit{Chol}=(`r Chol`)
$$
es la correlación de Pearson de
$$
(`r rank(BMI)`)\mbox{ i }(`r rank(Chol)`)
$$

Comprobémoslo:

```{r}
cor(BMI,Chol,method="spearman")
cor(c(1,4.5,6,4.5,3,2),c(1,2,5,6,3.5,3.5))
```


## Contrastes de correlación

En un *contraste de correlación* de dos variables poblacionales continuas $X$ e $Y$, la hipótesis nula es que no hay correlación entre las dos variables, lo cual traduce que no hay ninguna relación entre ellas.
$$
\left\{
\begin{array}{ll}
H_0: & \rho_{XY}=0\\
H_1: & \rho_{XY}> 0\text{ o }\rho_{XY}< 0\text{ o }\rho_{XY}\neq 0
\end{array}\right.
$$


>Si en un contraste de correlación rechazamos la hipótesis nula, en particular concluimos que las variables $X$ e $Y$ son dependientes (porque si fueran independientes, su correlación seria 0).



No explicaremos cómo se hace a mano este contraste ni qué hipótesis tienen que satisfacer las variables poblacionales para que el resultado sea fiable. Si estáis interesados en el detalle, podéis consultar la [correspondiente entrada de la Wikipedia](https://en.wikipedia.org/wiki/pearson_correlation_coefficient#Testing_using_Student's_t-distribution). Simplemente tenéis que saber que se efectúa con la función `cor.plot`. Su sintaxis es similar a la de las otras funciones que efectúan contrastes.

[*Ejemplo*]{style="color: blue;"}
Queremos contrastar si hay correlación positiva entre el BMI y el nivel de colesterol de un adulto sano, con un nivel de significación del 5%.


*Variables poblacionales de interés*:

* $\mathit{BMI}$: "Tomamos un adulto sano y anotamos su BMI"
* $\mathit{Chol}$: "Tomamos un adulto sano y anotamos el nivel de colesterol en mg/l"

*Contrast*:
$$
\left\{
\begin{array}{ll}
H_0: & \rho_{\textit{BMI,Chol}}=0\\
H_1: & \rho_{\textit{BMI,Chol}}>0
\end{array}\right.
$$

Empleamos las muestras de `BMI` y `Chol` del Ejemplo que venimos trabajando.

```{r}
cor.test(BMI,Chol,alternative="greater")
```

*Conclusión*: No hemos obtenido evidencia estadísticamente significativa que el BMI y el nivel de colesterol de un adulto sano tengan correlación positiva (test de correlación, p-valor 0.06, IC para $\rho$ 95%  [-0.086 a 1]).


[*Cuidado*]{style="color: red;"}
La conclusión es que no hemos obtenido evidencia que el BMI y el nivel de colesterol *tengan correlación positiva* **en la población de los adultos sanos**. No en nuestra muestra, que sí que ha dado correlación positiva. Recordad que los contrastes siempre se refieren a la población.



## Gráficos para datos multidimensionales 

### Otro gráfico para datos bivariantes

El boxplot bivariante es un gráfico de dispersión que incluye dos elipses estimadas, la interior que contiene aproximadamente el 50% de los datos y la exterior que contiene aproximadamente el 95% de los datos. Este tipo de gráficos nos ayuda a localizar datos atípicos. Veamos un ejemplo con los datos de los pingüinos.

```{r, message=FALSE}
library(tidyverse)
library(MVA)
library(palmerpenguins)

a2<- penguins %>%
  select(body_mass_g,bill_length_mm) %>%
  na.omit %>% as.matrix()

bvbox(a2,xlab = "Peso del pingüino en gr", 
           ylab = "Longitud del pico en mm",
      pch = 19, cex = 1.25, col = "red")
```

Las dos rectas dentro de las elipses del gráfico anterior son estimaciones de la recta de regresión.La recta más oscura es la habitual de mínimos cuadrados utilizando todas las observaciones. La recta más clara es una estimación más robusta que reduce la influencia de cualquier valor extremo. 


### Matriz de dispersión

Es posible ver los gráficos de dispersión por pares entre diversas variables cuantitativas utilizando una matriz. Se puede generar utilizando la función "pairs()" de R base. Veamos un ejemplo con nuestros datos de pingüinos.

```{r}
a<-penguins %>%
  select(3:7) %>%
  na.omit

pairs(a,
      col = c("red", "blue")[as.integer(a$sex)], 
      pch = 18)
```

Existen muchas otras funciones para crear matrices de gráficos de dispersión. Una que nos gusta es `ggpairs()` del paquete `GGally`. 

```{r, message=FALSE, warning=FALSE}
library(GGally)
ggpairs(a)
```

Por defecto, la diagonal principal de la matriz contiene la curva de densidad para cada variable. Por debajo de la diagonal principal se muestran los gráficos de dispersión y la correlación (para las variables cuantitativas) o el cruce entre variables si se trata de una variable categórica.

### Caras de Chernoff

Cada variable del conjunto de datos se usa para representar una característica de la cara. Chernoff usó hasta 18 variables para representar diferentes rasgos faciales como cabeza, nariz, ojos, cejas, boca y orejas. El gráfico de Chernoff tiene como ventaja la facilidad humana para reconocer patrones de caras. El inconveniente es que la representación es muy dependiente de las variables escogidas para representar cada rasgo. Por ejemplo: la boca y la forma de la cabeza son rasgos más llamativos que las orejas o la longitud de la nariz, por tanto, el mismo conjunto puede sugerir distintos patrones de similitud entre las observaciones. Veamos un ejemplo.


```{r}
library("aplpack")
b<- penguins %>% 
      filter(species=="Adelie", 
             island=="Torgersen",
             sex=="female",
             year==2007)

faces(b[,3:6],face.type = 1, scale =TRUE,print.info = TRUE)
```

Alternativamente, podemos representar cada elemento por una figura geométrica, donde las similitudes entre figuras indican las similitudes entre los elementos.

```{r}
stars(a[,1:4], key.loc = c(44, 1.5),cex=0.45,
      labels=row.names(a[,1:4]), draw.segments=TRUE)
```

Por supuesto que `ggplot2`  nos da opciones más modernas de este tipo de gráficos. Ver, por ejemplo:

* [ggradar](https://www.r-bloggers.com/2016/10/the-grammar-of-graphics-and-radar-charts/)

* [fmsb library](https://www.r-bloggers.com/2023/10/creating-interactive-radar-charts-in-r-with-the-fmsb-library/)


### Matriz de correlaciones

Las correlaciones por pares entre varias variables se muestran visualmente mediante un mapa de calor de la matriz de correlaciones. La función `ggcorrplot()` del paquete "ggcorrplot" de R puede utilizarse para construirlo. A continuación se muestra un ejemplo que utiliza el conjunto de datos de los pingüinos:

```{r}
library(ggcorrplot)
penguins %>%
  select(3:6) %>%
  na.omit(.) %>% 
  cor(.) %>% 
  ggcorrplot(., hc.order = TRUE,
		type = "lower",
		colors = c("#6D9EC1",
					"yellow", "#E46726"))
```


En el gráfico, el color naranja denota correlaciones positivas y el gris correlaciones negativas. Al especificar `hc.order`,las variables también se ordenan.


### Gráficos de mosaico

Hasta ahora, hemos explorado métodos para visualizar relaciones entre variables cuantitativas. Pero, ¿qué hacemos si hay más de dos variables categóricas?

Un método consiste en utilizar gráficos de mosaico, en los que las frecuencias de una tabla de contingencia multidimensional se representan mediante regiones rectangulares anidadas que son proporcionales a su frecuencia de celda. La función `mosaicplot()` de la librería `vcd` proporciona características más amplias que la de R base. 

Si se añade la opción opción `shade=TRUE` colorea la figura basándose en los residuos estandarizados de un modelo loglineal para la tabla. La idea central detrás de un modelo loglineal es modelar las probabilidades de ocurrencia de cada categoría en la tabla de contingencia y el modelo se ajusta utilizando técnicas de regresión logística.


```{r, message=FALSE}
library(vcd)
a<- penguins %>%
  select(island,species,sex) %>%
  na.omit()
a2<- table(a)

mosaicplot(a2,shade=TRUE, main="")
```

## [*Práctica 3* ]{style="color: red;"}


De las fuentes de datos que se citan abajo o de alguna otra fuente que sea de libre acceso, seleccionad un conjunto de datos que os guste y pensad en responder una pregunta a partir de estos. El conjunto de datos debe cumplir con las siguientes condiciones:

  * Que contenga al menos un par de variables cualitativas (una ordinal y otra nominal).
  * Que contenga al menos cinco variables cuantitativas, cuantas más mejor.
  * Si tenéis que unir varias tablas de datos, usad las herramientas que hemos presentado de tidyverse.
  * Algunas fuentes de datos:
    * [Compendium of Data Sources for Data Science, Machine Learning, and Artificial Intelligence](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4567555).
    * [datos.gob.es](https://datos.gob.es/es/catalogo)
    * [IBESTAT](https://ibestat.caib.es/ibestat/inici)
    * [Datasets 2023 Makeover Mondey](https://www.makeovermonday.co.uk/data/)



  1. Cread un repositorio en Github para vuestro grupo con un nombre que sea fácilmente identificable para los profesores de la asignatura. Cread un proyecto nuevo en RStudio conectado al repositorio anterior que contenga los documentos/scripts y presentación del proyecto.
  
2. El proyecto debe incluir:

    * Resumen del problema que trataréis, su contexto y la fuente de datos. Describid cada una de las variables que conforman la base de datos del estudio. Plantead una pregunta u objetivo del proyecto.
  
    * Tibble o dataframe con la base de datos limpia. Indicad los pasos e instrucciones que habéis utilizado para lograr convertir vuestros datos en `tidy data` (si aplica).
  
    * Un análisis multivariante exploratorio de los datos con visualizaciones apropiadas y su interpretación en el contexto del problema.
    
    * La estimación del vector de medias y la matriz de covarianza/correlación de las variables cuantitativas, así como un contraste de correlación. 
    
    * Escribid una conclusión con los resultados importantes del análisis.
    
    * Preparad una presentación de máximo 15 minutos en la que cada miembro del grupo exponga una parte del trabajo.
    
    * Cada grupo tiene 15 minutos para exponer y 5 para responder las dudas y preguntas de los compañeros y de los profesores de la asignatura. La calificación se realizará por medio de la rúbrica publicada en Aula Digital que podéis encontrar en la sección "Bloque I".
    
    * El peso de la evaluación es el siguiente: 0.8 $\cdot$ Nota de la Exposición + 0.2 $\cdot$ Nota del repositorio$. Para calcular la nota de la Exposición, se tomará la calificación media de vuestros compañeros (excluidos los integrantes del propio grupo) y la asignada por los profesores. Si la diferencia de notas entre los profesores y los compañeros es mayor a 2 puntos, contará únicamente la nota de los profesores.
    
## Distancias

Como hemos mencionado, la información multivariante es una matriz de datos de orden $n \times p$, con $n$ el número de filas y $p$ el número de variables consideradas.  A veces, para determinar cuáles observaciones multivariantes son semejantes y cuáles no, se emplean matrices de distancias o similaridades. Dependiendo del contexto, hay que seleccionar la más apropiada para obtener resultados confiables. A continuación, presentamos algunas de las distancias más utilizadas.


### Distancia euclídea

La distancia euclídea entre dos realizaciones $\underline{X}=(x_1,\ldots,x_p)$ e $\underline{Y}=(y_1,\ldots,y_p)$ se define como 

$$d_E(\underline{X},\underline{Y})=\sqrt{\displaystyle\sum_{k=1}^p (x_k-y_k)^2}$$ 

Esta distancia equivale a la suma de las longitudes de los segmentos que unen cada par de posiciones. La siguiente imagen muestra el perfil de dos usuarios en base a las valoraciones que han hecho de 10 ítems.


```{r, message=FALSE}
library(tidyverse)
usuario_a <- c(4, 4.5, 4, 7, 7, 6, 5, 5.5, 5, 6)
usuario_b <- c(4, 4.5, 4, 7, 7, 6, 5, 5.5, 5, 6) + 3
datos <- data.frame(usuario = rep(c("a", "b"), each = 10),
                    valoracion = c(usuario_a, usuario_b),
                    item = 1:10)
ggplot(data = datos, aes(x = as.factor(item), y = valoracion,
                         colour = usuario)) +
  geom_path(aes(group = usuario)) +
  geom_point() +
  geom_line(aes(group = item), colour = "firebrick", linetype = "dashed") +
  labs(x = "item") +
  theme_bw() + theme(legend.position = "bottom")
```

La distancia euclideana entre los dos usuarios es:

```{r}
sqrt(sum((usuario_a-usuario_b)^2))
```

Las funciones `dist()` de R base y `get_dist` de la librería `factoextra` se pueden utilizar para calcular las distancias entre todas las filas (observaciones) de una matriz o data frame. Por defecto, las funciones devuelven una matriz triangular inferior.

[*Ejemplo*]{style="color: blue;"}: Para el data frame de los pingüinos, calculamos las distancias euclideanas entre las ocho hembras de la especie Adelie que fueron registradas en Torgensen el año 2007.

```{r, message=FALSE}
p_ATf<- penguins %>% 
  filter(species=="Adelie", 
         island=="Torgersen",
         sex=="female",
         year==2007) %>% select(3:5)

dist(p_ATf)

library(factoextra)
get_dist(p_ATf, stand=TRUE)
```

Las distancias grandes indican mayores disimilitudes entre las observaciones. 

Observa que hemos calculado la distancia euclideana con la función `get_dist` pero estandarizando los datos especificando `stand=TRUE`, es decir, para cada variable (columna), restamos el valor medio de la columna y dividimos por la desviación típica de la columna, lo cual tiene sentido hacerlo cuando las variables del conjunto de datos están en escalas diferentes. El problema con la distancia euclideana es que no toma en cuenta la correlación entre las variables consideradas.


### Distancia de Minkowski

Otra distancia que podría ser considerada es la de Minkowski, cuya definición es la siguiente: 

$$d_{M_q}(\underline{X}_i,\underline{X}_j)=\left(\displaystyle\sum_{k=1}^p|x_{ik}-x_{jk}|^q \right)^{1/q}, \;\; q>0$$
Presenta los mismos inconvenientes que $d_E$ $(d_E = d_{M_2})$. 

* Si $q=1$, se le llama distancia de Manhattan

* Si $q \rightarrow \infty$, se le llama distancia dominante
$$d_{M_\infty}(\underline{X}_i,\underline{X}_j)=\max\{ |x_{i1}-x_{j1}|,\ldots,|x_{ip}-y_{jp}|\}$$


La siguiente imagen muestra una comparación entre la distancia euclídea (segmento azul) y la distancia de Manhattan (segmento rojo y verde) en un espacio bidimensional. Existen múltiples caminos para unir dos puntos con el mismo valor de distancia de Manhattan, ya que su valor es igual al desplazamiento total en cada una de las dimensiones.

```{r, warning=FALSE}
datos <- data.frame(observacion = c("a", "b"), x = c(2,7), y = c(2,7))
manhattan <- data.frame(
              x = rep(2:6, each = 2),
              y = rep(2:6, each = 2) + rep(c(0,1), 5),
              xend = rep(2:6, each = 2) + rep(c(0,1), 5),
              yend = rep(3:7, each = 2))

manhattan_2 <- data.frame(
                x = c(2, 5, 5, 7),
                y = c(2, 2, 4, 4),
                xend = c(5, 5, 7, 7),
                yend = c(2, 4, 4, 7))

ggplot(data = datos, aes(x = x, y = y)) +
geom_segment(aes(x = 2, y = 2, xend = 7, yend = 7), color = "blue", size = 1.2) +
geom_segment(data = manhattan, aes(x = x, y = y, xend = xend, yend = yend),
             color = "red", size = 1.2) +
geom_segment(data = manhattan_2, aes(x = x, y = y, xend = xend, yend = yend),
             color = "green3", size = 1.2) +
geom_point(size = 3) +
theme(panel.grid.minor = element_blank(),
      panel.grid.major = element_line(size = 2),
      panel.background = element_rect(fill = "gray",
                                      colour = "white",
                                      size = 0.5, linetype = "solid"))
```


## Distancias basadas en la correlación

Las distancias basadas en la correlación son útiles utilizadas cuando la similitud queremos establecerla en términos del patrón o forma y no de desplazamiento o magnitud; por ejemplo en el análisis de datos de expresión génica. 

$$d_{cor}(\underline{X}_i,\underline{X}_j)=1-Cor(\underline{X}_i,\underline{X}_j)$$

donde la correlación puede ser de distintos tipos (Pearson, Spearman, Kendall…)


En la siguiente imagen mostramos el perfil de 3 usuarios. Veamos qué ocurre si utilizamos la distancia euclideana y la de correlación de Pearson para compararlos.

```{r}
usuario_a <- c(4, 4.5, 4, 7.5, 7, 6, 5, 5.5, 5, 6)
usuario_b <- c(4, 4.5, 4, 7.5, 7, 6, 5, 5.5, 5, 6) + 4
usuario_c <- c(5, 5.5, 4.8, 5.4, 4.7, 5.6, 5.3, 5.5, 5.2, 4.8)

datos <- data.frame(usuario = rep(c("a", "b", "c"), each = 10),
                    valoracion = c(usuario_a, usuario_b, usuario_c),
                    item = 1:10)

ggplot(data = datos, aes(x = as.factor(item),
                         y = valoracion, 
                         colour = usuario)) +
  geom_path(aes(group = usuario)) +
  geom_point() +
  labs(x = "item") +
  theme_bw() +
  theme(legend.position = "bottom")
```

```{r}
dist(x = rbind(usuario_a, usuario_b, usuario_c), method = "euclidean")

1 - cor(x = cbind(usuario_a, usuario_b, usuario_c), method = "pearson")
```

De acuerdo a la distancia euclideana, los usuarios a y c son los más similares, mientras que acorde a la correlación de Pearson, a y b son los más parecidos. Este ejemplo pone de manifiesto que no existe una única medida de distancia que sea mejor que las demás, sino que, dependiendo del contexto, una será más adecuada que otra.


## Distancia de Mahalanobis

$$d^2_M \left( \underline{X}_i,\underline{X}_j \right)= \left(\underline{X}_i-\underline{X}_j \right)^t {\bf S}^{-1}
\left(\underline{X}_i-\underline{X}_j\right),$$

donde ${\bf S}$ es la matriz de covarianzas muestrales de la matriz de datos ${\bf X}$.

Es adecuada como medida de discrepancia entre datos, porque:

* Es invariante frente a transformaciones lineales invertibles de las variables.

* $d_E=d_M$ cuando ${\bf S}={\bf I}$ y $d_K = d_M$ cuando ${\bf S}=diag(s^2_1,\ldots,s^2_p),$ 

* Esta distancia tiene en cuenta las correlaciones entre las variables. 

* No aumenta por el simple hecho de aumentar el número de variables registradas, sino que solamente aumentará cuando las nuevas variables no sean redundantes con respecto de la información aportada por las anteriores.

* Esta es la distancia más utilizada cuando todas las variables son cuantitativas.


## Distancias para variables binarias:

Si $X_1,\ldots,X_p$ son variables que toman valores $\{0,1\}$, existen muchos **coeficientes de similaridad** $s_{ij}$ entre dos observaciones $i,j$, calculados a partir de las frecuencias tales que $a+b+c+d=p$, donde:

* $a$: el número de variables con respuesta 1 en ambos individuos,

* $b$: el número de variables con respuesta 0 en la observación $i$ y con respuesta 1 en el individuo $j$,

* $c$: el número de variables con respuesta 1 en la observación $i$ y con respuesta 0 en la observación $j$,

* $d$: el número de variables con respuesta 0 en ambos individuos.


Algunos coeficientes de similaridad son:

**Sokal y Michener**: $s_{ij}=\frac{a+d}{p}$, \hspace{2in} **Jaccard**: $s_{ij} =\frac{a}{a+b+c}$

Aplicando uno de estos coeficientes a un conjunto de $n$ objetos se obtiene una matriz de similaridades ${\bf S}=(s_{ij})_{n\times n}$


[*Ejemplo*]{style="color: blue;"}: Se han medido 6 variables sobre 3 individuos:

ind. | $X_1$ | $X_2$ |$X_3$ |$X_4$ |$X_5$ |$X_6$ |
-----|-------|-------|------|------|------|------|
1 | 1 | 1 |0 | 0 | 1 | 1
2 | 1 | 1 | 1 | 0 | 0 | 1
3 | 1 | 0 | 0 | 1 | 0 | 1

```{r, echo=F, fig.align='center', out.width='80%'}
knitr::include_graphics("Figuras/tabla_binaria.PNG")
```


Las matrices de similaridad son:
$${\bf S}_{Sokal}=\left(\begin{array}{ccc} 
1 & 0.6667 & 0.5\\
0.667 & 1 & 0.5\\
0.5 & 0.5 & 1
\end{array}\right), \;\;\; {\bf S}_{Jaccard}=\left(\begin{array}{ccc} 
1 & 0.6 & 0.4\\
0.6 & 1 & 0.4\\
0.4 & 0.4 & 1
\end{array}\right) $$


## Distancias para variables categóricas:

Se mide una variable categórica nominal con $k$ niveles sobre una muestra de $n = n_1+\cdots+n_g$ observaciones provenientes de $g$ poblaciones diferentes. Se desea obtener una medida de disimilaridad entre estas poblaciones.

En estas condiciones, el vector de frecuencias de cada población ${\bf n}_{\alpha}=(n_{\alpha1},\ldots,n_{\alpha k})$, para $\alpha=1,\ldots, g$, tiene una distribución
conjunta multinomial con parámetros $(n_{\alpha}, {\bf p}_{\alpha})$, donde $n_{\alpha}=n_{\alpha1}+\ldots +n_{\alpha k}$ y ${\bf p}_{\alpha}=(p_{\alpha 1},\ldots,p_{\alpha k}$ es el vector de
probabilidades de los $k$ niveles en la población $\alpha$ (con $p_{\alpha 1}+\ldots + p_{\alpha k}=1$).

[*Ejemplo*]{style="color: blue;"}: La siguiente tabla contiene las proporciones génicas observadas entre 10 poblaciones. Observa que las filas suman 1.


Población | grupo A | grupo AB | grupo B | grupo O
----------|---------|----------|---------|--------
francesa | 0.21 | 0.06 | 0.06 | 0.67
checa | 0.25 | 0.04 | 0.14 | 0.57
germánica | 0.22 | 0.06 | 0.08 | 0.64
vasca | 0.19 | 0.04 | 0.02 | 0.75
china | 0.18 | 0.00 | 0.15 | 0.67
ainu | 0.23 | 0.00 | 0.28 | 0.49
esquimal | 0.30 | 0.00 | 0.06 | 0.64
negra USA | 0.10 | 0.06 | 0.13 | 0.71
española | 0.27 | 0.04 | 0.06 | 0.63
egipcia | 0.21 | 0.05 | 0.20 | 0.54


Dos medidas de disimilaridad para este tipo de variables son:

* **Distancia de Bhattacharyya**
$$d^2_{ij}=arccos\left( \displaystyle\sum_{l=1}^k \sqrt{p_{il} \; p_{jl}}\right)$$

* **Distancia de Balakrishnan-Sanghvi**
$$d^2_{ij}= 2 \displaystyle\sum_{l=1}^k \frac{(p_{il}-p_{jl})^2}{p_{il}+p_{jl}} $$

Para los datos del ejemplo, la matriz de distancias de Bhattacharyya es:
```{r}
poblacion=c("francesa","checa","germánica","vasca","china","ainu","esquimal","negra USA","española","egipcia")
grupoA=c(0.21,0.25,.22,.19,.18,.23,.3,.1,.27,.21)
grupoAB=c(.06,.04,.06,.04,.00,.00,.00,.06,.04,.05)
grupoB=c(.06,.14,.08,.02,.15,.28,.06,.13,.06,.2)
grupoO=c(.67,.57,.64,.75,.67,.49,.64,.71,.63,.54)
datos=data.frame(poblacion,grupoA,grupoAB,grupoB,grupoO)
```


```{r,message=FALSE}
library(philentropy)
distance(datos[,-1], method = "bhattacharyya")
```

Las poblaciones más cercanas (según la distancia de Battacharyya) son la francesa y la germánica con $d_{1,3}=0.00095$. Mientras que los más alejados son las poblaciones francesa y ainu con $d_{1,6}=0.08082$.



## Distancias para variables mixtas
Cuando disponemos de un conjunto de observaciones sobre variables tanto cuantitativas como cualitativas.

Se define la distancia de Gower como $d^2_{ij}=1-s_{ij}$, donde 
$$s_{ij}=\frac{\sum_{k=1}^{p_1}(1-|x_{ik}-x_{jk}|/G_k)+a+\alpha}{p_1+(p_2-d)+p_3}$$
es el coeficiente de similaridad de Gower,

* $p_1$ es el número de variables cuantitativas continuas,

* $p_2$ es el número de variables binarias,

* $p_3$ es el número de variables cualitativas (no binarias),

* $a$ es el número de coincidencias (1,1) en las variables binarias,

* $d$ es el número de coincidencias (0,0) en las variables binarias,

* $\alpha$ es el número de coincidencias en las variables cualitativas (no binarias) y

* $G_h$ es el rango de la $k$-ésima variable cuantitativa.

[*Ejemplo*]{style="color: blue;"}: Considera 7 variables registradas sobre 50 jugadores de la liga española de fútbol.

```{r, echo=F, fig.align='center', out.width='80%'}
knitr::include_graphics("Figuras/tabla_futbol.PNG")
```


* $X_1$ es el número de goles marcados, $X_2$ edad (en años),

* $X_3$ altura (m), $X_4$ peso (kg),

* $X_5$ pierna buena del jugador (1=drecha, 0=izquierda),

* $X_6$ nacionalidad: (1=Argentina, 2=Brasil, 3=Camerun, 4=Italia, 5=España, 6=Francia, 7=Uruguay, 8=Portugal, 9=Inglaterra).

* $X_7$ tipo de estudios (1=sin estudios, 2=básicos, 3=medios, 4=superiores)


```{r,message=FALSE,warning=FALSE}
futbol=read.table("datos/jugadores.txt")
library(StatMatch)
distancia=gower.dist(futbol)
head(distancia,2)
```



Con la librería `stats` de R base puedes calcular la distancia: "euclidean", "maximum", "manhattan", "canberra", "binary" o "minkowski".

La función `distance()`de la libreria `philentropy` es capaz de calcular 46 diferentes distancias/similaridades.


```{r}
getDistMethods()
```

## Visualización de matrices de distancias 

Una solución sencilla para visualizar las matrices de distancia es utilizar la función `fviz_dist()` de la librería  `factoextra`. Otros métodos especializados, como la agrupación jerárquica aglomerativa o el mapa de calor, las presentaremos más adelante.

[*Ejemplo*]{style="color: blue;"}: Visualizamos la tabla de los jugadores de fútbol.

```{r}
library(factoextra)
fviz_dist(as.dist(distancia))
```

Si el color es rojo, la similitud es alta (es decir, distancia cero). Si el color es azul,  la similitud es baja. El nivel de color es proporcional al valor de la distancia entre las observaciones. Se han ordenado a los jugadores por similitud.

[*Ejemplo*]{style="color: blue;"}: Otro ejemplo que podéis consultar son los mapas de calor que utilizamos en BIOCOM (grupo de investigación de la UIB en Biología Computacional y Bioinformática) para presentar las diferencias entre kernels ("distancias") para comparar grafos del metabolismo entre organismos [Figura 3 - Exploring the expressiveness of abstract metabolic networks](https://journals.plos.org/plosone/article/figure?id=10.1371/journal.pone.0281047.g003)

Observamos una buena separación entre animales, plantas y hongos en todos los mapas de calor. Los protistas están separados, pero en general están más cerca de los hongos que del resto de Eucariotas. Además, en todos los mapas de calor, los animales están bien separados de los demás reinos y muestran un patrón interno adicional que merece un análisis más detallado. También cabe destacar que todos los kernels ponen de manifiesto una marcada diferencia entre los animales y las plantas. Las plantas están más cerca de los hongos que de los animales.