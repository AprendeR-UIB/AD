## Análisis Multivariante

En general, los datos que se recogen en experimentos son multidimensionales: miden diversas variables aleatorias sobre una misma muestra de individuos. Esta información se organiza en tablas de datos con filas que representan a los individuos observados y cada columna corresponde a una variable diferente. Lo que hacemos es evaluar un vector de variables aleatorias (que llamaremos **vector aleatorio**) sobre los individuos de una población. 

En esta parte vamos a introducir algunos conceptos sobre vectores de variables aleatorias y tablas multidimensionales de datos cuantitativos.

## Poblaciones: vectores aleatorios

Un **vector aleatorio de dimensión $p$** es un vector formado por $p$ variables aleatorias 
$$
\underline{X}=(X_1,X_2,\ldots,X_p).
$$
Una **realización** de $\underline{X}$ es un vector $(x_1,\ldots,x_p)$ formado por los valores $X_1,\ldots,X_p$ sobre un individuo. 

Una **muestra** de $\underline{X}$ es un conjunto de realizaciones. 

Usualmente, organizamos una muestra de $\underline{X}$ por medio de una tabla de datos con las columnas definidas por las variables $X_1,\ldots,X_p$ y donde cada fila es una realización de estas variables, es decir, un vector formado por los valores de $X_1,\ldots,X_p$ sobre un individuo de la muestra.


[*Ejemplo*]{style="color: blue;"}:
Sea $\textit{BMI}$ la variable aleatoria que registra el índice de masa corporal (BMI) de una persona, $C$ la variable aleatoria que registra el nivel de colesterol en mg/dl de una persona y $E$  la variable aleatoria que da la edad de una persona en años, entonces
$$
\underline{X}=(\textit{BMI},C,E)
$$
es un **vector aleatorio** de dimensión 3. Cada vez que tomamos una persona y anotamos el BMI, el nivel de colesterol y la edad y organizamos estas medidas en este orden en un vector, obtenemos una **realitzación** de este vector aleatorio $\underline{X}$.  Entonces,una **muestra** de $\underline{X}$ será un conjunto de vectores con el BMI, el nivel de colesterol y la edad de un grupo de personas de la población. Por ejemplo 

```{r,echo=FALSE}
library(kableExtra)
```


```{r,echo=FALSE}
kable(data.frame(BMI=c(18.3,24.4,24.6,24.4,22.2,19.5), C=c(170,202,215,218,210,210),E=c(49,39,50,44,40,36)))
```


Sean $\underline{X}=(X_1,X_2,\ldots,X_p)$ un vector aleatorio y,  $\mu_i$ y $\sigma_i$ la media y la desviación típica, respectivamente, de cada $X_i$. 

* El **valor esperado**, o **vector de medias**,  de  $\underline{X}$ es el vector formado por los valores esperados, o medias de sus componentes:
$$
E(\underline{X})=(\mu_1,\ldots,\mu_p).
$$
Para abreviar, a veces indicaremos este vector simplemente con $\boldsymbol\mu$.

* El **vector de varianzas**  de  $\underline{X}$ es el vector formado por las varianzas de sus componentes:
$$
\sigma^2(\underline{X})=(\sigma_1^2,\ldots,\sigma_p^2).
$$

* El **vector de desviaciones típicas**  de  $\underline{X}$ es el vector formado por las desviaciones típicas de sus componentes:
$$
\sigma(\underline{X})=(\sigma_1,\ldots,\sigma_p).
$$

## Covarianza 

La **covarianza** de dos variables $X$ e $Y$ es una medida del comportamiento conjunto de estas dos variables. 

Formalmente, dadas dos variables aleatorias $X,Y$ con medias $\mu_X$ y $\mu_Y$, respectivamente, su **covarianza** es 
$$
\sigma_{X,Y}=E\left((X-\mu_X)\cdot ( Y-\mu_Y)\right).
$$
Es fácil comprobar que
$$
\sigma_{X,Y}=E(X\cdot Y) -\mu_X\cdot \mu_Y.
$$


En efecto,
$$
\begin{array}{rl}
\sigma_{X,Y} & =E((X-\mu_X) ( Y-\mu_Y))=
E(XY-\mu_XY-\mu_YX+\mu_X\mu_Y)\\ &
=E(XY)-\mu_XE(Y)-\mu_YE(X)+\mu_X\mu_Y\\ &=
E(XY)-\mu_X\mu_Y-\mu_Y\mu_X+\mu_X\mu_Y
=E(XY)-\mu_X\mu_Y
\end{array}
$$


La covarianza de  $X$ e $Y$ puede tomar cualquier valor real (no como la varianza, que siempre es positiva), y  mide el grado de variación conjunta de las variables en el sentido sigüiente:

* $\sigma_{X,Y}>0$ significa que cuando $X$ es más grande en un individuo 1 que en un individuo 2, $Y$  tiende  también a ser más gran en el individuo 1 que en el individuo 2.


```{r, echo=FALSE}
set.seed(100)
x=rnorm(50,5,1)
y=x+rnorm(50,0,0.5)
z=8-x+rnorm(50,0,0.5)
set.seed(300)
t=rnorm(50,6,2)
plot(x,y,pch=20,xlab="",ylab="",main="Covarianza > 0",cex=1.5)
```


* $\sigma_{X,Y}<0$ significa que cuando $X$ es más grande en un individuo 1 que en un individuo 2, $Y$ tiende a ser más pequeña en el individuo 1 que en el individuo 2.



```{r, echo=FALSE}
plot(x,z,pch=20,xlab="",ylab="",main="Covarianza < 0",cex=1.5)
```

* $\sigma_{X,Y}=0$ significa que no hay ninguna tendencia en este sentido.


```{r, echo=FALSE}
plot(x,t,pch=20,xlab="",ylab="",main="Covarianza = 0",cex=1.5)
par(mfrow=c(1,1))
```



El signo de la covarianza  refleja la "tendencia del crecimiento conjunto" de las variables:

* Covarianza positiva significa la misma tendencia: Si $X$ aumenta, $Y$ tiende a aumentar. Esto suele expresarse diciendo que hay **asociación positiva** entre $X$ e $Y$.

* Covarianza negativa significa la tendencia inversa: Si $X$ aumenta, $Y$ tiende a disminuir.  Esto suele expresarse diciendo que hay **asociación negativa** entre $X$ e $Y$.


 
**Si $X$ e $Y$ son variables independientes, su covarianza es 0**, porque en este caso $E(X\cdot Y) =\mu_X\mu_Y$ y por tanto
$$
\sigma_{X,Y}=E(X\cdot Y) -\mu_X\cdot \mu_Y=\mu_X\cdot \mu_Y-\mu_X\cdot \mu_Y=0.
$$
Intuitivamente, si $X$ e $Y$ son independientes, significa que el hecho de que el valor de $X$ aumente de un individuo a otro no tiene ningún efecto sobre el valor de $Y$.


¿Por qué, si $X$ e $Y$ son independientes, $E(X\cdot Y) =\mu_X\mu_Y$? 

Os lo demostraremos en el caso discreto; el argumento en el caso continuo es lo mismo cambiando sumatorios por integrales.
$$
\begin{array}{rl}
E(X\cdot Y)\!\!\! &\displaystyle =\sum_{x\in D_X,y\in D_Y} xyP(X=x,Y=y)\\
&\displaystyle =\sum_{x\in D_X,y\in D_Y} xyP(X=x)P(Y=y)\\
&\text{(por la independencia de $X$ e $Y$)}\\
&\displaystyle =\Big(\sum_{x\in D_X}xP(X=x)\Big)\Big(\sum_{y\in D_Y} yP(Y=y)\Big)=E(X)E(Y)
\end{array}
$$


Es importante remarcar que la igualdad $E(X\cdot Y) =\mu_X\mu_Y$ es equivalente a la igualdad $\sigma(X+Y)^2 =\sigma(X)^2+\sigma(Y)^2$ que decíamos que satisfacen las variables independientes. En efecto
$$
\begin{array}{l}
\sigma(X+Y)^2 -(\sigma(X)^2+\sigma(Y)^2)\\
\quad = E((X+Y)^2)-E(X+Y)^2-(E(X^2)-E(X)^2+E(Y^2)-E(Y)^2)\\
\quad = E(X^2+2XY+Y^2)-(E(X)+E(Y))^2\\
\qquad\qquad -E(X^2)+E(X)^2-E(Y^2)+E(Y)^2\\
\quad = E(X^2)+2E(XY)+E(Y^2)-E(X)^2-2E(X)E(Y)-E(Y)^2\\
\qquad\qquad -E(X^2)+E(X)^2-E(Y^2)+E(Y)^2\\
\quad = 2E(XY)-2E(X)E(Y)=2(E(XY)-\mu_X\mu_Y)
\end{array}
$$
y por tanto
$$
\sigma^2(X+Y) -(\sigma^2(X)+\sigma^2(Y))=0 \Longleftrightarrow E(XY)-\mu_X\mu_Y=0
$$




**Si $X$ e $Y$ son variables independendientes, su covarianza es 0**, pero la implicación al contrario es falsa: **Dos variables aleatorias pueden tener covarianza 0 y no ser independientes**.

Veamos un ejemplo de este hecho:




Supongamos que tenemos un dado tetraédrico no trucado con las caras marcadas con los valores -2, -1, 1 y 2. Sean $X$ la variable aleatoria que consiste en llanzar el dado y anotar el resultado (la cara que queda en tierra), e $Y$ la variable aleatoria que consite en lanzar el dado y anotar el *cuadrado* del resultado obtenido. Como las cuatro caras del dado son equiprobables, 
$$
\begin{array}{l}
\displaystyle P(X=-2)=P(X=-1)=P(X=1)=P(X=2)=\frac{1}{4}\\
\displaystyle P(Y=1)=P(Y=4)=\frac{1}{2}
\end{array}
$$
  
  

Comp $Y$ es función de $X$, ya que $Y=X^2$, $X$ e $Y$ no pueden ser independientes. Veamos que, en efecto, no lo son. Observad que los únicos posibles valores para el vector $(X,Y)$ en una tirada del dado son (-2,4), (-1,1), (1,1) y (2,4), cada uno con probabilidad 1/4. Entonces, por ejemplo, la probabilidad de obtener en una tirada $X=-1$ i $Y=4$ es 0, porque es imposible, mientras que
$$
P(X=-1)\cdot P(Y=4)=\frac{1}{4}\cdot\frac{1}{2}=\frac{1}{8}\neq 0.
$$

Veamos ahora que la covarianza de  $X$ e $Y$ es 0. Para calcularla, primero necesitamos calcular los valores esperados de las variables:
$$
\begin{array}{l}
\displaystyle \mu_X=(-2)\cdot \frac{1}{4}+(-1)\cdot \frac{1}{4}+1\cdot \frac{1}{4}+2\cdot \frac{1}{4}=0\\
\displaystyle \mu_Y=1\cdot \frac{1}{2}+4\cdot \frac{1}{2}=2.5
\end{array}
$$
Por tanto
$$
\begin{array}{l}
\sigma_{X,Y}=E\big(X\cdot Y\big)-\mu_X\cdot \mu_Y=E\big(X\cdot Y\big)-0\cdot 2.5=E\big(X\cdot Y\big)\\
\qquad =P\big(X=-2,Y=4\big)\cdot (-2\cdot 4)+P\big(X=-1,Y=1\big)\cdot (-1\cdot 1)\\
\qquad\qquad\qquad +P\big(X=1,Y=1\big)\cdot (1\cdot 1)+P\big(X=2,Y=4\big)\cdot (2\cdot 4)\\
\qquad =\displaystyle \frac{1}{4}\cdot (-8)+\frac{1}{4}\cdot (-1)+\frac{1}{4}\cdot 1+\frac{1}{4}\cdot 8=0.
\end{array}
$$
Así pues, $X$ e $Y$ son variables dependientes, pero su covarianza es 0.

Dos propiedades importantes más de la covarianza:

* La covarianza es simétrica: 
$$
\begin{array}{rl}
\sigma_{X,Y}\!\!\! & =E((X-\mu_X)\cdot ( Y-\mu_Y))\\
& =E(( Y-\mu_Y)\cdot (X-\mu_X))=\sigma_{Y,X}
\end{array}
$$

* La covarianza de una variable aleatoria con ella misma es su varianza:
$$
\sigma_{X,X}=E((X-\mu_X)^2)=\sigma^2(X)
$$

La **matriz de covarianzas** de un vector aleatorio $\underline{X}=(X_1,\ldots,X_p)$ es la matriz formada por las covarianzas de los pares de variables que la formen:
$$
\sigma_{\underline{X},\underline{X}}=\begin{pmatrix} \sigma_{X_1,X_1} & \sigma_{X_1,X_2} & \ldots & \sigma_{X_1,X_p}\\
\sigma_{X_2,X_1} & \sigma_{X_2,X_2} & \ldots & \sigma_{X_2,X_p}\\
\vdots & \vdots &\ddots  & \vdots\\
\sigma_{X_p,X_1} & \sigma_{X_p,X_2} & \ldots & \sigma_{X_p,X_p}\\
 \end{pmatrix}
$$

Esta matriz es simétrica y las entradas de la diagonal son las varianzas de las variables del vector, porque $\sigma_{X_i,X_i}=\sigma^2_{X_i}$.

## Correlación

Como hemos dicho, el signo de la covarianza tiene una interpretación sencilla, puesto que refleja la tendencia del crecimiento conjunto de las variables. Pero, su magnitud no tiene una interpretación sencilla. 

Como alternativa, se puede medir la tendencia de que haya una *relación lineal* entre dos variables aleatorias **continuas** empleando el llamado *coeficiente de correlación lineal de Pearson* (o, para abreviar, la **correlación**), que viene a ser una versión normalizada de la covarianza. 

En concreto, la **correlación** de las variables $X$ e $Y$ se define como el cociente de su covarianza entre el producto de sus desviaciones típicas:
$$
\rho_{X,Y}=\frac{\sigma_{X,Y}}{\sigma_{X} \sigma_{Y}}
$$

La correlación tiene las propiedades importantes siguientes:

1. No tiene unidades (porque las unidades de $\sigma_X$ son las de $X$, las unidades de $\sigma_Y$ son las de $Y$, y las unidades de $\sigma_{X,Y}$ son las de $X$ por las de $Y$)

2. Toma valores entre -1 y 1: $-1\leqslant \rho_{X,Y}\leqslant 1$

3. Es simétrica, $\rho_{X,Y}= \rho_{Y,X}$

4. La correlación de una variable con ella misma es 1: $\rho_{X,X}=1$

5. $\rho_{X,Y}=\pm 1$ si, y solo si, las variables $X,Y$ tienen una *relación lineal perfecta*. Es decir, $\rho_{X,Y}=\pm 1$ si, y solo si, existen $a,b\in \mathbb{R}$ con $a\neq 0$ y tales que $Y= X+b$. La pendiente $a$ de esta recta tiene el mismo signo que $\rho_{X,Y}$.

6. Cuanto más se acerca $|\rho_{X,Y}|$ a 1, más se acerca $Y$ a ser función lineal de $X$.

* Si $\rho_{X,Y}>0$, la función es creciente
* Si $\rho_{X,Y}<0$, la función es decreciente

6. Si $\rho_{X,Y}=0$, decimos que las variables $X$ e $Y$ son **incorreladas**. Notamos que la correlación es 0 si, y solo si, la covarianza es 0. Por lo tanto, *si $X$ y $Y$ son independientes, también son incorreladas. El recíproco en general es falso*.



Si una de las dos variables tiene desviación típica 0, en la fórmula de la correlación aparece un 0 en el denominador y no la podemos calcular. En este caso, se toma $\rho_{X,Y}=0$. El motivo intuitivo es que una variable constante es siempre independiente de cualquier otra variable (haga lo que haga la otra, siempre toma el mismo valor) y hemos quedado que las variables independientes son incorreladas.


La *matriz de correlaciones* de un vector aleatorio $\underline{X}=(X_1,\ldots,X_p)$ es la matriz formada por las correlaciones de pares de sus variables:
$$
\rho(\underline{X})
=\begin{pmatrix} 1 & \rho_{X_1,X_2} & \ldots & \rho_{X_1,X_p}\\
\rho_{X_2,X_1} & 1 & \ldots & \rho_{X_2,X_p}\\
\vdots & \vdots & \ddots & \vdots\\
\rho_{X_p,X_1} & \rho_{X_p,X_2} & \ldots & 1\\
 \end{pmatrix}.
$$
Esta matriz es simétrica por la simetría de la correlación.


La correlación de Pearson de dos variables continuas mide la tendencia de las variables a variar conjuntamente de manera lineal. En particular, por ejemplo, si $\rho_{X,Y}>0$, $Y$ tiende a crecer cuando $X$ crece. Pero esto no significa que un aumento del valor de $X$ cause que el valor de $Y$ tienda a aumentar:

> **Correlación no implica causalidad!** 

La tendencia al crecimiento simultáneo de $X$ y $Y$ se puede deber a una tercera variable que las haga crecer las dos, o puede ser puramente espuria.




Si entráis a la página web [*Spurious Correlations*](https://tylervigen.com/spurious-correlations) podréis explorar un montón de correlaciones espurias. Nuestra preferida es una correlación de 0.947 entre la variable $X$= "Tomo un año y anoto el consumo per capita* de queso en los EE. UU." e $Y$= "Tomo un año y anoto el número de muertes por estrangulamiento accidental con las sábanas de la cama en los EE. UU.".

```{r, echo=FALSE, out.width="80%", fig.align='center'}
knitr::include_graphics("Figuras/spurcorr1.png")
```

Hay un ejemplo de correlación negativa que es importante tener presente para no dejarse engañar.

**Teorema**:
Si $X_1$, $X_2$  son dos copias independientes de una misma variable aleatoria $X$,
$$
\rho_{X_1,X_2-X_1}=-\frac{1}{\sqrt{2}}\approx -0.71
$$


Esto nos dice que, sea cual sea la variable $X$, si la medimos en dos momentos independientes o sobre dos individuos elegidos de manera independiente, la diferencia entre los dos valores *tiene una tendencia destacada a decrecer linealmente* en el primer valor. Por ejemplo:

* Hacéis un test y sacáis una nota muy baja ($X_1$). El día siguiente hacéis otro test similar ($X_2$) sin haber estudiado más. Lo más probable es que, por puro azar, saques una nota más alta (que $X_2-X_1$ sea grande, por lo tanto positivo).

* Hacéis un test y sacáis una nota muy alta ($X_1$). El día siguiente hacéis otro test ($X_2$) sin haber estudiado más. Lo más probable es que, por puro azar, saques una nota más baja (que $X_2-X_1$ sea pequeño, por lo tanto negativo).

Por si alguno necesita una demostración del teorema anterior, recordamos que
$$
\rho_{X_1,X_2-X_1}=\dfrac{\sigma_{X_1,X_2-X_1}}{\sigma_{X_1}\sigma_{X_2-X_1}}
$$
Ahora
$$
\begin{array}{l}
\sigma_{X_2-X_1}=\sqrt{\sigma^2_{X_2-X_1}}\\[2ex]
\quad =\sqrt{\sigma^2_{X_1}+\sigma^2_{X_2}}\ \text{(porque son independientes)}\\[2ex]
\quad =\sqrt{\sigma^2_{X}+\sigma^2_{X}}\ \text{(porque $X_1,X_2$ son copias de $X$)}\\[2ex]
\quad =\sqrt{2\sigma^2_{X}}=\sigma_{X}\sqrt{2}
\end{array}
$$
Luego,
$$
\begin{array}{l}
\sigma_{X_1,X_2-X_1}=E(X_1(X_2-X_1))-E(X_1)E(X_2-X_1)\\[1ex]
\quad =E(X_1X_2-X_1^2)-E(X_1)(E(X_2-E(X_1))\\[1ex]
\quad =E(X_1X_2)-E(X_1^2)-E(X_1)E(X_2)+E(X_1)E(X_1)\\[1ex]
\quad =E(X_1)E(X_2)-E(X_1^2)-E(X_1)E(X_2)+E(X_1)E(X_1)\\[1ex]
\quad \text{(porque  $X_1,X_2$ son independents)}\\[1ex]
\quad =-E(X_1^2)+E(X_1)E(X_1)=-\sigma^2_{X_1}=-\sigma^2_{X}
\end{array}
$$

Combinando lo anterior:
$$
\rho_{X_1,X_2-X_1}=\dfrac{\sigma_{X_1,X_2-X_1}}{\sigma_{X_1}\sigma_{X_2-X_1}}=\dfrac{-\sigma^2_{X}}{\sigma_{X}\cdot \sigma_{X}\sqrt{2}} =-\frac{1}{\sqrt{2}}
$$


Como ejemplo, generaremos una muestra $X$ de 101 "notas" aleatorias entre 0 y 100 con distribución binomial $B(100,0.5)$. Tomaremos como $X_1$ el vector de las primeras 100 notas,
$$
X_1=(x_1,x_2,\ldots,x_{100})
$$
y como $X_2-X_1$ el vector de las diferencias de cada nota $x_y$, $y\geqslant 2$, con la anterior:
$$
X_2-X_1=(x_2-x_1,x_3-x_2,\ldots,x_{101}-x_{100}).
$$
Calcularemos la correlación entre $X_1$ y $X_2-X_1$, y lo ilustraremos con un gráfico.



```{r,echo=FALSE}
set.seed(42)
```
```{r}
X=rbinom(101,100,0.5)
X1=X[-101]
X2.menos.X1=diff(X)
plot(X1,X2.menos.X1,pch=20,xlab=expression(X[1]),
     ylab=expression(X[2]-X[1]))
cor(X1,X2.menos.X1)
```
La correlación predicha por el teorema anterior es
```{r}
-1/sqrt(2) 
```

## Estadística descriptiva: Muestras

### Covarianza

Sean $X=(x_1,\ldots,x_n)$ y $Y=(y_1,\ldots,y_n)$ dos vectores obtenidos de mediciones de dos variables aleatorias cuantitativas sobre una misma muestra ordenada de individuos de tamño $n$ de una población. Sean $\overline{X}$ y $\overline{Y}$ sus medias muestrales. Entonces su *covarianza muestral* es
$$
\widetilde{S}_{X,Y} =\frac{1}{n-1} \sum_{i =1}^n\big((x_{i}-\overline{{X}})(y_i-\overline{Y})\big)
$$
y su **covarianza** (**a secas**) es
$$
{S}_{X,Y} =\frac{1}{n} \sum_{i =1}^n\big((x_{i}-\overline{{X}})(y_i-\overline{Y})\big)=\frac{n-1}{n}\widetilde{S}_{X,Y}.
$$
Es decir, la diferencia entre la versión "muestral" y la versión "a secas" recae en el denominador, $n-1$ y $n$ respectivamente.


> La covarianza de dos vectores solo tiene sentido cuando estos vectores representan los valores de dos variables cuantitativas sobre los mismos individuos, o sobre dos muestras emparejadas, y en el mismo orden. En particular, los dos vectores tienen que tener la misma longitud.


Como en el caso poblacional, la **covarianza** entre dos vectores mide la tendencia que tienen los datos a variar conjuntamente:


* Cuando $\widetilde{S}_{X,Y}>0$, si $x_i>x_j$ entonces $y_i$ tiende a ser más grande que $y_j$

* Cuando $\widetilde{S}_{X,Y}<0$, si $x_i>x_j$ entonces $y_i$ tiende a ser más pequeño que $y_j$

* Cuando $\widetilde{S}=0$, no hay ninguna tendencia en este sentido


Es fácil comprobar que:

* Las dos covarianzas son simétricas
$$
\widetilde{S}_{X,Y}=\widetilde{S}_{Y,X},\ {S}_{X,Y}={S}_{Y,X}
$$
* La varianza de un vector es su covarianza con él mismo
$$
\widetilde{S}_{X,X}=\widetilde{S}^2_{X},\ {S}_{X,X}={S}^2_{X}.
$$


[*Ejemplo*]{style="color: blue;"}

Hemos medido el índice de masa corporal, BMI, y el nivel de colesterol en 5 individuos sanos. Guardamos los resultados en un *dataframe* y calculamos las medias:


```{r}
BMI= c(18.3,24.4,24.6,24.4,22.2,19.5)
Chol=c(170,202,215,218,210,210)
DF=data.frame(BMI,Chol)
mean(BMI)
mean(Chol)
```


Entonces la covarianza muestral de estos dos vectores es
$$
\begin{array}{l}
\dfrac{1}{5}\Big((18.3-22.23)(170-204.17)+(24.4-22.23)(202-204.17)\\
\qquad +(24.6-22.23)(215-204.17)+(24.4-22.23)(218-204.17)\\
\qquad +(22.2-22.23)(210-204.17)+(19.5-22.23)(210-204.17)\Big)=`r round(cov(BMI,Chol),4)`
\end{array}
$$

Entonces la covarianza muestral de estos dos vectores es
$$
\begin{array}{l}
\dfrac{1}{6}\Big((18.3-22.23)(170-204.17)+(24.4-22.23)(202-204.17)\\
\qquad +(24.6-22.23)(215-204.17) +(24.4-22.23)(218-204.17)\\
\qquad +(22.2-22.23)(210-204.17)+(19.5-22.23)(210-204.17)\Big)=`r round(5*cov(BMI,Chol)/6,4)`
\end{array}
$$



La covarianza **muestral** de dos vectores numéricos de la misma longitud $n$ se calcula en R con la función `cov`.

```{r}
cov(BMI,Chol)
```

Para obtener su covarianza a secas, hay que multiplicar el resultado de `cov` por $(n-1)/n$.

```{r}
n=length(BMI)
cov(BMI,Chol)*(n-1)/n
```

Consideramos una tabla de datos cuantitativos de la forma
$$
\begin{array}{cccc}
X_1 & X_2 & \ldots & X_p\\ \hline
x_{1 1} & x_{1 2} &\ldots & x_{1 p}\\
x_{2 1} & x_{2 2} &\ldots & x_{2 p}\\
\vdots & \vdots   &   \ddots    &\vdots\\ 
x_{n 1} & x_{n 2} &\ldots & x_{n p}
\end{array}
$$
donde cada columna representa los valores de cierta variable $X_i$ y cada fila un individuo de una muestra de la población, de forma que la entrada $x_{ij}$ de esta tabla es el valor de $X_j$ sobre el individuo $i$-ésimo de la muestra.

La *matriz de covarianzas muestrales* de esta tabla es la matriz
$$
\widetilde{{S}}=
\begin{pmatrix}  
 \widetilde{S}^2_{X_1} & \widetilde{S}_{X_1,X_2} & \ldots & \widetilde{S}_{X_1,X_p}\\
 \widetilde{S}_{X_2,X_1} &  \widetilde{S}^2_{X_2} & \ldots & \widetilde{S}_{X_2,X_p}\\
  \vdots & \vdots  &  \ddots      & \vdots\\
\widetilde{S}_{X_p,X_1} & \widetilde{S}_{X_p,X_2} & \ldots &  \widetilde{S}^2_{X_p}
\end{pmatrix}
$$
y la *matriz de covarianzas* (*a secas*) se define de manera similar, pero con las covarianzas a secas:
$$
{S}=
\begin{pmatrix}  
 S^2_{X_1} & S_{X_1,X_2} & \ldots & S_{X_1,X_p}\\
 S_{X_2,X_1} & S^2_{X_2} & \ldots & S_{X_2,X_p}\\
  \vdots & \vdots  &  \ddots      & \vdots\\
S_{X_p,X_1} & S_{X_p,X_2} & \ldots & S^2_{X_p}
\end{pmatrix}
$$
Las dos son simétricas.


La matriz de covarianzas muestrales se calcula con la función `cov` aplicada a la matriz o al data frame de variables numéricas que almacena la tabla de datos. Para calcular la matriz de covarianzas a secas, se multiplica el resultado de `cov` por $(n-1)/n$, donde $n$ es el número de filas de la tabla.

[*Ejemplo*]{style="color: blue;"}

Añadiremos a los datos del ejemplo anterior una tercera variable con las edades de los 5 individuos.



```{r}
DF$Edad=c(49,39,50,44,40,36)
DF
```

La matriz de covarianzas muestrales de esta tabla es
```{r}
cov(DF)
```

Podréis observar que es simétrica, que la entrada (2,1) coincide con la covarianza de `BMI` y `Chol` que hemos calculado antes, y que en la diagonal obtenemos las varianzas muestrales de las variables de la tabla:
```{r}
apply(DF,MARGIN=2,FUN=var)
```

### Correlación de Pearson

Sean $X=(x_1,\ldots,x_n)$ e $Y=(y_1,\ldots,y_n)$ dos vectores obtenidos midiendo dos variables aleatorias continuas sobre una misma muestra de individuos de medida $n$ de una población.

La *correlación de Pearson* de $X$ e $Y$ es su covarianza muestral dividida por el producto de sus desviaciones típicas muestrales:
$$
R_{X,Y}=\frac{\widetilde{S}_{X,Y}}{\widetilde{S}_X\cdot \widetilde{S}_Y}.
$$

La *correlación de Pearson* de $X$ e $Y$ también es igual a su covarianza a secas dividida por el producto de sus desviaciones típicas a secas, porque los cambios de denominador se cancelan:
$$
R_{X,Y}=\frac{\widetilde{S}_{X,Y}}{\widetilde{S}_X\cdot \widetilde{S}_Y}=
\frac{\frac{n}{n-1}\cdot {S}_{X,Y}}{\sqrt{\frac{n}{n-1}}\cdot {S}_X \cdot\sqrt{\frac{n}{n-1}}\cdot{S}_Y}=
\frac{S_{X,Y}}{S_X \cdot S_Y}=R_{X,Y}.
$$

[*Ejemplo*]{style="color: blue;"}

Volvemos a la situación del ejemplo anterior. La covarianza muestral y las desviaciones típicas muestrales de los vectores `BMI` y `Chol` son
```{r}
cov(BMI,Chol)
sd(BMI)
sd(Chol)
```
y por tanto su correlación de Pearson es
$$
R_{BMI,Chol}=\frac{`r round(cov(BMI,Chol),3)`}{`r round(sd(BMI),3)`\cdot `r round(sd(Chol),3)`}= `r round(round(cov(BMI,Chol),3)/(round(sd(BMI),3)* round(sd(Chol),3)),3)`
$$


Algunas propiedades importantes de la correlación de Pearson:

* La correlación de Pearson es simétrica:
$$
R_{X,Y}=R_{Y,X}
$$

* La correlación de Pearson toma valores solo entre -1 y 1:
$$
-1\leqslant R_{X,Y}\leqslant 1
$$

* La correlación de Pearson de un vector con él mismo es 1:
$$
R_{X,X}=1
$$

* $R_{X,Y}$ tiene el mismo signo que $S_{X,Y}$, y por tanto este signo tiene el mismo significado que a la covarianza:

* Si $R_{X,Y}>0$ y si $x_i>x_j$, $y_i$ tiende a ser más grande que $y_j$

* Si $R_{X,Y}<0$ y si $x_i>x_j$, $y_i$ tiende a ser más pequeño que $y_j$

* Si $R=0$, no hay ninguna tendencia en este sentido


* $R_{X,Y}=\pm 1$ si, y solo si, todos los puntos $(x_i,y_i)$ están sobre una recta $y=ax+b$ con $a\neq 0$. La pendiente $a$ de esta relación lineal tiene el mismo signo que $R_{X,Y}$. Por lo tanto, la recta es creciente si $R_{X,Y}=1$ y decreciendo si $R_{X,Y}=- 1$.

* El coeficiente de determinación $R^2$ de la regresión lineal por mínimos cuadrados de $Y$ respecto de $X$ es igual al cuadrado de su correlación de Pearson:
$$
R^2=R_{X,Y}^2
$$


>Por lo tanto, cuanto más se acerca la correlación de Pearson de $X$ e $Y$ a 1 o a -1, más se acercan los puntos $(x_i,y_i)$ a estar sobre una recta. El signo de $R_{X,Y}$ indica si esta recta es creciente ($R_{X,Y}>0$) o decreciendo ($R_{X,Y}<0$).


Como en el caso poblacional, cuando uno de los vectores es constante, la correlación es igual a 0.



Con R, la correlación de Pearson de dos vectores se puede calcular con la función `cor`. Por ejemplo, la correlación del Pearson de los vectores `BMI` y `Chol` se obtiene con
```{r}
cor(BMI,Chol)
```


Veamos que su cuadrado es igual al $R^2$ de la regresión lineal de `Chol` en función de `BMI`:
```{r}
cor(BMI,Chol)^2
summary(lm(Chol~BMI))$r.squared
```

Para hacernos una idea de qué representa este valor de la correlación, veamos el gráfico de los puntos (BMI,Chol) con su recta de regresión lineal:

```{r}
plot(BMI,Chol,pch=20)
abline(lm(Chol~BMI),col="red",lwd=1.5)
```

Podemos observar como `Chol` tiende a crecer cuando `BMI` crece, pero los puntos (BMI,Chol) no tienden a estar sobre una recta.

[*Cuidado*]{style="color: red;"}
Es conveniente acompañar el cálculo de correlación o covarianza de dos vectores $x,y$ con un gráfico de los puntos $(x_i,y_i)$, porque conjuntos muy diferentes de puntos pueden dar lugar a la misma correlación.


Un ejemplo clásico de este hecho son los cuatro conjuntos de datos $(x_{1,i},y_{1,i})_{i=1,\ldots,11}$, $(x_{2,i},y_{2,i})_{i=1,\ldots,11}$,  $(x_{3,i},y_{3,i})_{i=1,\ldots,11}$,  $(x_{4,i},y_{4,i})_{i=1,\ldots,11}$ que forman el *dataframe* `anscombe` de R:

```{r}
str(anscombe)
```

Las correlaciones de los cuatro pares de vectores son muy parecidas:

```{r}
cor(anscombe$x1,anscombe$y1)
cor(anscombe$x2,anscombe$y2)
cor(anscombe$x3,anscombe$y3)
cor(anscombe$x4,anscombe$y4)
```


Pero si los dibujamos veréis que los cuatro conjuntos de puntos son muy diferentes:

```{r,eval=FALSE}
plot(anscombe$x1,anscombe$y1,pch=20,main="Conjunto de datos 1",cex=1.25)
abline(lm(y1~x1,data=anscombe),col="red",lwd=1.5)
plot(anscombe$x2,anscombe$y2,pch=20,main="Conjunto de datos 2",cex=1.25)
abline(lm(y2~x2,data=anscombe),col="red",lwd=1.5)
plot(anscombe$x3,anscombe$y3,pch=20,main="Conjunto de datos 3",cex=1.25)
abline(lm(y3~x3,data=anscombe),col="red",lwd=1.5)
plot(anscombe$x4,anscombe$y4,pch=20,main="Conjunto de datos 4",cex=1.25)
abline(lm(y4~x4,data=anscombe),col="red",lwd=1.5)
```
```{r,echo=FALSE,fig.width=10,out.width="90%"}
par(mfrow=c(2,2))
plot(anscombe$x1,anscombe$y1,pch=20,main="Conjunto de datos 1",cex=1.25)
abline(lm(y1~x1,data=anscombe),col="red",lwd=1.5)
plot(anscombe$x2,anscombe$y2,pch=20,main="Conjunto de datos 2",cex=1.25)
abline(lm(y2~x2,data=anscombe),col="red",lwd=1.5)
plot(anscombe$x3,anscombe$y3,pch=20,main="Conjunto de datos 3",cex=1.25)
abline(lm(y3~x3,data=anscombe),col="red",lwd=1.5)
plot(anscombe$x4,anscombe$y4,pch=20,main="Conjunto de datos 4",cex=1.25)
abline(lm(y4~x4,data=anscombe),col="red",lwd=1.5)
par(mfrow=c(1,1))
```

Ejemplos más espectaculares se pueden obtener con las funciones del paquete **datasaurus**, que permiten crear conjuntos de puntos de "formas" diferentes con los mismos estadísticos, y en particular la misma correlación. Empleando este paquete, hemos creado dos pares de vectores de datos `dino` y `star`, que hemos recogido en la siguiente [tabla de datos](https://github.com/AprendeR-UIB/MatesII/blob/master/Dades/Datasaurus.txt). Esta tabla de datos tiene tres variables: una variable `dataset` que indica el conjunto de datos, y las variables `x` e `y` que dan las coordenadas de los puntos que forman cada conjunto de datos. Comprobaremos que los dos pares de vectores de datos tienen el mismo coeficiente de correlación (al menos hasta la séptima cifra decimal) y los dibujaremos.


```{r}
datasaure=read.table("https://raw.githubusercontent.com/AprendeR-UIB/MatesII/master/Dades/Datasaurus.txt",header=TRUE,sep="\t")
str(datasaure)
dino=datasaure[datasaure$dataset=="dino",2:3]
star=datasaure[datasaure$dataset=="star",2:3]
cor(dino$x,dino$y)
cor(star$x,star$y)
```

```{r,eval=FALSE}
plot(dino,pch=20,cex=1.25)
plot(star,pch=20,cex=1.25)
```

```{r,echo=FALSE,fig.width=10,out.width="90%",fig.asp=0.5}
par(mfrow=c(1,2))
plot(dino,pch=20,cex=1.25)
plot(star,pch=20,cex=1.25)
par(mfrow=c(1,1))
```


Ahora supongamos que tenemos una tabla de datos numéricos de la forma
$$
\begin{array}{cccc}
X_1 & X_2 & \ldots & X_p\\ \hline
x_{1 1} & x_{1 2} &\ldots & x_{1 p}\\
x_{2 1} & x_{2 2} &\ldots & x_{2 p}\\
\vdots & \vdots   &   \ddots    &\vdots\\ 
x_{n 1} & x_{n 2} &\ldots & x_{n p}
\end{array}
$$

donde cada columna representa los valores de cierta variable $X_i$ y cada fila un individuo de una muestra de la población, de forma que la entrada $x_{ij}$ de esta tabla es el valor de $X_j$ sobre el individuo $i$-ésimo de la muestra.

Su *matriz de correlaciones de Pearson* es la matriz simétrica
$$
\begin{pmatrix}  
 1 & R_{X_1,X_2} & \ldots & R_{X_1,X_p}\\
 R_{X_2,X_1} & 1 & \ldots & R_{X_2,X_p}\\
  \vdots & \vdots  &  \ddots      & \vdots\\
R_{X_p,X_1} & R_{X_p,X_2} & \ldots & 1
\end{pmatrix}
$$


Esta matriz de correlaciones se calcula con la función `cor` aplicada a la matriz o el data frame de variables numéricas que almacena la tabla de datos. Por ejemplo, la matriz de correlaciones de Pearson de la tabla de datos `DF` del Ejemplo que venimos trabajando es

```{r}
cor(DF)
```

### Estimación

Las covarianzas de dos vectores obtenidos midiendo dos variables $X,Y$ sobre una muestra aleatoria simple de sujetos de una población estiman la covarianza poblacional de las variables $X,Y$ que han producido los vectores:

* La covarianza muestral $\widetilde{S}_{X,Y}$ siempre es un estimador no sesgado de la covarianza poblacional $\sigma_{X,Y}$

* La covarianza ${S}_{X,Y}=\frac{n-1}{n}\widetilde{S}_{X,Y}$ es un estimador sesgado de la covarianza poblacional $\sigma_{X,Y}$, con sesgo que tiende a 0, y es más eficiente que $\widetilde{S}_{X,Y}$

* La covarianza $S_{X,Y}$ es el estimador máximo verosímil de $\sigma_{X,Y}$ cuando la distribución conjunta de las variables $X,Y$ es la [normal bivariante](https://es.wikipedia.org/wiki/Distribuci%C3%B3n_normal_multivariada) que estudiaremos próximamente.


La correlación de Pearson de dos vectores obtenidos midiendo dos variables continuas $X,Y$ sobre una muestra aleatoria simple de sujetos de una población estima la correlación poblacional de las variables $X,Y$. En concreto:

* $R_{X,Y}$ es un estimador máximo verosímil de $\rho_{X,Y}$ cuando la distribución conjunta de $X,Y$ se normal bivariante. se un estimador sesgado, pero su sesgo tiende a 0.


### Correlación de Spearman


La correlación de Pearson mide específicamente la tendencia de dos variables continuas a depender linealmente la una de la otra. Si no esperamos que esta dependencia lineal exista, o si nuestras variables son discretas o simplemente ordinales, emplear la correlación de Pearson para analizar la relación entre dos variables no es lo más adecuado. Entre las propuestas alternativas, la más popular es la **correlación de Spearman**.

Intuitivamente, la correlación de Spearman mide la tendencia que si $x_i>x_j$, pase que $y_i>y_j$. Su valor es 1 si, para todos $y,j$,
$$
x_i>x_j\Longleftrightarrow y_i>y_j
$$
y su valor es  -1 si, para todo $i,j$, 
$$
x_i>x_j\Longleftrightarrow y_i<y_j
$$
Cuanto más se acerca la correlación de Spearman a 1 (o -1), para más parejas de índices $(i,j)$ tales que $x_i>x_j$ se tiene que $y_i>y_j$ ($y_i<y_j$, si se acerca a -1).



Formalmente, la correlación de Spearman de dos vectores $X$ e $Y$ se define como la correlación de Pearson de los vectores de **rangos** de $X$ e $Y$. El vector de rangos de un vector $X$ se obtiene sustituyendo cada valor de $X$ por su posición en el vector ordenado de menor a mayor, y en caso de empates asignando a grupos de valores empatados la media de las posiciones que ocuparían. Por ejemplo, el vector de rangos de

$$
x=(4,5,1,5,1,3,4,4)
$$
es
$$
(5,7.5,1.5,7.5,1.5,3,5,5)
$$
¿Cómo hemos calculado este vector?

1. Primero asignamos a cada valor del vector su posición si estuvieran ordenados de menor a mayor, y en caso de empate por ahora los ordenaremos de izquierda a derecha:
$$
\begin{array}{r|cccccccc}
x & 4& 5 & 1 & 5 & 1 & 3 & 4 & 4\\ \hline
\text{Posició} & 4 & 7 & 1 & 8 & 2 & 3 & 5 & 6\\
\end{array}
$$
2. Ahora, para asignar los rangos finales:

* El rango de los dos elementos 1 de $x$ es la media de las posiciones 1, 2 del vector ordenado: 1.5.
* Como que solo hay un 3 en $x$, su rango es su posición en el vector ordenado: 3.
* El rango de los tres elementos 4 es la media de las posiciones 4, 5 y 6 del vector ordenado: 5.
* Finalmente, el rango de los dos elementos 5 es la media de las posiciones 7, 8 del vector ordenado: 7.5.
    
$$
\begin{array}{r|cccccccc}
x & 4& 5 & 1 & 5 & 1 & 3 & 4 & 4\\ \hline
\text{Posició} & 4 & 7 & 1 & 8 & 2 & 3 & 5 & 6\\ \hline
\text{Rang} & 5 & 7.5 & 1.5 & 7.5 & 1.5 & 3 & 5 & 5
\end{array}
$$    


Por cierto, con R el vector de rangos se calcula con la función `rank`:


```{r}
rank(c(4,5,1,5,1,3,4,4))
```



Con R, la correlación de Spearman se calcula directamente con la función `cor` especificando el parámetro `method="spearman"`. (El valor por defecto del parámetro `method` es `"pearson"` y por eso no lo indicamos cuando calculamos la correlación de Pearson.)

[*Ejemplo*]{style="color: blue;"}
Consideremos los vectores `BMI` y `Chol` del ejemplo que venimos trabajando. Lo primero que haremos será calcular sus vectores de rangos:
  
  

$$
\begin{array}{|c|c||c|c|}
\hline
 BMI & \text{Rangs} & Chol&  \text{Rangs}
\\\hline\hline
`r DF[1,1]`& `r rank(DF$BMI)[1]` & `r DF[1,2]`& `r rank(DF$Chol)[1]` \\
`r DF[2,1]`&`r rank(DF$BMI)[2]` & `r DF[2,2]` & `r rank(DF$Chol)[2]`\\
`r DF[3,1]`&`r rank(DF$BMI)[3]` & `r DF[3,2]`& `r rank(DF$Chol)[3]` \\
`r DF[4,1]`&`r rank(DF$BMI)[4]` & `r DF[4,2]`&  `r rank(DF$Chol)[4]`\\ 
`r DF[5,1]`&`r rank(DF$BMI)[5]` & `r DF[5,2]`&  `r rank(DF$Chol)[5]`\\
`r DF[6,1]`&`r rank(DF$BMI)[6]` & `r DF[6,2]`&  `r rank(DF$Chol)[6]`\\\hline
\end{array}
$$


Por tanto, la correlación de Spearman de 
$$
\mathit{BMI}=(`r BMI`)\mbox{ i }\mathit{Chol}=(`r Chol`)
$$
es la correlación de Pearson de
$$
(`r rank(BMI)`)\mbox{ i }(`r rank(Chol)`)
$$

Comprobémoslo:

```{r}
cor(BMI,Chol,method="spearman")
cor(c(1,4.5,6,4.5,3,2),c(1,2,5,6,3.5,3.5))
```


## Contrastes de correlación

En un *contraste de correlación* de dos variables poblacionales continuas $X$ e $Y$, la hipótesis nula es que no hay correlación entre las dos variables, lo cual traduce que no hay ninguna relación entre ellas.
$$
\left\{
\begin{array}{ll}
H_0: & \rho_{XY}=0\\
H_1: & \rho_{XY}> 0\text{ o }\rho_{XY}< 0\text{ o }\rho_{XY}\neq 0
\end{array}\right.
$$


>Si en un contraste de correlación rechazamos la hipótesis nula, en particular concluimos que las variables $X$ e $Y$ son dependientes (porque si fueran independientes, su correlación seria 0).



No explicaremos cómo se hace a mano este contraste ni qué hipótesis tienen que satisfacer las variables poblacionales para que el resultado sea fiable. Si estáis interesados en el detalle, podéis consultar la [correspondiente entrada de la Wikipedia](https://en.wikipedia.org/wiki/pearson_correlation_coefficient#Testing_using_Student's_t-distribution). Simplemente tenéis que saber que se efectúa con la función `cor.plot`. Su sintaxis es similar a la de las otras funciones que efectúan contrastes.

[*Ejemplo*]{style="color: blue;"}
Queremos contrastar si hay correlación positiva entre el BMI y el nivel de colesterol de un adulto sano, con un nivel de significación del 5%.


*Variables poblacionales de interés*:

* $\mathit{BMI}$: "Tomamos un adulto sano y anotamos su BMI"
* $\mathit{Chol}$: "Tomamos un adulto sano y anotamos el nivel de colesterol en mg/l"

*Contrast*:
$$
\left\{
\begin{array}{ll}
H_0: & \rho_{\textit{BMI,Chol}}=0\\
H_1: & \rho_{\textit{BMI,Chol}}>0
\end{array}\right.
$$

Empleamos las muestras de `BMI` y `Chol` del Ejemplo que venimos trabajando.

```{r}
cor.test(BMI,Chol,alternative="greater")
```

*Conclusión*: No hemos obtenido evidencia estadísticamente significativa que el BMI y el nivel de colesterol de un adulto sano tengan correlación positiva (test de correlación, p-valor 0.06, IC para $\rho$ 95%  [-0.086 a 1]).


[*Cuidado*]{style="color: red;"}
La conclusión es que no hemos obtenido evidencia que el BMI y el nivel de colesterol *tengan correlación positiva* **en la población de los adultos sanos**. No en nuestra muestra, que sí que ha dado correlación positiva. Recordad que los contrastes siempre se refieren a la población.



## Gráficos para datos multidimensionales 

### Otro gráfico para datos bivariantes

El boxplot bivariante es un gráfico de dispersión que incluye dos elipses estimadas, la interior que contiene aproximadamente el 50% de los datos y la exterior que contiene aproximadamente el 95% de los datos. Este tipo de gráficos nos ayuda a localizar datos atípicos. Veamos un ejemplo con los datos de los pingüinos.

```{r, message=FALSE}
library(tidyverse)
library(MVA)
library(palmerpenguins)

a2<- penguins %>%
  select(body_mass_g,bill_length_mm) %>%
  na.omit %>% as.matrix()

bvbox(a2,xlab = "Peso del pingüino en gr", 
           ylab = "Longitud del pico en mm",
      pch = 19, cex = 1.25, col = "red")
```

Las dos rectas dentro de las elipses del gráfico anterior son estimaciones de la recta de regresión.La recta más oscura es la habitual de mínimos cuadrados utilizando todas las observaciones. La recta más clara es una estimación más robusta que reduce la influencia de cualquier valor extremo. 


### Matriz de dispersión

Es posible ver los gráficos de dispersión por pares entre diversas variables cuantitativas utilizando una matriz. Se puede generar utilizando la función "pairs()" de R base. Veamos un ejemplo con nuestros datos de pingüinos.

```{r}
a<-penguins %>%
  select(3:7) %>%
  na.omit

pairs(a,
      col = c("red", "blue")[as.integer(a$sex)], 
      pch = 18)
```

Existen muchas otras funciones para crear matrices de gráficos de dispersión. Una que nos gusta es `ggpairs()` del paquete `GGally`. 

```{r, message=FALSE, warning=FALSE}
library(GGally)
ggpairs(a)
```

Por defecto, la diagonal principal de la matriz contiene la curva de densidad para cada variable. Por debajo de la diagonal principal se muestran los gráficos de dispersión y la correlación (para las variables cuantitativas) o el cruce entre variables si se trata de una variable categórica.

### Caras de Chernoff

Cada variable del conjunto de datos se usa para representar una característica de la cara. Chernoff usó hasta 18 variables para representar diferentes rasgos faciales como cabeza, nariz, ojos, cejas, boca y orejas. El gráfico de Chernoff tiene como ventaja la facilidad humana para reconocer patrones de caras. El inconveniente es que la representación es muy dependiente de las variables escogidas para representar cada rasgo. Por ejemplo: la boca y la forma de la cabeza son rasgos más llamativos que las orejas o la longitud de la nariz, por tanto, el mismo conjunto puede sugerir distintos patrones de similitud entre las observaciones. Veamos un ejemplo.


```{r}
library("aplpack")
b<- penguins %>% 
      filter(species=="Adelie", 
             island=="Torgersen",
             sex=="female",
             year==2007)

faces(b[,3:6],face.type = 1, scale =TRUE,print.info = TRUE)
```

Alternativamente, podemos representar cada elemento por una figura geométrica, donde las similitudes entre figuras indican las similitudes entre los elementos.

```{r}
stars(a[,1:4], key.loc = c(44, 1.5),cex=0.45,
      labels=row.names(a[,1:4]), draw.segments=TRUE)
```

Por supuesto que `ggplot2`  nos da opciones más modernas de este tipo de gráficos. Ver, por ejemplo:

* [ggradar](https://www.r-bloggers.com/2016/10/the-grammar-of-graphics-and-radar-charts/)

* [fmsb library](https://www.r-bloggers.com/2023/10/creating-interactive-radar-charts-in-r-with-the-fmsb-library/)


### Matriz de correlaciones

Las correlaciones por pares entre varias variables se muestran visualmente mediante un mapa de calor de la matriz de correlaciones. La función `ggcorrplot()` del paquete "ggcorrplot" de R puede utilizarse para construirlo. A continuación se muestra un ejemplo que utiliza el conjunto de datos de los pingüinos:

```{r}
library(ggcorrplot)
penguins %>%
  select(3:6) %>%
  na.omit(.) %>% 
  cor(.) %>% 
  ggcorrplot(., hc.order = TRUE,
		type = "lower",
		colors = c("#6D9EC1",
					"yellow", "#E46726"))
```


En el gráfico, el color naranja denota correlaciones positivas y el gris correlaciones negativas. Al especificar `hc.order`,las variables también se ordenan.


### Gráficos de mosaico

Hasta ahora, hemos explorado métodos para visualizar relaciones entre variables cuantitativas. Pero, ¿qué hacemos si hay más de dos variables categóricas?

Un método consiste en utilizar gráficos de mosaico, en los que las frecuencias de una tabla de contingencia multidimensional se representan mediante regiones rectangulares anidadas que son proporcionales a su frecuencia de celda. La función `mosaicplot()` de la librería `vcd` proporciona características más amplias que la de R base. 

Si se añade la opción opción `shade=TRUE` colorea la figura basándose en los residuos estandarizados de un modelo loglineal para la tabla. La idea central detrás de un modelo loglineal es modelar las probabilidades de ocurrencia de cada categoría en la tabla de contingencia y el modelo se ajusta utilizando técnicas de regresión logística.


```{r, message=FALSE}
library(vcd)
a<- penguins %>%
  select(island,species,sex) %>%
  na.omit()
a2<- table(a)

mosaicplot(a2,shade=TRUE, main="")
```

