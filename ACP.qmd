## Análisis de Componentes Principales

## Introducción

Cuando nos enfrentamos a problemas que se desarrollan en espacios muestrales de altas dimensiones, puede interesar considerar el estudio de dichos problemas en espacios de menor dimensión.

El análisis de componentes principales, PCA por sus siglas en inglés (**P**rincipal **C**omponent **A**nalysis), es uno de los métodos más populares del análisis multivariado.

El PCA se utiliza cuando deseamos obtener una representación en menor dimensión para un conjunto de variables cuantitativas correlacionadas y queremos expresar la información importante como un conjunto de pocas variables nuevas llamadas **componentes principales**. Estas componentes se corresponden con una combinación lineal de las variables originales. 

Dado que la información de un conjunto de datos se corresponde a la variación total que contiene, el objetivo del PCA es identificar direcciones (o componentes principales) a lo largo de las cuales la variación en los datos es máxima.

La **reducción de la dimensionalidad**  consiste en  describir con cierta precisión los valores de las $p$ variables por un pequeño subconjunto $r<p$ de ellas con una pérdida mínima de información. Por lo tanto, proyectaremos la muestra original en el nuevo subespacio pero conservando algunas características. En particular, vamos a realizar un  ajuste ortogonal por mínimos cuadrados. 


Algunos ámbitos importantes en los que es frecuente la utilización de la Reducción de la dimensionalidad podrían ser:

* **Reconocimiento facial**: Al partir de una proyección facial de tamaño $M \times N$ píxeles puede resultar complicado el estudio completo de dicha imagen, puesto que da lugar a un vector con dimensión excesivamente alta. De modo que debemos reducir la dimensionalidad pero de forma que nos permita generar un sistema de clasificación facial de un individuo entre el resto de la población.


<br>

* **Modelización de secuencias genómicas**: Considera una proteína, formada por una secuencia de aminoácidos donde es posible encontrar hasta 20 tipos diferentes, su longitud puede llegar a ser de decenas hasta cientos de decenas de aminoácidos. De modo que, las proteínas con igual estructura pueden ser agrupadas en familias y el conjunto de familias proteicas distintas tendrá una dimensión inferior al conjunto de todas las proteínas. Pudiendo así encontrar de forma menos compleja propiedades particulares o incluso identificar nuevos miembros de una familia.



### Matriz (tabla) de datos

Supongamos que disponemos de una tabla de datos con los valores de $p$-variables en $n$ elementos de una población arreglados en una matriz $\mathbf{X}$ de la siguiente forma:

| ID | $\bf{x}_1$ | $\bf{x}_2$ | $\ldots$ | $\bf{x}_p$ | $\bf{v}_1$ | $\bf{v}_2$ |
|:--------:|:---------:|:---------:|:------------:|:---------:|:---------:|:---------:|
| $1$      | $x_{11}$  | $x_{12}$  | $\ldots$     | $x_{1p}$  | $v_{11}$  | $v_{12}$  |
| $2$      | $x_{21}$  | $x_{22}$  | $\ldots$     | $x_{2p}$  | $v_{21}$  | $v_{22}$  |
| $3$      | $x_{31}$  | $x_{32}$  | $\ldots$     | $x_{3p}$  | $v_{31}$  | $v_{32}$  |
| $\vdots$ | $\vdots$  | $\vdots$  | $\vdots$     | $\ddots$  | $\vdots$  | $\vdots$  |
| $n$      | $x_{n1}$  | $x_{n2}$  | $\ldots$     | $x_{np}$  | $v_{n1}$  | $v_{n2}$  |



* Donde las variables $x_1,\ldots, x_n$ describen a los $n$ individuos observados.

* Las variables $v_1$, $v_2$ son de perfil (o explicativas) y ayudan a
interpretar la variabilidad de los datos.





El objetivo del análisis es la **reducción de la dimensionalidad**. Buscamos  un **espacio de variables más reducido y fácil de interpretar**.


El problema es que si reducimos el número de variables es posible que "perdamos parte toda la variabilidad de los datos originales".


Así la idea básica es consentir una pérdida de información para lograr una ganancia en la significación.



```{r echo=FALSE, out.width = "100%",fig.align='center'}
knitr::include_graphics("Figuras/acp_1.PNG")
```

<!-- ## Análisis Factorial -->

<!-- * Algunos autores consideran el ACP como una parte del Análisis Factorial. -->


<!-- * En las técnicas de Análisi Factorialse postula que **la variabilidad total** se puede explicar mediante distintos tipos de factores: -->

<!--  * ["factores comunes"]{style="color: blue;"} subyacentes: $F_i$. -->

<!--  * ["factores específicos"]{style="color: blue;"} de las variables: $E_i$. -->

<!--  * ["Error o fluctuaciones aleatorias"]{style="color: blue;"}: $A_i$. -->


<!-- $$X_1=\alpha_{1 1} F_1+ \alpha_{1 2} F_2+\cdots +\alpha_{1 k} F_k+ E_1+ A_1$$ -->
<!-- $$X_2=\alpha_{2 1} F_1+ \alpha_{2 2} F_2+\cdots +\alpha_{2 k} F_k+ E_2+ A_2$$ -->
<!-- $$\vdots \; \; \; \; \; \; \; \; \vdots \; \; \; \; \; \; \; \; \vdots $$ -->
<!-- $$X_p=\alpha_{p 1} F_1+ \alpha_{p 2} F_2+\cdots +\alpha_{p k} F_k+ E_p+ A_p$$ -->


<!-- * Podríamos decir que en un **Análisis Factorial se fija a priori la -->
<!-- cantidad de varianza de cada variable que debe quedar interpretada por los factores comunes**.  -->


<!-- * Este valor recibe el nombre de **comunalidad** y se suele representar como $h_i^2$. -->


<!-- Utilizaremos la siguiente notación: -->


<!-- * La comunalidad $h_i^2$, de la variable $X_i$,  es la varianza explicada por $F_1,F_2,\ldots F_k.$ -->


<!-- *  La diferencia $s_i^2-h_i^2$  es la varianza  de la variable  $X_i$    que  explican los factores específicos y aleatorios. -->

<!-- <br> -->

<!-- $$\color{red}{\mbox{Var. observada = Var. común + Var. específica y aleatorios}}.$$ -->



<!-- Todos los factores son comunes: -->


<!-- $$X_1=\alpha_{1 1} CP_1+ \alpha_{1 2} CP_2+\cdots +\alpha_{1 p} CP_p$$ -->
<!-- $$X_2=\alpha_{2 1} CP_1+ \alpha_{2 2} CP_2+\cdots +\alpha_{2 p} CP_p$$ -->
<!-- $$\vdots \; \; \; \; \; \; \; \; \vdots \; \; \; \; \; \; \; \; \vdots $$ -->
<!-- $$X_p=\alpha_{p 1} CP_1+ \alpha_{p 2} CP_2+\cdots +\alpha_{p p} CP_p$$ -->

<br> <br>
<!-- Se trata de encontrar unas nuevas variables $CP_1,\ldots CP_p$, a las que -->
<!-- llamaremos -->
<!-- componentes principales, de forma que: -->



<!-- * Se cumplan las condiciones anteriores. -->

<!-- * El origen de las variables esté situado en el vector de medias o centro de gravedad de las observaciones. -->

<!-- * Sean incorreladas entre si, es decir,  $Cor(CP_i,CP_j)=0$ para $i=j, \;\; i,j=1\ldots,p$. -->

<!-- * Se cumple que $Var(CP_1)\geq Var(CP_2)\geq\cdots\geq Var(CP_p)$ y estas varianzas sean máximas. -->

<!-- * Se conserva la varianza total (inercia) de la nube de puntos. -->

<!-- ```{r echo=FALSE, out.width = "40%",fig.align='center'} -->
<!-- knitr::include_graphics("Figuras/acp_2.PNG") -->
<!-- ``` -->

### Enfoque geométrico


<!-- * **Sobre los datos centrados**: A cada variable se le resta su media -->
<!-- $x_i-\overline{x}_i$; de modo que las variables centradas tienen media cero y la misma varianza que las variables originales. Se le suele llamar **ACP de covarianzas**.    -->


<!-- * **Sobre los datos tipificados**:  $\frac{x_{i}-\overline{x}_i}{s_i}$. Las variables tipificadas tienen media cero y varianza 1.  Se le suele llamar **ACP de correlaciones o normado**. -->

<!-- Dada una matriz de datos $\mathbf{X}_{n\times p}$, denotemos por $\tilde{\mathbf{X}}$ la matriz de datos centrada. Recordad que:  -->


<!-- * El estimador sesgado de la matriz de covarianzas de $\mathbf{X}$ viene dada por  -->

<!-- $$\mathbf{S}=1/n \; \tilde{\mathbf{X}}^t\tilde{\mathbf{X}}$$  -->

<!-- * Si llamamos $\mathbf{Z}$ a la tabla de los datos tipificados, la matriz de correlaciones viene dada por:  -->

<!-- $$\mathbf{R}=1/n \; \mathbf{Z}^t\mathbf{Z}$$  -->


<!-- **Propiedades** -->

<!-- * Los **componentes principales vienen determinadas por los vectores propios ortonormales** -->
<!-- (ordenados de mayor a menor valor propio) de la matriz de covarianzas (para datos centrados) y de la matriz de correlaciones (para los datos tipificados). -->


<!-- * Así en el ACP de covarianzas cada variable interviene con su propia -->
<!-- varianza mientras que el ACP de correlaciones todas las variables tienen varianza 1. -->

<!-- **Interpretación geométrica** -->

Supongamos que  $p=2$ y que la nube de puntos de nuestra matriz de datos es la de la siguiente figura:
```{r ,fig=TRUE,fig.align="center",fig.width=7,fig.height=7,echo=FALSE}
n=20
m=10
dades=read.table("datos/dades.txt")
dades=as.matrix(dades)
Hn=matrix(-1/n,n,n)
diag(Hn)=rep(1-1/n,n)
dades.cent=Hn%*%dades
plot(dades.cent,asp=1,xlab="X",ylab="Y",xlim=c(-10,10),ylim=c(-5.5,5.5))
```

La siguiente figura muestra las dos  **componentes principales**, es decir, las direcciones de las proyecciones que tienen máxima variabilidad.

```{r inter2, fig=TRUE,fig.align="center",fig.width=8,fig.height=8,echo=FALSE}
# n=20
# m=10
# dades=read.table("dades.txt")
# dades=as.matrix(dades)
# Hn=matrix(-1/n,n,n)
# diag(Hn)=rep(1-1/n,n)
# dades.cent=Hn%*%dades
# plot(dades.cent,asp=1,xlab="X",ylab="Y",xlim=c(-10,10),ylim=c(-5.5,5.5))
# estudi.acp=prcomp(dades,scale.=FALSE)
# direccions=estudi.acp$rotation
# m.cov=t(dades.cent)%*%dades.cent
# vaps=eigen(m.cov)$values
# veps=eigen(m.cov)$vectors
# mag=10
# lines(c(0,mag*direccions[1,1]),c(0,mag*direccions[2,1]),col="red",lwd=3)
# lines(c(0,mag*direccions[1,2]),c(0,mag*direccions[2,2]),col="red",lwd=3)
# lines(c(0,-mag*direccions[1,1]),c(0,-mag*direccions[2,1]),col="red",lwd=3)
# lines(c(0,-mag*direccions[1,2]),c(0,-mag*direccions[2,2]),col="red",lwd=3)
# xaux=-6
# text(xaux,direccions[2,1]*xaux/direccions[1,1]-1,"CP1",cex=1)
# xaux=-1.5
#text(xaux,direccions[2,2]*xaux/direccions[1,2]-2,"CP2",cex=1)
```

```{r echo=FALSE, out.width = "80%",fig.align='center'}
knitr::include_graphics("Figuras/eje_CP.PNG")
```

Si proyectamos en la dirección de la **primera componente** obtendremos las proyecciones siguientes (en color azul):


```{r inter3,fig=TRUE,fig.align="center",fig.width=8,fig.height=8,echo=FALSE}
# n=20
# m=10
# dades=read.table("dades.txt")
# dades=as.matrix(dades)
# Hn=matrix(-1/n,n,n)
# diag(Hn)=rep(1-1/n,n)
# dades.cent=Hn%*%dades
# plot(dades.cent,asp=1,xlab="X",ylab="Y",xlim=c(-10,10),ylim=c(-5.5,5.5))
# estudi.acp=prcomp(dades,scale.=FALSE)
# direccions=estudi.acp$rotation
# m.cov=t(dades.cent)%*%dades.cent
# vaps=eigen(m.cov)$values
# veps=eigen(m.cov)$vectors
# mag=10
# lines(c(0,mag*direccions[1,1]),c(0,mag*direccions[2,1]),col="red",lwd=3)
# lines(c(0,mag*direccions[1,2]),c(0,mag*direccions[2,2]),col="red",lwd=3)
# lines(c(0,-mag*direccions[1,1]),c(0,-mag*direccions[2,1]),col="red",lwd=3)
# lines(c(0,-mag*direccions[1,2]),c(0,-mag*direccions[2,2]),col="red",lwd=3)
# xaux=-6
# text(xaux,direccions[2,1]*xaux/direccions[1,1]-1,"CP1",cex=1)
# xaux=-1.5
# text(xaux,direccions[2,2]*xaux/direccions[1,2]-2,"CP2",cex=1)
# proj.ort=function(x0,y0,v1,v2,b){m=v2/v1;x1=(m*y0+x0-m*b)/(m^2+1);
#                         y1=(m^2*y0+m*x0+b)/(m^2+1);return(c(x1,y1))}
# #projecció sobre CP1
# for (i in 1:dim(dades)[1]){
#   aux=proj.ort(dades.cent[i,1],dades.cent[i,2],veps[1,1],veps[2,1],0)
#   points(aux[1],aux[2],pch=20,col="blue",lwd=5)
#   lines(c(dades.cent[i,1],aux[1]),c(dades.cent[i,2],aux[2]),lty=2)
# }
```

```{r echo=FALSE, out.width = "80%",fig.align='center'}
knitr::include_graphics("Figuras/eje_CP_2.PNG")
```

<br><br>

Lo que significa  que la varianza de los puntos azules  es máxima; en el sentido de que cualquier  otra dirección o recta, las proyecciones sobre ésta tendrán a lo más igual varianza.

Los puntos  azules  representan las coordenadas que tienen los puntos de nuestra tabla de datos (centrada) tomando como eje de abcisas la **primera componente** $CP_1$.


Si proyectamos en la dirección de la **"segunda componente"**, obtendremos las proyecciones siguientes (en color verde):

```{r inter4.1,fig=TRUE,fig.align="center",fig.width=8,fig.height=8,echo=FALSE}
# n=20
# m=10
# dades=read.table("dades.txt")
# dades=as.matrix(dades)
# Hn=matrix(-1/n,n,n)
# diag(Hn)=rep(1-1/n,n)
# dades.cent=Hn%*%dades
# plot(dades.cent,asp=1,xlab="X",ylab="Y",xlim=c(-10,10),ylim=c(-5.5,5.5))
# estudi.acp=prcomp(dades,scale.=FALSE)
# direccions=estudi.acp$rotation
# m.cov=t(dades.cent)%*%dades.cent
# vaps=eigen(m.cov)$values
# veps=eigen(m.cov)$vectors
# mag=10
# lines(c(0,mag*direccions[1,1]),c(0,mag*direccions[2,1]),col="red",lwd=3)
# lines(c(0,mag*direccions[1,2]),c(0,mag*direccions[2,2]),col="red",lwd=3)
# lines(c(0,-mag*direccions[1,1]),c(0,-mag*direccions[2,1]),col="red",lwd=3)
# lines(c(0,-mag*direccions[1,2]),c(0,-mag*direccions[2,2]),col="red",lwd=3)
# xaux=-6
# text(xaux,direccions[2,1]*xaux/direccions[1,1]-1,"CP1",cex=1)
# xaux=-1.5
# text(xaux,direccions[2,2]*xaux/direccions[1,2]-2,"CP2",cex=1)
# proj.ort=function(x0,y0,v1,v2,b){m=v2/v1;x1=(m*y0+x0-m*b)/(m^2+1);
#                         y1=(m^2*y0+m*x0+b)/(m^2+1);return(c(x1,y1))}
# #projecció sobre CP2
# for (i in 1:dim(dades)[1]){
#   aux=proj.ort(dades.cent[i,1],dades.cent[i,2],veps[1,2],veps[2,2],0)
#   points(aux[1],aux[2],pch=20,col="green",lwd=5)
#   lines(c(dades.cent[i,1],aux[1]),c(dades.cent[i,2],aux[2]),lty=2)
# }
```

```{r echo=FALSE, out.width = "80%",fig.align='center'}
knitr::include_graphics("Figuras/eje_CP_3.PNG")
```

## Cálculo de las componentes

### Cálculo de la primera componente

La primera componente principal se define como la combinación lineal de las variables originales que tiene varianza máxima. Los valores en esta primera componente de los $n$ individuos se representarán por un vector $\mathbf{z_1}$ dado por
$$\mathbf{z_1}=\mathbf{Xa_1}.$$
Si las variables originales tienen media cero, $\mathbf{z_1}$ también tendrá media nula. Su varianza será

\begin{equation} \tag{1}
Var(\mathbf{z_1})=\frac{1}{n}\mathbf{z_1^t}\mathbf{z_1}=\frac{1}{n}\mathbf{a_1^t X^t}\mathbf{Xa_1}=\mathbf{a_1^t S}\mathbf{a_1}
\end{equation}

donde $\mathbf{S}$ es la matriz de varianzas-covarianzas de las observaciones. 

Es obvio que podemos maximizar la varianza tanto como queramos aumentando el módulo del vector $\mathbf{a_1}$. Para que maximizar (1) tenga solución debemos imponer una restricción al módulo del vector $\mathbf{a_1}$, y, sin pérdida de generalidad, impondremos que $\mathbf{a_1^t}\mathbf{a_1}=1$. Introducimos esta restricción mediante el multiplicador de Lagrange:
$$M=\mathbf{a_1^t}S\mathbf{a_1}-\lambda(\mathbf{a_1^t}\mathbf{a_1}-1)$$
y maximizamos esta expresión derivando respecto a las componentes de $\mathbf{a_1}$ e igualando a cero. Entonces
$$\frac{\partial M}{\partial \mathbf{a_1}}=2\mathbf{Sa_1}-2 \lambda \mathbf{a_1}=0,$$

cuya solución es:

\begin{equation} \tag{2}
\mathbf{Sa_1}= \lambda \mathbf{a_1},
\end{equation}

que implica que $\mathbf{a_1}$ es un vector propio de la matriz $\mathbf{S}$, y $\lambda$ su correspondiente valor propio. 

Para determinar qué valor propio de $\mathbf{S}$ es la solución de (2), multiplicamos por la izquierda por $\mathbf{a_1^t}$ a esta ecuación,

\begin{equation}
\mathbf{a_1^t S a_1}= \lambda \mathbf{a_1^t a_1}= \lambda
\end{equation}

y **concluimos, por (1) que $\lambda$ es la varianza de $\mathbf{z_1}$**. Como esta es la cantidad que queremos maximizar, $\lambda$ será el mayor valor propio de la matriz $\mathbf{S}$. Su vector asociado, $\mathbf{a_1}$, define los coeficientes de cada variable en la primera componente principal.

## [*Ejemplo:* ]{style="color: blue;"}
Calculad la primera componente principal con los logaritmos del fichero `acciones.txt` que podéis descargad en Aula Digital. Las observaciones corresponden a distintas acciones que cotizan en el mercado español y las variables son: 

* `V1` la rentabilidad efectiva por dividendos,
* `V2` la proporción de beneficios que va a dividendos
* `V3` el ratio entre precio por acción y beneficios.

¿Cuál de las variables está mejor representada por la componente principal 1?

**Solución**

Cargamos los datos

```{r}
datos<-read.table("datos/acciones.txt")
n=dim(datos)[1]
n
```

La tabla presenta tres medidas de rentabilidad de 34 acciones en bolsa. Vamos a reescribirlas para ganar interpretabilidad. Llamamos $d$ a los dividendos por acción, $p$ al precio de la acción, $B$ al beneficio y $N$ al número de acciones. Entonces:

* `V1` es la rentabilidad efectiva por dividendos, es decir, dividendos repartidos por acción divididos por precio de la acción. $V1=\frac{d}{p}$.

* `V2` es la proporción de beneficios que va a dividendos. $V2=\frac{dN}{B}$.

* `V3` es el cociente entre precio y beneficio por acción. $V3=\frac{p}{B/N}=\frac{pN}{B}$

Exploramos los datos  

```{r, message=FALSE, echo=FALSE}
library(GGally)
ggpairs(datos)
```

Las densidades indican un alejamiento de la distribución normal para las tres variables. Las dos primeras sugieren la presencia de dos grupos de datos distintos (acciones con comportamientos distintos), y la tercera tiene una densidad muy asimétrica, con al menos un valor atípico.

Por la forma de cálculo de las variables, es lógico esperar alta correlación positiva entre `V1` y `v2`. La correlación negativa baja entre `V1` y `v3`, así como alta negativa entre `V2` y`v3`.


Ahora, estimamos la matriz de varianzas-covarianzas de las variables originales `V1`, `v2` y `V3`, con el estimador sesgado:

```{r}
S0=round(((n-1)/n)*cov(datos),2)
S0
```

Recordad que las densidades de las tres variables han mostrado una clara falta de normalidad y entre ellas hay relaciones no lineales. En estas condiciones, la matriz de varianzas-covarianzas no es un buen resumen de las relaciones de dependencia existentes.

Para tratar de resolver el problema anterior, podemos usar el logaritmo que es una de las transformaciones más utilizadas para datos positivos en los siguientes casos:

1. Los datos describen el tamaño de las cosas (renta de países o familias habitantes en las principales ciudades del mundo, tamaño de empresas, consumo de energía en hogares, etc), son generalmente muy asimétricas, pero se convierten en aproximadamente simétricas al expresar la variable en logaritmos.

2. Cuando las diferencias relativas entre los valores de la variable son importantes, conviene expresar las variables en logaritmos, ya que las diferencias entre logaritmos equivalen a diferencias relativas en la escala original.

3. **La variabilidad de las variables transformadas es independiente de las unidades de medida**. Para mostrar esta propiedad, supongamos que tenemos una sola variable aleatoria $X$ que transformamos con $Y = \log X$, la variable transformada tiene media $\mu_Y$ y varianza $\sigma^2_Y$. Si cambiamos las unidades de medida de $X$ multiplicando por una constante, $Z = kX$, entonces la variable $\log Z$ tiene media $Y + \log k$ y la misma varianza que la variable $\log X$. Por tanto, **al tomar logaritmos en las variables, las varianzas pueden compararse aunque los datos tengan distintas dimensiones**.

De acuerdo a los anterior, aplicamos una transformación logarítmica a nuestros datos, con lo cual, la matriz de varianzas-covarianzas de las variables transformadas, sería:

```{r}
datos_l=log(datos)
S=round(((n-1)/n)*cov(datos_l),2)
S
```

Observad que los logaritmos modifican mucho los resultados. **Los datos ahora son más homogéneos** y la variable de mayor varianza pasa a ser la primera, el logaritmo de la rentabilidad efectiva, mientras que la menor es la segunda, el logaritmo de la proporción de beneficios que va a dividendos. La relación entre el logaritmo del ratio precio/beneficios y la rentabilidad efectiva es negativa. Las otras relaciones son débiles. 

 
Calculamos los valores propios de la matriz de varianzas covarianzas de los datos transformados que son las raíces de la ecuación 

$$
\begin{equation} 
\begin{split}
|S-\lambda I| & =  \left| \begin{pmatrix}0.35 & 0.15 & -0.19\\ 0.15 & 0.13 & -0.03 \\ -0.19 & -0.03 & 0.16\end{pmatrix} -\begin{pmatrix} \lambda & 0 & 0\\ 0 & \lambda & 0 \\ 0 & 0 & \lambda\end{pmatrix}  \right| \\ \\
 & = 0.000382-0.0628\lambda+0.64 \lambda^2 -\lambda^3 =0
\end{split}
\end{equation}
$$

Buscamos las raíces de este polinomio son
 
```{r,warning=FALSE}
library(polynom)
p=polynomial(coef=c(0.00038,-0.0628,0.64,-1))
raices=round(solve(p),3)
```

Las raíces son $\lambda_1$=`r raices[3]`, $\lambda_2$=`r raices[2]` y $\lambda_3$=`r raices[1]`. 

El vector propio asociado a $lambda_1$ nos da los pesos de la primera componente principal. Para calcularlo manualmente, debemos resolver el sistema
$$S \mathbf{a_1}= \lambda_1 \mathbf{a_1}$$ 
que conduce a

$$
\begin{equation} 
\begin{split}
\begin{pmatrix}0.35 & 0.15 & -0.19\\ 0.15 & 0.13 & -0.03 \\ -0.19 & -0.03 & 0.16\end{pmatrix} \begin{pmatrix} a_{11} \\a_{12} \\ a_{131} \end{pmatrix} &=0.521 \cdot \begin{pmatrix} a_{11} \\a_{12} \\ a_{131} \end{pmatrix}
\end{split}
\end{equation}
$$
$$
\begin{equation} 
\begin{split}
\begin{pmatrix}-0.171 a_{11}+0.15 a_{12} -0.19 a_{13} \\ 0.15 a_{11}-0.391 a_{12} -0.03 a_{13}\\ -0.19 a_{11}-0.03 a_{12} -0.361 a_{13}\end{pmatrix}  &= \begin{pmatrix} 0 \\0 \\ 0 \end{pmatrix}
\end{split}
\end{equation}
$$
Este sistema es compatible indeterminado. Para encontrar una de las infinitas soluciones tomemos la primera variable como parámetro, $x$, y resolvemos el sistema en función de $x$. La solución es,

$$\{a_{11}=x,\; a_{12}=0.427x,\; a_{13}=-0.562x$$

El valor de $x$ lo obtenemos imponiendo que el vector tenga norma uno, con lo que resulta:

$$\mathbf{a_1}=\begin{pmatrix} -0.817 \\-0.349 \\ 0.459 \end{pmatrix}$$

Por lo tanto, la primera componente es
$$Z_1=-0.817 \log(d/p)-0.349 \log(dN/p)+0.459 \log(pN/B)$$


que indica que este primer componente depende básicamente de la rentabilidad por dividendos. Esta variable es la que mejor explica la variabilidad conjunta de las acciones.


## Cálculo de la segunda componente

Vamos a obtener el mejor plano de proyección de l matriz $\mathbf{X}$. Lo calcularemos estableciendo como función objetivo que la suma de las varianzas de $\mathbf{z_1}=\mathbf{Xa_1}$ y $\mathbf{z_2}=\mathbf{Xa_2}$
sean máximas, donde $\mathbf{a_1}$ y $\mathbf{a_2}$ son los vectores que definen el plano. La función objetivo será:

\begin{equation} \tag{3}
\phi=\mathbf{a_1^t S}\mathbf{a_1} + \mathbf{a_2^t S}\mathbf{a_2} - \lambda_1 (\mathbf{a_1^t}\mathbf{a_1}-1) - \lambda_2 (\mathbf{a_2^t}\mathbf{a_2}-1)
\end{equation}

que incorpora las restricciones de que las direcciones deben de tener módulo unitario. Derivando e igualando a cero:
$$\frac{\partial \phi}{\partial \mathbf{a_1}}=2\mathbf{Sa_1}-2 \lambda_1 \mathbf{a_1}=0$$

$$\frac{\partial \phi}{\partial \mathbf{a_2}}=2\mathbf{Sa_2}-2 \lambda_2 \mathbf{a_1}=0$$

La solución del sistema es:
\begin{equation} \tag{4}
\mathbf{Sa_1}= \lambda \mathbf{a_1},
\end{equation}
\begin{equation} \tag{5}
\mathbf{Sa_2}= \lambda \mathbf{a_2},
\end{equation}

que indica que $\mathbf{a_1}$ y $\mathbf{a_2}$ deben ser vectores propios de $\mathbf{S}$. 

Tomando los vectores propios de norma uno y sustituyendo en (3), se obtiene que, en el máximo, la función objetivo es
\begin{equation} \tag{6}
\phi=\lambda_1+\lambda_2
\end{equation}

es claro que $\lambda_1$ y $\lambda_2$ deben ser los dos valores propios mayores de la matriz $\mathbf{S}$ y $\mathbf{a_1}$ y $\mathbf{a_2}$ sus correspondientes vectores propios. 

Observad que la covarianza entre $\mathbf{z_1}$ y $\mathbf{z_2}$, dada
por $\mathbf{a_1^t S a_2}$ es cero ya que \mathbf{a_1^t a_2}=0, y las variables$\mathbf{z_1}$ y $\mathbf{z_2}$ estarán incorreladas.

Se puede demostrarse que si en lugar de maximizar la suma de
varianzas, que es la traza de la matriz de covarianzas de la proyección, se maximiza la varianza generalizada (el determinante de la matriz de covarianzas) se obtiene el mismo resultado.

### Generalización

Análogamente, el espacio de dimensión $r$ que mejor representa a los puntos viene definido por los vectores propios asociados a los $r$ mayores valores propios de $\mathbf{S}$. Estas direcciones se denominan direcciones principales de los datos y a las nuevas variables por ellas definidas componentes principales. En general, la matriz  $\mathbf{X}$ (y por tanto la  $\mathbf{S}$) tiene rango $p$, existiendo entonces tantas componentes
principales como variables que se obtendrán calculando los valores propios o raíces características, $\lambda_1, \ldots, \lambda_p$, de la matriz de varianzas y covarianzas de las variables, $\mathbf{S}$ , mediante:

\begin{equation} \tag{7}
|\mathbf{S}-\lambda\mathbf{I}|=0
\end{equation}

y sus vectores asociados son:
\begin{equation} \tag{8}
(\mathbf{S}-\lambda_i\mathbf{I})\mathbf{a_i}=0.
\end{equation}

Los términos $\lambda_i$ son reales, al ser la matriz $\mathbf{S}$ simétrica, y positivos, ya que $\mathbf{S}$ es definida positiva. 

Por ser $\mathbf{S}$ simétrica si $\lambda_j$ y $\lambda_h$ son dos raíces distintas sus vectores asociados son ortogonales. 

Si $\mathbf{S}$ fuese semidefinida positiva de rango $r < p$, lo que
ocurriría si $p−r$ variables fuesen combinación lineal de las demás, habría solamente $r$ raíces características positivas y el resto serían ceros.

Llamando $\mathbf{Z}$ a la matriz cuyas columnas son los valores de las $p$ componentes en los $n$ individuos, estas nuevas variables están relacionadas con las originales mediante:

\begin{equation} \tag{9}
\mathbf{Z}=\mathbf{X}\mathbf{A}
\end{equation}

donde $\mathbf{A^t A}=\mathbf{I}$. Calcular las componentes principales equivale a aplicar una transformación ortogonal $\mathbf{A}$ a las variables $\mathbf{X}$ (ejes originales) para obtener unas nuevas variables $\mathbf{Z}$ incorreladas entre sí. Esta operación puede interpretarse como elegir unos nuevos ejes coordenados, que coincidan con los “ejes naturales” de los datos.



### [*Ejemplo:* ]{style="color: blue;"}

Vamos a realizar el análisis de componentes principales (PCA) sobre el conjunto de datos de las acciones del mercado español, esta vez utilizando a R para calcular los valores y vectores propios.

Recordad que ya hemos calculado la matriz de varianzas-covarianzas muestral sesgada de los datos transformados, $S$=`r S`.

```{r}
sol=eigen(S)
```

Los valores propios son: 

$$\lambda_1=0.521,\quad \lambda_2=0.113,\quad \lambda_3 = 0.007.$$

* Los vectores propios ortonormales correspondientes a los valores propios,  son los que aparecen con el nombre: `$vectors`


Las expresiones de las variables nuevas $CP_i$ en función de los logaritmos de las originales son:

$$\begin{array}{rl}
Z_1 = & 0.817 \cdot \log V_1 + 0.349\cdot \log V_2 - 0.459 \cdot \log V_3, \\
Z_2 = & 0.043 \cdot \log V_1 + 0.758\cdot \log V_2 + 0.651 \cdot \log V_3, \\
Z_3 = & 0.575 \cdot \log V_1 - 0.552 \cdot \log V_2 + 0.604 \cdot \log V_3, \\
\end{array}$$

La nueva matriz de datos respecto de las componentes principales será:

```{r}
X=matrix(c(datos_l$V1,datos_l$V2,datos_l$V3),nrow=34)
Z=X %*% sol$vectors # Z=XA  con A la matriz de vectores propios
head(Z)
```

Si representamos gráficamente las dos primeras componentes, podemos observar que se puede distinguir entre los dos grupos de acciones.

```{r}
plot(Z[,1],Z[,2])
```

**¿Qué hubiese pasado si centramos los datos?**

Aunque no es obligatorio centrar la matriz de datos para PCA, es una práctica común y recomendada, ya que ayuda a garantizar que las componentes principales reflejen de manera más precisa la estructura de variabilidad en los datos.

Si no se centran los datos, la primera componente principal estaría influenciada por la ubicación de los datos en el espacio original, es decir, por la media de los datos. En nuestro ejemplo, esto está minimizado por el efecto de la transformación logarítmica.


Veamos los resultados centrando los datos

```{r}
colMeans(X)
Hn=diag(n)-1/n # matriz de centrado
cX=Hn%*%X # matriz centrada
round(cX,3)
```

```{r}
Sc<-(1/n)*t(cX)%*%Hn%*%cX # estimador sesgado de la matriz de covarianza
round(Sc,3) # daría igual cov(cX)*(n-1)/n
solc<-eigen(Sc)
```

```{r}
Zc=cX %*% solc$vectors 
plot(Zc[,1],Zc[,2])
```

## Propiedades PCA con la matriz de covarianzas.

Los componentes principales como nuevas variables tienen las propiedades siguientes:

1. Las componentes principales reproducen la varianza total. 
$$\sum_{i=1}^p Var(\mathbf{Z}_i)=\sum_{i=1}^p \lambda_i=tr(\mathbf{S})=\sum_{i=1}^p s_i^2$$. 

2. Los componentes principales tienen correlación cero entre sí
(son *incorrelados*) por lo tanto su matriz de covarianzas es

$$\mathbf{S}_{Z}=\left(\begin{array}{cccc}
\lambda_1& 0 &\ldots &0\\
0& \lambda_{2}&\ldots & 0\\
\vdots & \vdots & & \vdots\\
0 & 0&\ldots &\lambda_{p}
\end{array}
\right)$$

$\det(\mathbf{S}_{Z})=\prod_{i=1}^p \lambda_i =\det(\mathbf{S})$. Luego los componentes principales conservan la varianza generalizada.


2. La proporción de varianza explicada por la componente $j$-ésima es
$$\frac{\lambda_j}{\sum_{i=1}^p \lambda_i}.$$


Además al ser *incorrelados* la proporción de varianza explicada por los $k$ primeros componentes es $$\frac{\sum_{i=1}^k \lambda_i}{\sum_{i=1}^p \lambda_i}.$$
<br>

$\mbox{Cov}(\tilde{\mathbf{X}}_i, \mathbf{Z}_j)=\frac{\sqrt{\lambda_j} a_{j i}}{s_i}$, donde $a_{j i}$ es la $i$-ésima componente del vector propio $\mathbf{a}_j$.

## PCA normado o con matriz de correlaciones

Las componentes principales se obtienen maximizando la varianza de la proyección. En términos de las variables originales esto supone maximizar:

$$M=\displaystyle \sum_{i=1}^p a_i^2s_i^2+ 2 \displaystyle \sum_{i=1}^p  \displaystyle \sum_{j=i+1}^p a_i a_j s_{ij}$$
con la restricción $\mathbf{a^ta}=1$. Si alguna de las variables, por ejemplo la primera, tiene una varianza $s^2_1$, mayor que las demás, la manera de aumentar $M$ es hacer tan grande como podamos la coordenada $a_1$ asociada a esta variable. En el límite si una variable tiene una varianza mucho mayor que las demás el primer componente principal coincidirá muy aproximadamente
con esta variable.

Cuando las variables tienen unidades distintas esta propiedad no es conveniente: si disminuimos la escala de medida de una variable cualquiera, de manera que aumenten en magnitud sus valores numéricos (pasamos por ejemplo de medir en km. a medir en metros), el peso de esa variable en el análisis aumentará, ya que en la ecuación anterior:

(1) su varianza será mayor y aumentará su coeficiente en el componente, $a_1$, ya que contribuye más a aumentar M;

(2) sus covarianzas con todas las variables aumentarán, con el consiguiente efecto de incrementar $a_i$.

En resumen, **cuando las escalas de medida de las variables son muy distintas, la maximización de $M$ dependerá decisivamente de estas escalas de medida y las variables con valores más grandes tendrán más peso en el análisis.** 

Si queremos evitar este problema, **conviene estandarizar las variables antes de calcular los componentes, de manera que las magnitudes de los valores numéricos de las variables $X$ sean similares.**

La estandarización resuelve otro posible problema. Si las variabilidades de la $X$ son muy distintas, las variables con mayor varianza van a influir más en la determinación de la primera componente. Este problema se evita al estandarizar las variables, ya que entonces las varianzas son la unidad, y las covarianzas son los coeficientes de correlación. La ecuación
a maximizar se transforma en:

$$M'=1 + 2 \displaystyle \sum_{i=1}^p  \displaystyle \sum_{j=i+1}^p a_i a_j r_{ij}$$
siendo $r_{ij}$ el coeficiente de correlación lineal entre las variables $i$, $j$. En consecuencia la solución depende de la correlaciones y no de las varianzas.

**Las componentes principales normados se obtiene calculando los vectores y valores propios de la matriz R, de coeficientes de correlación**. Llamando $\lambda_p^R$ a las raíces características de esa matriz, que suponemos no singular, se verifica que:
$${\sum_{i=1}^p \lambda_i^R}=traza(R)=p$$
Las propiedades de las componentes extraídos de $R$ son:

1. La proporción de variación explicada por $\lambda_p^R$ será $\frac{\lambda_p^R}{p}$.

2. Las correlaciones entre cada componente $z_j$ y las variables $X$ originales vienen dadas directamente por $a^t_j \sqrt{\lambda_j}$, siendo $\mathbf{z_j}=\mathbf{Xa_j}$.

Estas propiedades son consecuencia inmediata de los resultados de la sección anterior.

* **Cuando las variables X originales están en distintas unidades conviene aplicar el análisis de la matriz de correlaciones o análisis normado**. 

* Cuando las variables tienen las mismas unidades, ambas alternativas son posibles. 

* Si las diferencias entre las varianzas de las variables son informativas y queremos tenerlas en cuenta en el análisis no debemos estandarizar las variables: por ejemplo, supongamos dos índices con la misma base pero uno fluctua mucho y el otro es casi constante. Este hecho es informativo, y para tenerlo en cuenta en el análisis, no se deben estandarizar las variables, de manera que el índice de mayor variabilidad tenga más peso. Por el contrario, si las diferencias de variabilidad no son relevantes podemos eliminarlas con el análisis normado. En caso de duda, conviene realizar ambos análisis, y
seleccionar aquel que conduzca a conclusiones más informativas. 


## Análisis de Componentes Principales en R

Vamos a explicar las funciones de R para hacer el PCA, para ello utilizaremos los famosos datos de las flores iris.

<p align="center">
![](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Machine+Learning+R/iris-machinelearning.png){width=70%}

```{r}
head(iris,2)
tail(iris,2)
```

Lo primero que hacemos es revisar si las variables están correlacionadas, requisito necesario para obtener una representación más simple de éstas.

```{r}
cov(iris[,1:4])
cor(iris[,1:4])
```

Comparamos las funciones de R para hacer el PCA

```{r estinf,echo=FALSE,out.width="80%", fig.align ="center"}
knitr::include_graphics("Figuras/tabla_comparacion.png")
```

Calculamos las componentes con los datos escalados con la librería `factoextra` 

```{r,message=FALSE}
library(ggplot2)
library("factoextra")
iris.acp=prcomp(iris[,1:4], scale = TRUE)
```

Los valores propios muestran el porcentaje de varianza explicada por cada componente principal.

```{r}
lambdas=get_eigenvalue(iris.acp)
lambdas
```

Observamos que las dos primeras componentes principales explican aproximadamente el 96% de la variación total. Puede ser razonable, trabajar con esas dos componentes para el análisis posterior de estos datos.

Un método alternativo para determinar el número de componentes principales es observar el diagrama de valores propios ordenados de mayor a menor. El número de componentes se determina en el punto, más allá del cual los valores propios restantes son todos relativamente pequeños y de tamaño comparable.

```{r,fig.height=3,fig.height=4}
fviz_eig(iris.acp, addlabels = TRUE, ylim=c(0,100))
```

La representación de variables difiere de la gráfica de las observaciones: Las observaciones están representadas por sus proyecciones, pero las variables están representados por sus correlaciones.

```{r}
fviz_pca_var(iris.acp, col.var = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE) 
```

El gráfico anterior también se conoce como círculo de correlación variable. Muestra las relaciones entre todas las variables. Se puede interpretar de la siguiente manera:

* Las variables correlacionadas positivamente se agrupan.

* Las variables correlacionadas negativamente se colocan en lados opuestos del origen de la trama (cuadrantes opuestos).

* La distancia entre variables y el origen mide la calidad de la representación de las variables, las que están alejadas del origen están bien representadas.

La calidad de representación de las variables se llama cos2 (coseno cuadrado, coordenadas cuadradas). Es posible crear un diagrama de barras de las variables `cos2`:

```{r}
var <- get_pca_var(iris.acp)
fviz_cos2(iris.acp, choice = "var", axes = 1:2)
```

* Un cos2 alto indica una buena representación de la variable en el componente principal. En este caso, la variable se coloca cerca de la circunferencia del círculo de correlación.

* Un cos2 bajo indica que la variable no está perfectamente representada por los PC. En este caso, la variable está cerca del centro del círculo.


Para ver como se relacionan las componentes principales con los datos
originales, veamos los autovectores. 

```{r}
iris.acp$rotation
```


La primera componente principal da aproximadamente el mismo peso a la longitud del sépalo, la longitud del pétalo y el ancho del pétalo, pero da peso de signo contrario al ancho del sépalo. 

La segunda componente principal se refiere principalmente al sépalo.


El biplot es un gráfico que permite representar las variables originales y las observaciones transformadas en los ejes de componentes principales. 


* Cada flecha corresponde a una variable.

* Nos fijamos primeramente en las direcciones de las flechas y su sentido.

* Dos flechas que apunten al mismo lugar indica  correlación alta. 

* Dos flechas con sentidos diferentes pero en la misma dirección indican una correlación negativa.

* Cuando dos variables no están correladas en absoluto, se observan dos flechas apuntando en direcciones totalmente perpendiculares. 

* En cuanto a la diferencia en la longitud de las flechas, una menos larga informa que su variable está peor representada que una de largo mayor. Es una forma de medir la calidad de representación. 


```{r}
fviz_pca_biplot(iris.acp, repel = TRUE,
                col.var = "#2E9FDF", # color para las variables
                col.ind = "#696969"  # color para las observaciones
                )
```


En el gráfico anterior podemos observar lo siguiente:

* Todas las variables originales tienen influencia en las componentes principales (lo cual se evidencia en el tamaño de las flechas).

* La flecha más larga corresponde al ancho del sépalo, ya que tiene una fuerte influencia (loading) sobre la segunda componente.

* La dirección de esta última flecha indica que el “loading” del ancho del sépalo para la primera componente es negativo.

* Los “loadings” de las variables longitud del pétalo y ancho del pétalo con respecto a la segunda componente son muy bajos (las flechas son prácticamente horizontales).

* La variable longitud del sépalo tiene loadings relativamente altos en las dos componentes principales.

**Acceso a los resultados del ACP**

```{r}
# Resultados por Variables
res.var=get_pca_var(iris.acp)
res.var$contrib        # Contribuciones a las CP 
res.var$cos2           # Calidad de la representación
```

 
```{r}
# Resultados por observaciones
res.obs=get_pca_ind(iris.acp)
head(res.obs$coord,3)  #Coordenadas
head(res.obs$contrib,3)  #Contribuciones a las CP
head(res.obs$cos2,3)  # Calidad de la representación
```

**¿Qué tan bien lo hace el ACP?**
```{r,fig.height=4,warning=FALSE, message=FALSE}
library("ggfortify")
autoplot(iris.acp, data = iris, colour = 'Species',
         loadings = TRUE, loadings.colour = 'blue',
         loadings.label = TRUE, loadings.label.size = 3)
```


## Ejercicio:

Considera los datos `europa.dat` que están disponibles en Aula Digital. Los datos corresponden a los porcentajes de población empleados en diferentes actividades económicas en Europa para el año 1979. Las variables consideradas son: Agricultura, Minas, Fábricas, Suministro Eléctrico, Construcción, Industrias de Servicio, Finanzas, Servicios Sociales y Personales y, Transporte y Comunicaciones. Utiliza el método de componentes principales para reducir el número de variables, y tratar de determinar grupos de países con comportamientos semejantes en la distribución de su fuerza de trabajo. En este caso, usa la matriz de covarianza para el cálculo de las componentes principales, ya que todos los datos están medidos en la misma escala (porcentaje de la población) y por las caractersticas de los datos, no parece una buena idea considerarlos todos de igual manera.

