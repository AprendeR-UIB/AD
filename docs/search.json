[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Análisis de datos",
    "section": "",
    "text": "Presentación\nEsto es una edición en línea de los apuntes de Análisis de Datos.\nEl enfoque es teórico-práctico para el grado de Matemática de la UIB y se puede emplear como un curso previo una asignatura de Análisis de Datos en grados de informática."
  },
  {
    "objectID": "intro.html#la-estadística-y-el-método-científico",
    "href": "intro.html#la-estadística-y-el-método-científico",
    "title": "1  Introducción al Análisis de Datos",
    "section": "1.1 La estadística y el método científico",
    "text": "1.1 La estadística y el método científico\n\nLa ciencia avanza definiendo teorías que intentan explicar el mundo.\nLa comunidad científica elabora teorías/hipótesis que intentan explicar hechos que ocurren. Una hipótesis es científica si existe alguna manera de comprobar su veracidad.\nPodemos diseñar experimentos para comprobar si se cumplen las afirmaciones de la teoría.\nComo la naturaleza tiene un comportamiento con “incertidumbre”, es decir, que si repetimos el experimento se obtienen resultados similares pero no idénticos, la estadística permite analizar estos resultados y ver si las desviaciones de la teoría son razonables o no.\nSe ha definido estadística de muchas maneras. La que más nos gusta, y que está relaciona con la situación que acabamos de explicar, es que:\n\n\nLa estadística es la ciencia que permite adquirir conocimiento generalizable a partir de datos.\n\n\nLa estadística ayuda en todas las fases del método científico:\n\nPlanteamiento del problema: Diseño de experimentos y encuestas, determinación del tamaño de la muestra y métodos de muestreo adecuados para garantizar que los datos recopilados sean representativos de la población objetivo.\nRecopilación de datos: Proporciona herramientas para recopilar y organizar datos relevantes sobre el problema.\nAnálisis de datos: Aplicación de técnicas descriptivas (Análisis explorartorio de datos), así como técnicas inferenciales (contrastes de hipótesis, ajustes de modelos,etc) para sacar conclusiones sobre la población en función de la muestra recopilada.\nInterpretación de resultados: Ayuda a los científicos a determinar si los resultados son estadísticamente significativos y si las conclusiones se pueden generalizar a la población más amplia.\nComunicación de hallazgos: La estadística se usa para comunicar los resultados de manera efectiva a través de gráficos, tablas y tests estadísticos. Esto es esencial para que otros investigadores puedan comprender y evaluar los resultados.\nReproducibilidad: Proporciona métodos estadísticos claros y transparentes, se permite que otros repitan los experimentos y análisis para verificar la validez de los hallazgos.\nToma de decisiones: En muchos campos científicos, los resultados estadísticos se utilizan para tomar decisiones importantes. Por ejemplo, en la medicina, la estadística se usa para evaluar la eficacia de tratamientos y tomar decisiones sobre su uso en la práctica clínica.\n\nCuando alguien realiza un nuevo descubrimiento lo envía a una revisión por pares de la comunidad científica. Para que estos acepten el descubrimiento y pase a formar parte del conocimiento científico debes poner a disposición:\n\nLos datos brutos (raw data) junto con el modelo de datos.\nEl código parametrizado y con las líneas más importantes comentadas.\nLa documentación (artículo/ reporte) donde se interpretan y presentan los resultados más relevantes.\n\n\nEn resumen, la estadística es una herramienta esencial que ayuda a garantizar que la investigación científica sea rigurosa, confiable y basada en evidencia sólida."
  },
  {
    "objectID": "intro.html#gestión-básica-de-datos",
    "href": "intro.html#gestión-básica-de-datos",
    "title": "1  Introducción al Análisis de Datos",
    "section": "1.2 Gestión básica de datos",
    "text": "1.2 Gestión básica de datos\n\nEn estadística, siempre se empieza obteniendo unos datos sobre un grupo (relativamente pequeño) de individuos de una población. Bueno, en realidad, no se empieza obteniendo los datos, sino planificando cuidadosamente cómo se van a obtener, pero todo forma parte de la “obtención” de los datos.\nSe generaliza la información que se ha obtenido sobre este grupo de personas al total de la población.\nY no se trata de trucos de magia adivinatoria, sino de una ciencia cuya metodología ha sido validada por medio de demostraciones matemáticas o, en el peor de los casos, mediante simulaciones numéricas (el equivalente en matemáticas de los experimentos en las otras ciencias).\n\nAsí pues, la situación de partida a la hora de aplicar técnicas estadísticas es que disponemos de un conjunto de datos que describen algunas características de un grupo de individuos. El análisis estadístico de estos datos puede ser entonces de dos tipos básicos:\n\nAnálisis exploratorio de datos, cuando nuestro objetivo sea simplemente resumir, representar y explicar los datos concretos de los que disponemos. La estadística descriptiva es el conjunto de técnicas que se usan con este fin.\nAnálisis inferencial, si nuestro objetivo es deducir (inferir), a partir de estos datos, información significativa sobre el total de la población de interés. Las técnicas que se usan en este caso forman la estadística inferencial.\n\n\n\n\n\n\n\n\n\n\nAmbos tipos de análisis están relacionados. Por un lado, porque es conveniente (obligatorio, en nuestra opinión) empezar cualquier análisis inferencial dando un vistazo a los datos que se usarán.\nPor otro, porque muchas técnicas descriptivas permiten estimar propiedades de la población de la que se ha extraído la muestra. Por citar un ejemplo, la media aritmética de las alturas de un grupo de individuos nos da un valor más o menos representativo de sus alturas, pero también sirve para estimar la altura media de los individuos de la población total.\nLa estadística inferencial entra en juego cuando se quiere obtener información sobre una población y no se puede acceder a todos sus integrantes. Si por ejemplo queremos conocer la altura media de los estudiantes matriculados en esta asignatura de la UIB en este curso, en principio no necesitamos para nada la estadística inferencial. Sois pocos, os mediríamos a todos y calcularíamos la media. En todo caso, usaríamos técnicas de estadística descriptiva para arropar este valor representando la distribución de vuestras alturas de manera adecuada.\nPero si quisiéramos conocer la altura media de los mallorquines entre 18 y 25 años, sería muy complicado medirlos a todos. Entonces, lo que haríamos sería tomar una muestra representativa de esta población, medirlos y a partir de sus alturas estimar dicha altura media. Naturalmente, lo más seguro es que de esta manera no obtuviéramos el valor exacto de la altura media de los mallorquines de 18 años, nos tendríamos que conformar con obtener una aproximación dentro de un cierto margen de error y determinar la probabilidad de acertar con nuestra estimación y este margen de error. La estadística inferencial es la que nos permite acotar el error que podamos haber cometido y calcular la probabilidad de cometerlo, incluyendo la metodología que tendríamos que haber usado para tomar la muestra en primer lugar."
  },
  {
    "objectID": "intro.html#r-rstudio---posit-rmarkdowm---quarto",
    "href": "intro.html#r-rstudio---posit-rmarkdowm---quarto",
    "title": "1  Introducción al Análisis de Datos",
    "section": "1.3 R/ RStudio - Posit / RMarkdowm - Quarto",
    "text": "1.3 R/ RStudio - Posit / RMarkdowm - Quarto\nTodas las técnicas que usaremos en la asignatura pueden ser implementadas y/o desarrolladas en software libre como Python y R. Ambos se consideran lenguajes de programación esenciales para la ciencia de datos. Lo ideal sería dominar ambos para tener una base de programación completa, pero:\n\nR es un lenguaje específico utilizado para el análisis de datos y la estadística.\nR es muy adecuado para un sub-campo del aprendizaje automático conocido como aprendizaje estadístico. Cualquier persona con una formación formal en estadística debería reconocer la sintaxis y la construcción de R.\nAl igual que Python, R cuenta con una sólida comunidad, estructurada alrededor de la “Comprehensive R Archive Network”, o CRAN, pero no ofrece un desarrollo de software de propósito general como Python.\nCada día salen nuevos paquetes que extienden las funcionalidades de R y cubren casi todas las necesidades computacionales y estadísticas de un científico. Para que os hagáis una idea, en el momento de revisar estas notas (septiembre de 2023) el número de paquetes en el repositorio de la CRAN acaba de superar los 19800.\nEl acceso a R se proporciona a través de RStudio, entorno que presenta una ventana de visualización, un explorador de archivos, un visor de datos y un editor. Este entorno suele ser menos intimidante que el shell de R. Además, cuenta con ayuda integrada, resaltado de sintaxis y completado contextual por tabulaciones; todas estas herramientas facilitan el trabajo.\nRStudio tiene un nuevo nombre desde julio de 2022: Posit. Posit es una palabra que significa proponer una idea para su discusión, proviene de la aspiración científica de construir niveles cada vez mayores de conocimiento y comprensión de experimentos que generan\n\n\n\n\n\n\n\n\n\n\n\nPosit tiene como misión la creación de software libre y de código abierto para la ciencia de datos, la investigación científica y la comunicación técnica. Han incluido algunas herramientas para Python a través de Quarto.\nQuarto está pensado como un cuaderno de laboratorio moderno donde predomina R pero que soporta código Python (reticulate), SQL, Julia, entre otros; pensado para experimentos que requieren multilenguaje.\nPosit Cloud permite acceder al potente conjunto de herramientas de ciencia de datos de Posit directamente desde su navegador. Esto favorecerá el trabajo en equipo. Podéis revisar la siguiente Guía para crearos una cuenta."
  },
  {
    "objectID": "intro.html#control-de-versiones-con-git-github",
    "href": "intro.html#control-de-versiones-con-git-github",
    "title": "1  Introducción al Análisis de Datos",
    "section": "1.4 Control de versiones con Git / GitHub",
    "text": "1.4 Control de versiones con Git / GitHub\n\nEn esta asignatura la forma de llegar a un resultado de análisis de datos es tan importante como el propio resultado. Además, uno de los objetivos es exponeros al uso de herramientas de software para la ciencia de datos moderna.\nLa idea de reproducibilidad lleva implícita la colaboración. El código que se produce es parte de la documentación del proceso y es fundamental compartirlo (aunque sólo sea con uno mismo).\nLo anterior se logra mejor con un sistema de control de versiones distribuido como Git. Mantener un registro sobre los proyectos, es lo que permite rastrear y gestionar cambios en el código a lo largo del tiempo. Se puede decir que nos permite guardar el progreso de nuestro código de tal forma que, si en algún momento cometemos algún error irreversible en una versión posterior, siempre podremos recuperar una versión anterior en la que todo funcionaba correctamente y retomar el proyecto desde ese punto.\nGit permite la colaboración, pero carece de características sociales y herramientas específicas para la colaboración en equipo. GitHub proporciona herramientas para la revisión de código, la gestión de problemas y la colaboración en proyectos.\n\n\n\n\n\n\n\n\n\n\n\nGitHub es un servicio en la nube donde se pueden subir repositorios propios y compartir el código con otras personas de tal forma que sea accesible desde Internet.\nUn repositorio funciona como una carpeta virtual. En él se encuentran todos los archivos de un proyecto y el historial de revisiones de cada uno, permitiendo restablecer una versión del código en caso de error en su ejecución.\nPodemos ver proyectos de otras usuarios, valorarlos, proponer mejoras en el código, GitHub es una de las aplicaciones que mejora la gestión de proyectos y el acceso a recursos compartidos.\nEn octubre del 2021 se estrenó GitHub Copilot, una herramienta de inteligencia artificial en la nube desarrollada conjuntamente entre GitHub y OpenAI. Su objetivo es sugerir y autocompletar el código escrito en entornos de desarrollo integrados (IDE).\n\n\nEn esta clase, utilizaremos GitHub como sistema de gestión del aprendizaje para distribuir y recopilar las entregas como repositorios.\n\n\nCrearemos en GitHub un repositorio por estudiante/equipo para cada entrega. Utilizaremos un sencillo flujo de trabajo centralizado que sólo requiere realizar acciones simples como push, pull, add, rm, commit, status y clone.\n\n\n1.4.1 Git / GitHub con R\nVeamos cómo configurar todo. Gran parte de lo que está aquí proviene del libro Happy Git and GitHub for the useR de Jenny Bryan y del artículo de David Keyes puedes ver sus vídeos en caso de que, la breve explicación que presentamos abajo, no sea suficiente para ti.\n\nInstalar Git: El primer paso es instalar Git, en el Capítulo 6 del libro explican el proceso para los usuarios de Mac, Windows y Linux. Nosotros ya lo tenemos instalado, así que mostramos cómo verificar si tienes Git instalado y su versión usando el terminal en RStudio.\n\nEn el terminal de RStudio:\n\nwhich git # ruta donde está instalado el Git\ngit --version # version\n\n\nConfigurar Git (Editar gitconfig file):El siguiente paso es configurar Git. Esto se trata en el Capítulo 7 del libro, aunque mostramos lo que creemos es un proceso un poco más fácil. Específicamente, sugerimos usar la función edit_git_config() del paquete usethis, que abrirá su archivo gitconfig. Agrega tu nombre y correo electrónico y cierra esto.\n\nEn la consola de RStudio:\n\nlibrary(usethis)\nusethis::edit_git_config()\n# Modificar en el fichero \".gitconfig\" los apartados: \"name\" y \"email\" \n# y guardar el fichero\n\n\nInicializar un repositorio Git: Ahora que has instalado y configurado Git, puedes usarlo localmente. La función use_git() agregará un repositorio Git (a menudo denominado “repositorio”) a un proyecto RStudio existente. Aquí crearemos un nuevo proyecto y luego inicializaremos un repositorio de Git.\n\nEn RStudio: * Crear un proyecto nuevo\n\nSeleccionar “Nuevo Directorio”\nProyecto\n\nActivar: “Create a git repository”\n\n\nEn la consola de RStudio:\n\nlibrary(usethis)\nusethis::use_git()\n# Elegir siempre la opción: 1\n# Y ante la ventana, seleccionar: \"Save\"\n\nY visitar la pestaña: “Git” en RStudio.\n\nVer historial de confirmación: Ahora que tu proyecto de RStudio tiene un repositorio Git asociado, verás una pestaña adicional en la parte superior derecha: la pestaña Git. Desde aquí, puedes ver todo el historial de cambios en tu código a lo largo del tiempo (¡todavía no muchos!).\n\nEn RStudio:\nVisitar la pestaña: “Git” de RStudio Pulsar el icono del reloj para ver el historial de “Commit” realizados para ver el “Initial Commit”.\n\nHacer una confirmación (commit) y ver más historia: Git no realiza un seguimiento automático de los cambios de la manera en que lo hace una herramienta como Google Docs. En su lugar, tienes que decirle a Git: Hice cambios y quiero que mantengas un registro de ellos. Decirle a Git esto se llama hacer una confirmación (commit) y puedes hacerlo desde RStudio.\n\nCada commit tiene un mensaje de confirmación, lo que es útil porque, cuando miras tu historial de código, ves lo que hiciste en cada momento (es decir, en cada commit). RStudio tiene una herramienta integrada para ver su historial de código. Puedes hacer clic en cualquier commit para ver qué cambió, en relación con el commit anterior. Las líneas que se agregaron en verde; y las que se eliminaron en rojo.\nEn RStudio:\n\nCrear un fichero de script R: “test.R” y guardarlo.\nVisita la pestaña “Git” de RStudio y pulsa sobre el botón de “commit” para confirmar la creación del fichero: “test.R”.\nEn el panel del commit añada un texto que lo defina.\nHaz varios cambios en el fichero “test.R” y en cada uno de ellos haz de nuevo un “commit”.\nRevisa luego la historia de los cambios que se han producido en el historial (pulsar el icono del reloj).\nObserva los nuevos cambios resaltados en color verde. Frente a los valores antiguos que aparecerán en color rojo.\n\n\n\n1.4.2 Conectar RStudio y GitHub\nEl proceso hasta ahora nos ha permitido usar Git localmente. Pero, ¿qué pasa si queremos conectarnos a GitHub? ¿Cómo lo hacemos?\nLa mejor manera de conectar RStudio y GitHub es usando tu nombre de usuario y un token de acceso personal (PAT). Para generar un token de acceso personal, usa la función create_github_token() de usethis. Esto te llevará a la página correspondiente en el sitio web de GitHub, donde le darás un nombre a tu token y lo copiarás (¡no lo pierdas porque nunca volverá a aparecer!).\nEn la consola de RStudio:\n\nlibrary(usethis)\nusethis::create_github_token()\n\n\nPulsa sobre el enlace que aparece en la salida en la consola.\nSe abrirá una página web de Github en la que tendrás que pulsar el botón “Generate token”.\nCopia el token que aparece en Github (lo utilizarás en el siguiente paso).\nAhora que has creado un token de acceso personal, debes almacenarlo para que RStudio pueda acceder a él y sepa conectarse a tu cuenta de GitHub. La función gitcreds_set() del paquete gitcreds te ayudará aquí. Ingresará tu nombre de usuario de GitHub y el token de acceso personal como contraseña (NO tu contraseña de GitHub). Una vez que hayas hecho todo esto, ¡habrás conectado RStudio a GitHub!.\n\nEn la consola de RStudio:\n\nlibrary(gitcreds)\ngitcreds::gitcreds_set()\n# Ante la pregunta: \"Enter password or token\"\n# introduce el token copiado en el paso anterior\n\n\n\n1.4.3 Conectar proyectos de RStudio con repositorios de GitHub\nAhora que hemos conectado RStudio y GitHub, discutamos cómo hacer que los dos funcionen juntos. La idea básica es que configures los proyectos que creas en RStudio con repositorios GitHub asociados. Cada proyecto de RStudio vive en un solo repositorio de GitHub.\n¿Cómo conectamos un proyecto de RStudio a un repositorio de GitHub? Happy Git and GitHub for the useR propone tres estrategias. Demostraremos la forma más sencilla\ncrear un repositorio en GitHub primero. Cree el repositorio y, a continuación, cuando inicie un nuevo proyecto en RStudio, utilice la opción de control de versiones, introduzca la URL de su repositorio y listo.\nGitHub primero\nCrea el repositorio en GitHub y, a continuación, cuando inicies un nuevo proyecto en RStudio, utiliza la opción de control de versiones, introduce la URL de tu repositorio y listo.\nPara bajar un repositorio creado en Github a un proyecto local en RStudio, tendréis que realizar los siguientes pasos:\n\nCrear un nuevo repositorio en nuestra cuenta de Github (o utilizar uno ya existente): pulsar el botón “Create repository”.\nCopiar al portapapeles la primera dirección que aparece (pulsando el botón de la derecha). Coincide con la dirección url que aparece en la barra del navegador.\nEn RStudio seleccionamos crear “New project”, elegimos “Version Control” y luego seleccionamos “Git”.\nIntroducimos en el primer cuadro de texto la url copiada anteriormente. Pulsamos “Create Project”.\nA continuación podrás consultarse la pestaña “Git” y ver la información asociada al repositorio descargado.\n\n\n\n1.4.4 Flujo de trabajo general\nAhora que hemos conectado RStudio y GitHub, podemos compartir nuestro trabajo entre los dos.\nPush (Subir a Github)\n“Push” significa enviar cualquier cambio en tu código de RStudio a GitHub. Para hacer esto, primero tenemos que hacer un commit. Después de confirmar, ahora tenemos un botón (la flecha hacia arriba) en RStudio que podemos usar para enviar nuestro código a GitHub.\nEn RStudio:\n\nCreamos un nuevo fichero de script R o un fichero Rmd y lo guardamos.\nPulsamos en la pestaña “Git” sobre el botón de “commit”. Marcamos todos los ficheros sobre los checks de “Staged”, rellenamos la descripción del commit y pulsamos sobre el botón de “commit”.\nDespués de hacer el commit, pulsamos sobre el botón “Push” para subir los cambios a Github.\nA continuación puedes comprobar en la página de Github del repositorio que se han actualizado los últimos ficheros considerados en el último commit.\n\nPull (Descargar desde Github)\nLo opuesto a “empujar”Push” es bajar (“Pull”). Utilizando el botón de flecha hacia abajo, RStudio va al repositorio de GitHub, toma el código más reciente y lo lleva a su editor local.\nHacer “Push” regularmente es extremadamente importante si estás colaborando, aunque si eres el único que trabaja en un proyecto de RStudio y un repositorio GitHub asociado, sabes que tu código local coincide con lo que está en GitHub, por lo que es menos importante.\nEn la página de Github de nuestro repositorio:\n\nEditamos uno de los ficheros de nuestro repositorio pulsando sobre el icono de un lápiz (a la derecha). Realizamos alguna modificación sobre el fichero (o ficheros).\nPulsamos en la parte inferior de la página en el botón de “Commit changes” (rellenando los comentarios que creamos oportunos sobre el commit que se está realizando). Se puede navegar por la página de Github para consultar todos los commits realizados (y mucha más información).\n\nVolvemos a RStudio:\n\nEn la pestaña “Git” pulsamos sobre el botón de la flecha que apunta hacia abajo (verde) para realizar un “Pull” o descarga de los cambios en Github a nuestro proyecto local en RStudio.\nDespués de eso puedes comprobare que los ficheros locales de nuestro proyecto se han actualizado con los cambios que se han producido en el repositorio.\n\n¡Lo lograste!\n¡Ahora está todo configurado para usar Git y GitHub con RStudio!"
  },
  {
    "objectID": "intro.html#los-datos-y-sus-tipos",
    "href": "intro.html#los-datos-y-sus-tipos",
    "title": "1  Introducción al Análisis de Datos",
    "section": "1.5 Los datos y sus tipos",
    "text": "1.5 Los datos y sus tipos\nEn vuestro curso de Estadística estudiasteis algunas técnicas básicas de estadística descriptiva. Estas técnicas consisten en una serie de valores y gráficos que nos permiten resumir y explorar un conjunto de datos, con el objetivo final de entenderlos o describirlos lo mejor posible.\nLos datos de los que disponemos suelen ser multidimensionales, en el sentido de que observamos varias características (variables) de una serie de individuos. Almacenamos estos datos en tablas de datos como la que presentamos abajo, donde cada columna corresponde a una variable y cada fila son los datos de un individuo concreto. Así, en esta tabla, cada fila representa un niño y cada columna recoge una de las características que hemos anotado: su nombre, su altura (en cm), su número de hermanos, el color de sus cabellos, el número semanal de refrescos que suele tomar, y su grado de satisfacción con un juego para móvil (entre 0 y 5).\n\n\n\nUna pequeña tabla de datos sobre niños\n\n\n\nNombre\nAltura\nHermanos\nCabello\nRefrescos semanales\nSatisfacción App\n\n\n\n\n1\nMarta\n135\n2\nrubio\n2-3\n4\n\n\n2\nLaura\n132\n1\nnegro\n2-3\n4\n\n\n3\nXavier\n138\n0\nnegro\n0-1\n3\n\n\n4\nJoan\n141\n3\ncastaño\n4-5\n2\n\n\n5\nMaria\n134\n2\nrojo\n0-1\n3\n\n\n6\nMaria\n136\n1\ncastaño\n6 o más\n5\n\n\n\n\n\n\n\n\n\n\nEn este curso vamos a “sobrecargar” el término variable, en el sentido de que tendrá dos significados diferentes que esperamos que podáis distinguir según el contexto:\n\nPor un lado, llamaremos variable a una característica que puede tomar diferentes valores sobre diferentes individuos; cuando tenga este sentido, a veces le añadiremos el adjetivo poblacional. Por ejemplo, la altura de las personas (de todo el mundo, de un país, de una ciudad…) es una variable poblacional.\nPor otro lado, también llamaremos una variable a un vector formado por los valores de una variable poblacional sobre los sujetos de una muestra. Por ejemplo, las alturas de los niños recogidas en la tabla forman una variable en este sentido.\n\n\n\nLos tipos básicos de datos que consideramos en este curso son los siguientes:\n\nDatos cualitativos. Son los que expresan una cualidad del individuo, como por ejemplo el sexo cromosómico (macho, hembra), el género de una persona (hombre, mujer, lesbiana, gay, bisexual, transexual, intersexual, asexual), tipos de cáncer (de mama, de colon, de próstata…)… Si solo pueden tomar dos valores (“Sí” o “No”, “Macho” o “Hembra”…) los llamamos binarios o dicotómicos y si pueden tomar más de dos valores, politómicos o multicotómicos, dependiendo de lo que queramos complicar los adjetivos. A los posibles valores que puede tomar un tipo de datos cualitativo se los suele llamar niveles.\nLos datos cualitativos pueden ser iguales o distintos, y no admiten ningún otro tipo de comparación.\nDatos ordinales. Son datos similares a los cualitativos, en el sentido de que expresan una cualidad del individuo, pero con la diferencia de que se pueden ordenar de manera natural. Por ejemplo, los niveles de gravedad de una enfermedad (sano, leve, grave, muy grave, …) o las calificaciones en un examen (suspenso, aprobado, notable, sobresaliente) son datos ordinales. En cambio, no se pueden ordenar de manera significativa los sexos o los tipos de cáncer de los individuos: por eso son datos cualitativos y no ordinales.\nTambién se suele llamar a los posibles valores que puede tomar un tipo de datos ordinal sus niveles.\nDatos cuantitativos. Son datos que se refieren a medidas que sean números genuinos, con los que tenga sentido operar, tales como edades, longitudes, pesos, tiempos, números de individuos, etc. Distinguimos dos tipos:\n\nDiscretos: Pueden tomar solo valores que avanzan a saltos y que podemos identificar con números naturales: número de hermanos, número de ingresos en un día en un hospital…\nContinuos: Podrían tomar cualquier valor real dentro de un intervalo si se pudieran medir con precisión infinita: altura, temperatura, tiempo…\n\n\n\nEn la tabla anterior:\n  \n* La variable \"Nombre\" es cualitativa.\n* La variable \"Altura\" es cuantitativa continua.\n* La variable \"Hermanos\" es cuantitativa discreta.\n* La variable \"Cabello\" es cualitativa.\n* La variable \"Refrescos semanales\" es ordinal.\n* La variable \"Satisfacción App\" también es ordinal.\n\n\nDos puntos relevantes a tener en cuenta y que justifican algunas clasificaciones que puede que encontréis dudosas en el ejemplo anterior:\n\nNo todo número es un dato cuantitativo. Solo los consideramos cuantitativos cuando son números genuinos, “de verdad”. Por ejemplo, si pedimos a un paciente que califique su dolor con un número natural de 0 a 10, no es un dato cuantitativo, sino ordinal:\n\nNo es una medida precisa del dolor; no son números “de verdad”, sino abreviaturas de “Nada”, “Un poquito”,…, “Matadme”.\nTener dolor 6 no significa “tener el doble de dolor” que tener dolor 3 (si lo significara, ¿cuál sería el valor correspondiente “al doble de dolor” que 7?). En cambio, una persona con 6 hermanos sí que tiene el doble de hermanos que si tuviera 3.\nNo tiene sentido sumarlos u operarlos en general. Por ejemplo, si yo tengo dolor de nivel 6 y tú tienes dolor de nivel 5, entre los dos no tenemos dolor de nivel 11. En cambio, si yo tengo 6 hermanos y tú 5, entre los dos sí que tenemos 11 hermanos.\n\nEste es justamente el caso de la variable “Satisfacción App” de la tabla anterior. Pese a que sus valores son números, el único contenido real que tienen es su orden: a la María que toma muchos refrescos le ha gustado la app bastante más que a la María que apenas toma refrescos.\nLa distinción discreto-continuo es puramente teórica. En realidad, todo dato es discreto porque no podemos medir nada con precisión infinita, pero las herramientas matemáticas “continuas” (derivadas, integrales, etc.) son mucho más potentes que las discretas, por lo que siempre que tenga sentido, es conveniente considerar una variable como continua.\nObservad, por ejemplo, la diferencia entre la altura, pongamos que medida en cm y redondeada a unidades como en la tabla anterior, y el número de hermanos. Ambos se presentan como números naturales, pero los números de hermanos no admiten mayor precisión, mientras que las alturas las podríamos medir, con los aparatos adecuados, en mm, en µm, en nm…. Como además las herramientas para tratar datos continuos son mucho más potentes, vamos a considerar las alturas como datos continuos, mientras que los números de hermanos no hay más remedio que tratarlos como discretos.\nEn concreto, es conveniente considerar en la práctica como datos continuos aquellos que dan lugar a números naturales muy grandes, como por ejemplo los números de glóbulos rojos en un litro de sangre, de bases nucléicas en un genoma, o de personas de un país. La diferencia entre diez millones, diez millones uno, diez millones dos… puede considerarse como continua: de hecho, si tomamos el millón como unidad, la diferencia está en la séptima cifra decimal.\n\n\n\n\nHemos dicho que la variable “Cabello” es cualitativa. En principio, el color de los cabellos no tiene ningún orden “natural”. Pero si en un estudio definimos un orden claro para esta variable (por ejemplo, por la longitud de onda correspondiente) y este orden es relevante en nuestro estudio, habrá que considerarla una variable ordinal.\n\n\n\n\n\nLa variable “Refrescos semanales” es de un tipo de datos ordinales muy concreto que a veces se califican de cuantitativos agrupados: sus niveles se obtienen agrupando en intervalos los posibles valores de una variable cuantitativa (en este caso, la variable discreta que mide el número preciso de refrescos semanales).\n\n\n\nEl análisis, tanto descriptivo como inferencial, de un conjunto de datos es diferente según su tipo.\n\nAsí, para datos cualitativos sólo tiene interés estudiar y representar las frecuencias con que aparecen sus diferentes valores, mientras que el análisis de datos cuantitativos suele involucrar el cálculo de medidas estadísticas, como la media o la desviación típica, que expresen numéricamente sus propiedades.\nOs dejamos el material Aprender R1 para que repaséis los capítulos 10 al 14 correspondientes a la parte de Estadística descriptiva."
  },
  {
    "objectID": "intro.html#práctica-1",
    "href": "intro.html#práctica-1",
    "title": "1  Introducción al Análisis de Datos",
    "section": "1.6 Práctica 1:",
    "text": "1.6 Práctica 1:\n\nFormad grupos de 3 integrantes.\nTrabajaréis con los datos pingüinos, leed la documentación y seguid las siguientes instrucciones:\n\nCread un repositorio en Github para vuestro grupo con un nombre que sea fácilmente identificable para los profesores de la asignatura, por ejemplo,“Entrega_1_AD”.\nCread un proyecto nuevo en RStudio conectado al repositorio que habéis creado en el paso anterior. Agregad un documento de quarto donde trabajaréis.\nInstalad y cargad en RStudio la librería palmerpenguins , así como el conjunto de datos penguins\n\n\n\n#install.packages(\"palmerpenguins\",dep=TRUE)\nlibrary(\"palmerpenguins\")\nprint(penguins, width = 50)\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;\n 1 Adelie  Torgersen           39.1          18.7\n 2 Adelie  Torgersen           39.5          17.4\n 3 Adelie  Torgersen           40.3          18  \n 4 Adelie  Torgersen           NA            NA  \n 5 Adelie  Torgersen           36.7          19.3\n 6 Adelie  Torgersen           39.3          20.6\n 7 Adelie  Torgersen           38.9          17.8\n 8 Adelie  Torgersen           39.2          19.6\n 9 Adelie  Torgersen           34.1          18.1\n10 Adelie  Torgersen           42            20.2\n# ℹ 334 more rows\n# ℹ 4 more variables: flipper_length_mm &lt;int&gt;,\n#   body_mass_g &lt;int&gt;, sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n\n\n\n\n\n\n\n\nCon lo que sabéis de R base, realizad un análisis exploratorio de datos y redactad un reporte con los hallazgos más importantes. No olvidéis agregar en el reporte el URL de vuestro repositorio de GitHub.\nEntregad el reporte en la tarea de Aula Digital disponible. Revisad la fecha en que cierra la tarea."
  },
  {
    "objectID": "intro.html#gramática-limpia-y-coherente-con-tidyverse",
    "href": "intro.html#gramática-limpia-y-coherente-con-tidyverse",
    "title": "1  Introducción al Análisis de Datos",
    "section": "1.7 Gramática limpia y coherente con Tidyverse",
    "text": "1.7 Gramática limpia y coherente con Tidyverse\n\n1.7.1 La librería Tidyverse\n\n\n\n\n\n\n\n\n\n\nTidyverse es una colección de paquetes/librerías de R para ciencia de datos que comparten una filosofía de diseño, gramática y estructuras similar. En la página Tidverse.org podéis encontrar una descripción detallada de cada una de las librerías, un Blog con artículos de interés para la ciencia de datos, ayuda y recursos de aprendizaje.\nTodos estos paquetes están pensados para:\n\nTener una tecnología con la que puedan convivir diferentes tipos de profesionales (como por ejemplo: informáticos, economistas, matemáticos, gestores) compartiendo el mismo flujo de datos.\nFacilitar el análisis y modelización de datos\n\n\n\n\n\n\n\n\n\n\n\n\nHadley Wickham, su creador, es el director de los científicos de datos de RStudio (actual Posit) y profesor adjunto de estadística en la Universidad de Auckland, la Universidad de Stanford y la Universidad de Rice.\nLas librerías de tidyverse han venido a sustituir R base por su eficiencia y facilidad de programación para no informáticos. Casi todas las consultas a páginas técnicas de R son o incluyen código de tidyverse.\nPaquetes del core de tidyverse:\n\nggplot2: Permite crear gráficos de forma declarativa. Le introducimos los datos, le decimos cómo asignar variables a la estética, qué tipo de gráfico utilizar, y ggplot2 nos devolverá un gráfico más “elegante” y fácil de editar que los de R base.\ndplyr: Gramática de manipulación de datos, un conjunto coherente de acciones que resuelven los retos más comunes como juntar datos y transformarlos.\ntidyr: Conjunto de funciones que ayudan a obtener datos ordenados. Los datos ordenados son datos con una forma consistente: en resumen, cada variable va en una columna, y cada fila es una unidad muestral.\nreadr: Proporciona una forma rápida y amigable de leer datos rectangulares (como csv, tsv).\npurrr: Mejora el conjunto de herramientas de programación funcional (PF) de R proporcionando un conjunto completo y coherente de herramientas para trabajar con funciones y vectores. Una vez dominados los conceptos básicos, purrr permite sustituir muchos bucles for por código más fácil de escribir y más expresivo.\ntibble: Un formato más moderno que el data frame, manteniendo lo que en el tiempo ha demostrado ser eficaz, y desechando lo que no.\nstringr: Conjunto cohesivo de funciones diseñadas para hacer el trabajo con cadenas de texto lo más fácil posible.\nforcats: Conjunto de herramientas útiles que resuelven problemas comunes con factores. R utiliza factores para manejar variables categóricas, variables que tienen un conjunto fijo y conocido de posibles valores.\npurrr: programación funcional (pipes)\n\nHay muchos otros paquetes que se integran sin problemas, por ejemplo, lubridate (para manejar datos tomados en el tiempo), stringr (texto), forcats (factores), etc.\nPara instalar y cargar tidyverse\n\n\n#install.packages(\"tidyverse\")\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter()     masks stats::filter()\n✖ dplyr::group_rows() masks kableExtra::group_rows()\n✖ dplyr::lag()        masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nSe puede ver la versión del paquete tidyverse y la de los paquetes base.\nCuidado: algunas funciones de R se sobrescriben por sus equivalentes de tidyverse. En ocasiones es preferible indicar explícitamente el nombre de la función que deseamos utilizar, por ejemplo: dplyr::group_by para distinguir de plyr::group_by (dplyr es una evolución del paquete plyr).\n\n1.7.1.1 ¿Qué significa tidy data?\n\n\n\n\n\n\n\n\n\n\nDecimos que unos datos están bien estructurados o son “tidy data” si se cumplen los siguientes principios:\n\nCada variable forma una columna.\nCada observación forma una fila.\nCada tipo de unidad de observación forma una tabla.\n\n\n\n\n\n\n\n\n\n\n\n\nAlgunas formas de violar los principios de los datos ordenados son:\n\nLas cabeceras de las columnas son valores, no nombres de variables.\nSe almacenan múltiples variables en una columna.\nLas variables se almacenan tanto en filas como en columnas.\nSe almacenan múltiples tipos de unidades de observación en la misma tabla.\nUna misma unidad de observación se almacena en varias tablas.\n\n\nVeamos ejemplos de datos “No tidy” creados a partir del conjunto de datos con el que venimos trabajando de los pingüinos.\nEjemplo 1:\n\n\n# A tibble: 3 × 4\n  species   Biscoe Dream Torgersen\n  &lt;fct&gt;      &lt;int&gt; &lt;int&gt;     &lt;int&gt;\n1 Adelie        44    56        52\n2 Chinstrap     NA    68        NA\n3 Gentoo       124    NA        NA\n\n\nEjemplo 2:\n\n\n# A tibble: 5 × 3\n  col            island     year\n  &lt;chr&gt;          &lt;fct&gt;     &lt;int&gt;\n1 Gentoo_NA      Biscoe     2007\n2 Adelie_male    Torgersen  2007\n3 Gentoo_female  Biscoe     2008\n4 Chinstrap_male Dream      2008\n5 Adelie_male    Torgersen  2009\n\n\nEjemplo 3:\n\n\n# A tibble: 3 × 4\n  term              bill_length_mm bill_depth_mm flipper_length_mm\n  &lt;chr&gt;                      &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;\n1 bill_length_mm            NA            -0.235             0.656\n2 bill_depth_mm             -0.235        NA                -0.584\n3 flipper_length_mm          0.656        -0.584            NA    \n\n\nEjemplo 4:\n\n\n# A tibble: 6 × 6\n  species   island sex    model              mpg   cyl\n  &lt;fct&gt;     &lt;fct&gt;  &lt;fct&gt;  &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;\n1 Chinstrap Dream  female &lt;NA&gt;              NA      NA\n2 Gentoo    Biscoe female &lt;NA&gt;              NA      NA\n3 Gentoo    Biscoe male   &lt;NA&gt;              NA      NA\n4 &lt;NA&gt;      &lt;NA&gt;   &lt;NA&gt;   Merc 450SLC       15.2     8\n5 &lt;NA&gt;      &lt;NA&gt;   &lt;NA&gt;   Dodge Challenger  15.5     8\n6 &lt;NA&gt;      &lt;NA&gt;   &lt;NA&gt;   Pontiac Firebird  19.2     8\n\n\n\nSi tenemos datos provenientes de distintas fuentes, seguramente tendremos que limpiarlos y juntarlos en un único tibble.\n\n\n\n\n1.7.2 El operador de tuberías (pipe) %&gt;%\n\nLos pipes básicos pasan un valor, atributo u objeto (LHS: Left Hand Side) a la siguiente llamada de función (RHS: Right Hand Side) como primer argumento\n\n\nx %&gt;% f # equivalente a: f(x)\nx %&gt;% f(y) # equivalente a: f(x, y)\nx %&gt;% f %&gt;% g %&gt;% h # equivalente a: h(g(f(x)))\n\n\nLos pipes también se usan con marcadores de posición; en este caso, reenvian un valor u objeto (LHS) a la siguiente llamada de función (RHS) como cualquier argumento.\n\n\nx %&gt;% f(.) # equivalente a: x %&gt;% f\nx %&gt;% f(y, .) # equivalente a: f(y, x)\nx %&gt;% f(y, z = .) # equivalente a: f(y, z = x)\nx %&gt;% f(y = nrow(.),\n        z = ncol(.))  # equivalente a: f(x, y = nrow(x), z = ncol(x))\n\n\nUna secuencia de código que comienza con el marcador de posición (.) devuelve una función que puede utilizarse para aplicar posteriormente la tubería a valores concretos.\n\n\nf &lt;- . %&gt;% cos %&gt;% sin # equivalente a: f &lt;- function(.) sin(cos(.))\n\n\nf(20) # equivalente a: la tubería 20 %&gt;% cos %&gt;% sin\n\n\nPara saber más sobre %&gt;%, haced vignette(\"magrittr\") en la consola de R.\nSe puede obtener %&gt;% en Rstudio desktop utilizando el atajo de teclado: Ctrl + Shift + M.\nEjemplo: ¿Cuál es la masa corporal media, en gramos, de los pingüinos estudiados durante el año 2007?\n\n\n# Sin pipes\nmean(subset(penguins, year == 2007)$body_mass_g, na.rm = T)\n\n[1] 4124.541\n\n# Con pipes (tidyverse)\nresultado &lt;- penguins %&gt;% \n  subset(year == 2007) %&gt;% \n  .$body_mass_g %&gt;% \n  mean(na.rm = T)\n\nLa respuesta a la pregunta formulada sería: “La masa corporal media, en gramos, de los pingüinos estudiados durante el año 2007 es 4125 gramos”.\n\nVentajas de usar pipes:\n\nEl estilo secuencial de las tuberías mejora la lectura del código en comparación con las funciones anidadas.\nHace innecesario almacenar los resultados intermedios.\nEs muy fácil añadir o eliminar pasos (empalmes de tuberías) individuales en el “pipeline”.\n\nLas versiones recientes de R también tienen un operador de tuberías nativo (|&gt;).\n\n\nmtcars |&gt; head(2) #  es lo mismo que  head(mtcars, 2)\n\n              mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4      21   6  160 110  3.9 2.620 16.46  0  1    4    4\nMazda RX4 Wag  21   6  160 110  3.9 2.875 17.02  0  1    4    4\n\nmtcars |&gt; subset(cyl == 4) |&gt; nrow()  \n\n[1] 11\n\n\n\n\n1.7.3 Data frames avanzados tibbels\nEL paquete tibble proporciona un objeto de tipo data frame mejorado: tbl_df. Un tibble se puede crear de cuatro maneras diferentes.\n\nA partir de vectores columna:\n\n\ntibble(\n  x = c(\"a\", \"b\"),\n  y = c(1, 2),\n  z = c(T, F)\n)\n\n# A tibble: 2 × 3\n  x         y z    \n  &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt;\n1 a         1 TRUE \n2 b         2 FALSE\n\n\n\nEscribiendo en texto por columnas:\n\n\ntribble(\n  ~x, ~y, ~z,\n  \"a\", 1, T,\n  \"b\", 2, F\n)\n\n# A tibble: 2 × 3\n  x         y z    \n  &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt;\n1 a         1 TRUE \n2 b         2 FALSE\n\n\n\nCreando un tibble a partir de otro objeto de las clases matrix o data.frame:\n\n\ndata.frame(\n  x = c(\"a\", \"b\"),\n  y = c(1, 2),\n  z = c(T, F)\n) %&gt;% \nas_tibble\n\n# A tibble: 2 × 3\n  x         y z    \n  &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt;\n1 a         1 TRUE \n2 b         2 FALSE\n\n\n\nCreando un tibble a partir de vectores con nombre:\n\n\nc(x = \"a\", y = \"b\", z = 1) %&gt;%\n  enframe(name = \"x\", value = \"y\")\n\n# A tibble: 3 × 2\n  x     y    \n  &lt;chr&gt; &lt;chr&gt;\n1 x     a    \n2 y     b    \n3 z     1    \n\n\n\nDiferencias entre tibble y data.frame\n\nUn tibble nunca cambia el tipo de entrada. Ya no hay que preocuparse de que los caracteres se conviertan automáticamente en cadenas.\nUn tibble puede tener columnas que son listas.\nUn tibble puede tener nombres de variables no estándar. Pueden empezar por un número o contener espacios. Para utilizarlo se refiere a estos en un backtick, por ejemplo, peso en Kg.\nSólo recicla vectores de longitud 1.\nNo tiene como atributo nombres de filas row.names.\nPor defecto, tibble() imprime sólo las diez primeras filas, todas las columnas que caben en la pantalla, y las clases de las columnas\n\n\n\npenguins\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n# data.frame(penguins) recordad que esto imprime todo el data frame\n\n\nglimpse nos da la versión transpuesta de print()\n\n\npenguins %&gt;% glimpse\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n\nEl subconjunto de un tibble ([]) siempre devuelve otro tibble y nunca un vector (en contraste con los objetos de un data.frame).\n\n\ndata.frame(penguins) %&gt;% .[, \"species\"] %&gt;% class\n\n[1] \"factor\"\n\n\n\npenguins[, \"species\"] %&gt;% class\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\n\nEl subconjunto de un data.frame busca el nombre de la variable más parecida\n\n\nnames(data.frame(penguins))\n\n[1] \"species\"           \"island\"            \"bill_length_mm\"   \n[4] \"bill_depth_mm\"     \"flipper_length_mm\" \"body_mass_g\"      \n[7] \"sex\"               \"year\"             \n\nhead(data.frame(penguins)$spec)\n\n[1] Adelie Adelie Adelie Adelie Adelie Adelie\nLevels: Adelie Chinstrap Gentoo\n\n\n\ntibble no permite la coincidencia parcial, es decir, siempre se debe proporcionar el nombre completo de la columna.\n\n\nhead(penguins$spec)\n\nWarning: Unknown or uninitialised column: `spec`.\n\n\nNULL\n\n\n\nhead(penguins$species)\n\n[1] Adelie Adelie Adelie Adelie Adelie Adelie\nLevels: Adelie Chinstrap Gentoo\n\n\n\nLas tibbles dan mejores mensajes Warnings y Errors para solucionar problemas.\n\n\n\n1.7.4 Lectura de datos de texto rectangulares readr\nEl paquete readr proporciona funciones de lectura y escritura para múltiples formatos de archivo:\n\nread_delim(): archivos delimitados en general\nread_csv(): archivos separados por comas\nread_csv2(): archivos separados por punto y coma. En la mayoría de los países europeos, Microsoft Excel utiliza ; como delimitador común\nread_tsv(): archivos separados por tabulaciones\nread_fwf(): archivos de ancho fijo\nread_table(): archivos separados por espacios en blanco\nread_log(): archivos de registro web\nConvenientemente, las funciones write_*() funcionan de forma análoga.\nSe utiliza el paquete readxl para archivos de Excel,\nEl paquete haven para archivos de Stata, SAS y SPSS,\nEl paquete googlesheets4 para Google Sheets\nEl paquete rvest para archivos HTML. Esta es la librería de referencia en el contexto de la extracción de datos de la web con R\n\nPara ilustrar el paquete readr, hemos creado previamente un archivo csv que contiene los datos de los pingüinos, utilizando write_csv(penguins, archivo = \"datos/penguins.csv\").\n\ndata &lt;- read_csv(file = \"./datos/penguins.csv\")\n\nNew names:\nRows: 344 Columns: 9\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(3): species, island, sex dbl (6): ...1, bill_length_mm, bill_depth_mm,\nflipper_length_mm, body_mass_g...\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n\n\ndata &lt;- read_csv(file = \"./datos/penguins.csv\", col_select = c(species, island))\n\nNew names:\nRows: 344 Columns: 2\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(2): species, island\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n\n\ndata &lt;- read_csv(file = \"./datos/penguins.csv\",\n                 col_names = paste(\"Var\", 1:8, sep = \"_\"))\n\nRows: 345 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): Var_2, Var_3, Var_4, Var_5, Var_6, Var_7, Var_8, X9\ndbl (1): Var_1\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\ndata &lt;- read_csv(file = \"./datos/penguins.csv\", skip = 5)\n\nRows: 339 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Adelie, Torgersen, female\ndbl (6): 5, 36.7, 19.3, 193, 3450, 2007\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nObserva que la salida de cualquier función read_*() es un objeto tibble.\nreadr imprime las especificaciones de las columnas después de la importación.\nPor defecto, readr intenta inferir el tipo de columna (por ejemplo, int, dbl, chr, fct, date, lgl) a partir de las primeras 1.000 filas y analiza las columnas en consecuencia.\nUna buena práctica es especificar de forma explícita el formato de las columnas.\n\n\nread_csv(\n  archivo = \"./datos/penguins.csv\",\n  col_types = cols(\n    species = col_character(),\n    año = col_datetime(formato = \"%Y\"),\n    isla = col_skip())\n  )\n\n\nAnalizar sólo las primeras 1.000 filas es eficiente, pero puede llevar a conjeturas erróneas:\n\n\nread_csv(file = \"./datos/penguins.csv\", guess_max = 2000)\n\n\nEncuentra más información y funciones de readr en la hoja de trucos.\nA veces puedes tener problemas al leer datos de texto (tipo carácter): los signos especiales como ö, ä o ü pueden ser codificados de forma extraña como símbolos. En esos casos debes controlar la codificación los datos en la función read_csv (por ejemplo, UTF-8)\nSupongamos que deseamos dejar de utilizar los archivos .xlsx y .csv ya que no son capaces de almacenar de forma fiable los metadatos (por ejemplo, los tipos de datos).\nLas funciones write_rds() y read_rds() proporcionan una buena alternativa para serializar tus objetos de R (por ejemplo, tibbles, modelos) y almacenarlos como archivos .rds.\nMás info sobre archivos rds\n\n\npenguins %&gt;% \n  write_rds(file = \"./datos/penguins.rds\")\n\n\npenguins &lt;- read_rds(file = \"./datos/penguins.rds\")\n\nNota que:\n\nwrite_rds() solo puede utilizarse para guardar un objeto a la vez,\nun archivo .rds cargado debe ser almacenado en una nueva variable, es decir, darle un nuevo nombre,\nread_rds() ¡conserva los tipos de datos!\n\n\n\n1.7.5 Ordenando datos tidyr\n\nPodemos cambiar el formato de una tabla de datos con las funciones: pivot_longer() y pivot_wider() tal como se muestra en la siguiente figura.\n\n\n\n\n\n\n\n\n\n\n\nEjemplo: Tomamos la tabla de datos no tidy del Ejemplo 1 de estas notas y tratamos de estructurarla correctamente. La tabla la hemos llamado ejemplo1 y os la presentamos a continuación:\n\n\n\n# A tibble: 3 × 4\n  species   Biscoe Dream Torgersen\n  &lt;fct&gt;      &lt;int&gt; &lt;int&gt;     &lt;int&gt;\n1 Adelie        44    56        52\n2 Chinstrap     NA    68        NA\n3 Gentoo       124    NA        NA\n\n\nVamos a aplicar la función pivot_longer para hacerla tidy:\n\nnueva &lt;- ejemplo1 %&gt;% \n  pivot_longer(\n    cols = c(Biscoe, Dream, Torgersen),\n    names_to = \"Isla\", values_to = \"Frecuencia\"\n  )\n\nnueva\n\n# A tibble: 9 × 3\n  species   Isla      Frecuencia\n  &lt;fct&gt;     &lt;chr&gt;          &lt;int&gt;\n1 Adelie    Biscoe            44\n2 Adelie    Dream             56\n3 Adelie    Torgersen         52\n4 Chinstrap Biscoe            NA\n5 Chinstrap Dream             68\n6 Chinstrap Torgersen         NA\n7 Gentoo    Biscoe           124\n8 Gentoo    Dream             NA\n9 Gentoo    Torgersen         NA\n\n\n\nLa función pivot_wider() invierte el efecto de pivot_longer(). Lo dejamos como Ejercicio\nPuedes encontrar más información acerca de pivot_*() en tidyr.\nOtra cosa que podemos hacer con tidyr es “agrupar” datos de manera que cada grupo se convierte en una sola fila en un data frame. La función nest() genera datos anidados en un data frame con una fila por species y year.\n\n\nnested_penguins &lt;- penguins %&gt;% \n    nest(nested_data = \n           c(island, bill_length_mm, \n             bill_depth_mm,flipper_length_mm,\n             body_mass_g, sex))\nnested_penguins\n\n# A tibble: 9 × 3\n  species    year nested_data      \n  &lt;fct&gt;     &lt;int&gt; &lt;list&gt;           \n1 Adelie     2007 &lt;tibble [50 × 6]&gt;\n2 Adelie     2008 &lt;tibble [50 × 6]&gt;\n3 Adelie     2009 &lt;tibble [52 × 6]&gt;\n4 Gentoo     2007 &lt;tibble [34 × 6]&gt;\n5 Gentoo     2008 &lt;tibble [46 × 6]&gt;\n6 Gentoo     2009 &lt;tibble [44 × 6]&gt;\n7 Chinstrap  2007 &lt;tibble [26 × 6]&gt;\n8 Chinstrap  2008 &lt;tibble [18 × 6]&gt;\n9 Chinstrap  2009 &lt;tibble [24 × 6]&gt;\n\n\n\nLos datos anidados nested_penguins contienen tibbles con seis columnas cada uno y un número de observaciones que pueden ser distintos.\nLos datos anidados son útiles si queremos aplicar funciones a cada subgrupo de datos (por ejemplo, comparar estadísticos por especie).\nPara deshacer las estructuras de datos anidados se puede usar la función tidyr::unnest().\n\n\nnested_penguins %&gt;% unnest(cols = c(nested_data)) \n\n# A tibble: 344 × 8\n   species  year island    bill_length_mm bill_depth_mm flipper_length_mm\n   &lt;fct&gt;   &lt;int&gt; &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;\n 1 Adelie   2007 Torgersen           39.1          18.7               181\n 2 Adelie   2007 Torgersen           39.5          17.4               186\n 3 Adelie   2007 Torgersen           40.3          18                 195\n 4 Adelie   2007 Torgersen           NA            NA                  NA\n 5 Adelie   2007 Torgersen           36.7          19.3               193\n 6 Adelie   2007 Torgersen           39.3          20.6               190\n 7 Adelie   2007 Torgersen           38.9          17.8               181\n 8 Adelie   2007 Torgersen           39.2          19.6               195\n 9 Adelie   2007 Torgersen           34.1          18.1               193\n10 Adelie   2007 Torgersen           42            20.2               190\n# ℹ 334 more rows\n# ℹ 2 more variables: body_mass_g &lt;int&gt;, sex &lt;fct&gt;\n\n\n\nEjemplo: Dividir y combinar múltiples columnas en una sola.\n\n\npenguins %&gt;% unite(col = \"specie_sex\",\n                   c(species, sex), sep = \"_\", remove = T)\n\n# A tibble: 344 × 7\n   specie_sex  island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;       &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie_male Torge…           39.1          18.7               181        3750\n 2 Adelie_fem… Torge…           39.5          17.4               186        3800\n 3 Adelie_fem… Torge…           40.3          18                 195        3250\n 4 Adelie_NA   Torge…           NA            NA                  NA          NA\n 5 Adelie_fem… Torge…           36.7          19.3               193        3450\n 6 Adelie_male Torge…           39.3          20.6               190        3650\n 7 Adelie_fem… Torge…           38.9          17.8               181        3625\n 8 Adelie_male Torge…           39.2          19.6               195        4675\n 9 Adelie_NA   Torge…           34.1          18.1               193        3475\n10 Adelie_NA   Torge…           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 1 more variable: year &lt;int&gt;\n\n\n\nEjemplo: Separar una sola columna, que contiene varios valores, en varias columnas.\n\n\npenguins %&gt;% separate(bill_length_mm, sep = 2, into = c(\"cm\", \"mm\"))\n\n# A tibble: 344 × 9\n   species island  cm    mm    bill_depth_mm flipper_length_mm body_mass_g sex  \n   &lt;fct&gt;   &lt;fct&gt;   &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt;\n 1 Adelie  Torger… 39    \".1\"           18.7               181        3750 male \n 2 Adelie  Torger… 39    \".5\"           17.4               186        3800 fema…\n 3 Adelie  Torger… 40    \".3\"           18                 195        3250 fema…\n 4 Adelie  Torger… &lt;NA&gt;  &lt;NA&gt;           NA                  NA          NA &lt;NA&gt; \n 5 Adelie  Torger… 36    \".7\"           19.3               193        3450 fema…\n 6 Adelie  Torger… 39    \".3\"           20.6               190        3650 male \n 7 Adelie  Torger… 38    \".9\"           17.8               181        3625 fema…\n 8 Adelie  Torger… 39    \".2\"           19.6               195        4675 male \n 9 Adelie  Torger… 34    \".1\"           18.1               193        3475 &lt;NA&gt; \n10 Adelie  Torger… 42    \"\"             20.2               190        4250 &lt;NA&gt; \n# ℹ 334 more rows\n# ℹ 1 more variable: year &lt;int&gt;\n\n\n\nEjemplo: Separar una sola columna, que contiene varios valores, en varias filas.\n\n\npenguins %&gt;% separate_rows(island, sep = \"s\", convert = T)\n\n# A tibble: 564 × 8\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;chr&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torger           39.1          18.7               181        3750\n 2 Adelie  en               39.1          18.7               181        3750\n 3 Adelie  Torger           39.5          17.4               186        3800\n 4 Adelie  en               39.5          17.4               186        3800\n 5 Adelie  Torger           40.3          18                 195        3250\n 6 Adelie  en               40.3          18                 195        3250\n 7 Adelie  Torger           NA            NA                  NA          NA\n 8 Adelie  en               NA            NA                  NA          NA\n 9 Adelie  Torger           36.7          19.3               193        3450\n10 Adelie  en               36.7          19.3               193        3450\n# ℹ 554 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n1.7.6 Manejo de NAs\nSupongamos que tenemos una tibble que llamamos incompl_penguins y queremos hacer explícitos los valores perdidos porque la tabla luce de la siguiente forma:\n\n\n# A tibble: 4 × 3\n  species    year measurement\n  &lt;chr&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 Adelie     2007        41.6\n2 Adelie     2008        46.5\n3 Gentoo     2008        73.4\n4 Chinstrap  2007        NA  \n\n\nPara hacer explícitos los NAs, ejecutamos las siguientes instrucciones:\n\nincompl_penguins %&gt;% \n  complete(species, year, fill = list(measurement = NA))\n\n# A tibble: 6 × 3\n  species    year measurement\n  &lt;chr&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 Adelie     2007        41.6\n2 Adelie     2008        46.5\n3 Chinstrap  2007        NA  \n4 Chinstrap  2008        NA  \n5 Gentoo     2007        NA  \n6 Gentoo     2008        73.4\n\n\nSi, por el contrario, queremos hacer implícitos los valores perdidos, ejecutamos la instrucción:\n\nincompl_penguins %&gt;% \n  drop_na(measurement)\n\n# A tibble: 3 × 3\n  species  year measurement\n  &lt;chr&gt;   &lt;dbl&gt;       &lt;dbl&gt;\n1 Adelie   2007        41.6\n2 Adelie   2008        46.5\n3 Gentoo   2008        73.4\n\n\n\nPara reemplazar los valores faltantes por la entrada siguiente:\n\n\nincompl_penguins %&gt;% \n  fill(measurement, .direction = \"down\")\n\n# A tibble: 4 × 3\n  species    year measurement\n  &lt;chr&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 Adelie     2007        41.6\n2 Adelie     2008        46.5\n3 Gentoo     2008        73.4\n4 Chinstrap  2007        73.4\n\n\n\nReemplazar los valores que faltan por un valor predefinido.\n\n\nincompl_penguins %&gt;%\n  replace_na(replace = list(measurement = mean(.$measurement, na.rm = T)))\n\n# A tibble: 4 × 3\n  species    year measurement\n  &lt;chr&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 Adelie     2007        41.6\n2 Adelie     2008        46.5\n3 Gentoo     2008        73.4\n4 Chinstrap  2007        53.8\n\n\n\nMás información en la Hoja de trucos de tidyr.\nRecordad: Los argumentos de una función precedidos por un punto en el tidyverse pueden deberse a dos razones:\n\nLa función es todavía una versión no acabada, es decir, los desarrolladores aún piensan en la mejor manera de implementar y nombrar la función.\nLa función se aplica regularmente dentro de otra función para no confundir los argumentos de la función interna y la externa.\n\n\n\n\n\n1.7.7 Manipulación de datos dplyr\ndplyr proporciona un conjunto de funciones para manipular objetos tibbles. Las funciones están representadas por “acciones” que reflejan las operaciones subyacentes y siempre dan como resultado un tibble nuevo o modificado.\n\n1.7.7.1 Operaciones por filas\n\nfilter() selecciona las filas que cumplen uno o varios criterios lógicos\nslice() selecciona las filas en función de su ubicación en los datos\narrange() cambia el orden de las filas\n\nEjemplo: Seleccionar un data frame con los pingüinos de la especie “Adelie”.\n\npenguins %&gt;% \n  filter(species == \"Adelie\")\n\n# A tibble: 152 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 142 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nEjemplo: Encontrar los pingüinos con NAs en la variable bill_length_mm.\n\npenguins %&gt;% \n  filter(is.na(bill_length_mm) == T)\n\n# A tibble: 2 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen             NA            NA                NA          NA\n2 Gentoo  Biscoe                NA            NA                NA          NA\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nEjemplo: Seleccionar los pingüinos observados antes del año 2008 o después del año 2008 y que tienen masa corporal entre 3800 y 4000 gramos.\n\npenguins %&gt;% \n  filter(between(body_mass_g, 3800, 4000) & (year &lt; 2008 | year &gt; 2008))\n\n# A tibble: 28 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.5          17.4               186        3800\n 2 Adelie  Torgersen           38.6          21.2               191        3800\n 3 Adelie  Biscoe              35.9          19.2               189        3800\n 4 Adelie  Biscoe              38.2          18.1               185        3950\n 5 Adelie  Biscoe              38.8          17.2               180        3800\n 6 Adelie  Biscoe              35.3          18.9               187        3800\n 7 Adelie  Biscoe              40.5          18.9               180        3950\n 8 Adelie  Dream               37.2          18.1               178        3900\n 9 Adelie  Dream               40.9          18.9               184        3900\n10 Adelie  Dream               38.8          20                 190        3950\n# ℹ 18 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nEjemplo: Seleccionar los pingüinos de acuerdo a su ubicación.\n\npenguins %&gt;% \n  slice(23:27)\n\n# A tibble: 5 × 8\n  species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Biscoe           35.9          19.2               189        3800\n2 Adelie  Biscoe           38.2          18.1               185        3950\n3 Adelie  Biscoe           38.8          17.2               180        3800\n4 Adelie  Biscoe           35.3          18.9               187        3800\n5 Adelie  Biscoe           40.6          18.6               183        3550\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nEjemplo: Seleccionar los 5 primeros pingüinos\n\npenguins %&gt;% \n  slice_head(n = 5) \n\n# A tibble: 5 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n# alternativamente: \n#slice_head(frac = 0.05)\n\nEjemplo: Seleccionar una muestra aleatoria de n pingüinos\n\npenguins %&gt;% \n  slice_sample(n = 5)\n\n# A tibble: 5 × 8\n  species   island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;     &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Chinstrap Dream               52.8          20                 205        4550\n2 Adelie    Torgersen           37.3          20.5               199        3775\n3 Chinstrap Dream               43.2          16.6               187        2900\n4 Gentoo    Biscoe              47.5          14.2               209        4600\n5 Gentoo    Biscoe              52.2          17.1               228        5400\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\nEjemplo: Seleccionar los n pingüinos con el pico más grande.\n\n\npenguins %&gt;% \n  slice_max(bill_length_mm, n = 5)\n\n# A tibble: 5 × 8\n  species   island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;     &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Gentoo    Biscoe           59.6          17                 230        6050\n2 Chinstrap Dream            58            17.8               181        3700\n3 Gentoo    Biscoe           55.9          17                 228        5600\n4 Chinstrap Dream            55.8          19.8               207        4000\n5 Gentoo    Biscoe           55.1          16                 230        5850\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nEjemplo: Seleccionar los cinco pingüinos con menor masa corporal.\n\npenguins %&gt;% \n  arrange(body_mass_g) %&gt;% \n  slice_head(n = 5)  # equivalentente a: slice_min(body_mass_g, n = 3)\n\n# A tibble: 5 × 8\n  species   island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;     &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Chinstrap Dream            46.9          16.6               192        2700\n2 Adelie    Biscoe           36.5          16.6               181        2850\n3 Adelie    Biscoe           36.4          17.1               184        2850\n4 Adelie    Biscoe           34.5          18.1               187        2900\n5 Adelie    Dream            33.1          16.1               178        2900\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n1.7.7.2 Operaciones por columnas\n\nselect() selecciona determinadas columnas\nrename() cambia los nombres de las columnas\nrelocate() cambia el orden de las columnas\nmutate() transforma los valores de las columnas y/o crea nuevas columnas\n\nEjemplo: Selección por número de la(s) columna(s)\n\npenguins %&gt;% \n  select(1:3) %&gt;% \n  glimpse\n\nRows: 344\nColumns: 3\n$ species        &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie,…\n$ island         &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, …\n$ bill_length_mm &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, 42.…\n\n\nEjemplo: Selección por nombre de la(s) columna(s)\n\npenguins %&gt;% \n  select(species, island, bill_length_mm) %&gt;% \n  glimpse\n\nRows: 344\nColumns: 3\n$ species        &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie,…\n$ island         &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, …\n$ bill_length_mm &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, 42.…\n\n\nEjemplo: Seleccionar todas las columnas\n\npenguins %&gt;% \n  select(everything()) %&gt;% \n  glimpse\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n# select(last_col())\n\nEjemplo: Seleccionar las columnas cuyos nombres empiezan por un patrón específico\n\npenguins %&gt;% \n  select(starts_with(\"bill\")) %&gt;% \n  glimpse\n\nRows: 344\nColumns: 2\n$ bill_length_mm &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, 42.…\n$ bill_depth_mm  &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, 20.…\n\n# ends_with()\n\n\npenguins %&gt;% \n  select(contains(\"e\") & contains(\"a\")) %&gt;% \n  glimpse\n\nRows: 344\nColumns: 1\n$ year &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007,…\n\n\nEjemplo: Seleccionar columnas en base a una expresión regular (regex)\n\npenguins %&gt;% \n  select(matches(\"_\\\\w*_mm$\")) %&gt;% \n  glimpse\n\nRows: 344\nColumns: 3\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n\n\n\npenguins %&gt;% \n  select(where(is.numeric)) %&gt;% \n  glimpse\n\nRows: 344\nColumns: 5\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n\nEjercicio: ¿Qué columnas devuelven las siguientes consultas?\n\n\npenguins %&gt;% \n  select(ends_with(\"mm\"))\n\n\npenguins %&gt;% \n  select(-contains(\"mm\"))\n\n\npenguins %&gt;% \n  select(where(~ is.numeric(.))) %&gt;%  # select(where(is.numeric))\n  select(where(~ mean(., na.rm = T) &gt; 1000))\n\nEjemplo: Cambiar el nombre de la columna body_mass_g a bm y sex a gender.\n\npenguins %&gt;% rename(bm = body_mass_g, gender = sex) %&gt;% \n  colnames()\n\n[1] \"species\"           \"island\"            \"bill_length_mm\"   \n[4] \"bill_depth_mm\"     \"flipper_length_mm\" \"bm\"               \n[7] \"gender\"            \"year\"             \n\n\nEjemplo: Cambiar los nombres de las columnas que incluyen \"mm\" a mayúsculas.\n\npenguins %&gt;% rename_with(.fn = toupper, .cols = contains(\"mm\")) %&gt;% \n  colnames()\n\n[1] \"species\"           \"island\"            \"BILL_LENGTH_MM\"   \n[4] \"BILL_DEPTH_MM\"     \"FLIPPER_LENGTH_MM\" \"body_mass_g\"      \n[7] \"sex\"               \"year\"             \n\n\nEjemplo: Cambiar el orden de las columnas en la tibble de acuerdo al siguiente esquema:\n\ncolocar “especie” después de “masa corporal”.\ncolocar “sexo” antes de “especie”.\ncolocar “isla” al final\n\n\npenguins %&gt;% \n  relocate(species, .after = body_mass_g) %&gt;%\n  relocate(sex, .before = species) %&gt;%\n  relocate(island, .after = last_col()) %&gt;%\n  colnames()\n\n[1] \"bill_length_mm\"    \"bill_depth_mm\"     \"flipper_length_mm\"\n[4] \"body_mass_g\"       \"sex\"               \"species\"          \n[7] \"year\"              \"island\"           \n\n\nEjemplo: Crear una nueva variable bm_kg que ponga body_mass_g en kilogramos.\n\npenguins %&gt;% \n  mutate(bm_kg = body_mass_g/1000, .keep = \"all\", .after = island) %&gt;% \n  slice_head(n = 5)\n\n# A tibble: 5 × 9\n  species island    bm_kg bill_length_mm bill_depth_mm flipper_length_mm\n  &lt;fct&gt;   &lt;fct&gt;     &lt;dbl&gt;          &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;\n1 Adelie  Torgersen  3.75           39.1          18.7               181\n2 Adelie  Torgersen  3.8            39.5          17.4               186\n3 Adelie  Torgersen  3.25           40.3          18                 195\n4 Adelie  Torgersen NA              NA            NA                  NA\n5 Adelie  Torgersen  3.45           36.7          19.3               193\n# ℹ 3 more variables: body_mass_g &lt;int&gt;, sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\nUsamos .keep para especificar las columnas que se mantendrán después de la manipulación.\nUsamos .before/.after para especificar la posición de la nueva columna.\nPara anular una columna dada simplemente utiliza el mismo nombre de columna.\nPara mantener únicamente la nueva columna utiliza dplyr::transmute().\n\nEjemplo: Codificación de una variable categórica con “C” niveles de factor en “C-1” binarias o dummies.\n\npenguins %&gt;% \n  mutate(\n    sex_binary = case_when(\n      sex == \"male\" ~ 1,\n      sex == \"female\" ~ 0),\n    .keep = \"all\", .after = island\n  ) %&gt;% \n  slice_head(n = 3)\n\n# A tibble: 3 × 9\n  species island    sex_binary bill_length_mm bill_depth_mm flipper_length_mm\n  &lt;fct&gt;   &lt;fct&gt;          &lt;dbl&gt;          &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;\n1 Adelie  Torgersen          1           39.1          18.7               181\n2 Adelie  Torgersen          0           39.5          17.4               186\n3 Adelie  Torgersen          0           40.3          18                 195\n# ℹ 3 more variables: body_mass_g &lt;int&gt;, sex &lt;fct&gt;, year &lt;int&gt;\n\n\nEjemplo:\ncase_when: - Versión vectorizada de if_else\n\nFórmulas de dos lados: El LHS comprueba la condición, el RHS especifica el valor de sustitución\nPara los casos no coincidentes, la función devuelve NA\nUtiliza el LHS TRUE para capturar todos los casos no especificados explícitamente de antemano.\n\nEjemplo: Transformar las variables de medidas a metros\n\npenguins %&gt;% \n  mutate(\n    across(contains(\"mm\"), ~ ./1000),\n    .keep = \"all\"\n  ) %&gt;% \n  slice_head(n = 3)\n\n# A tibble: 3 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;int&gt;\n1 Adelie  Torgersen         0.0391        0.0187             0.181        3750\n2 Adelie  Torgersen         0.0395        0.0174             0.186        3800\n3 Adelie  Torgersen         0.0403        0.018              0.195        3250\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nEjemplo:\nacross:\n\nAplicar la misma transformación en varias columnas\nTe permite utilizar la semántica que conoces de la función select().\nNo requiere que se especifique explícitamente un nombre de columna, ya que sólo transforma las columnas existentes\n\nEjemplo: Definir species, island y sex como variables categóricas, es decir factores, usando across().\n\npenguins %&gt;% \n  mutate(\n    across(where(is.character), as.factor),\n    .keep = \"all\"\n  ) %&gt;% \n  slice_head(n = 3)\n\n# A tibble: 3 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n1.7.7.3 Operaciones sobre datos agrupados\n\ngroup_by() divide los datos en función de una o varias columnas\nsummarise() reduce un grupo de datos en una sola fila\n\nEjemplo: Partición de los datos por tipo de especie del pingüino\n\npenguins %&gt;% group_by(species)\n\n# A tibble: 344 × 8\n# Groups:   species [3]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nUtiliza group_keys(), group_indices() y group_vars() para acceder a las claves de agrupación, los índices de grupo por fila y las variables de agrupación.\n\ngroup_by() cambia la representación del tibble y lo transforma en un data frame agrupado (grouped_df). Esto nos permite operar en los subgrupos individualmente usando summarise().\n\nEjemplo:\n\npenguins %&gt;% group_by(species) %&gt;% \n  summarise(count = n(), .groups = \"drop\")\n\n# A tibble: 3 × 2\n  species   count\n  &lt;fct&gt;     &lt;int&gt;\n1 Adelie      152\n2 Chinstrap    68\n3 Gentoo      124\n\n\n\npenguins %&gt;% group_by(species, sex) %&gt;% summarise(count = n(), .groups = \"drop\")\n\n# A tibble: 8 × 3\n  species   sex    count\n  &lt;fct&gt;     &lt;fct&gt;  &lt;int&gt;\n1 Adelie    female    73\n2 Adelie    male      73\n3 Adelie    &lt;NA&gt;       6\n4 Chinstrap female    34\n5 Chinstrap male      34\n6 Gentoo    female    58\n7 Gentoo    male      61\n8 Gentoo    &lt;NA&gt;       5\n\n\n\npenguins %&gt;%\n  group_by(species) %&gt;%\n  summarise(\n    across(contains(\"mm\"), ~ mean(., na.rm = T), .names = \"{.col}_media\"),\n    .groups = \"drop\"\n  )\n\n# A tibble: 3 × 4\n  species   bill_length_mm_media bill_depth_mm_media flipper_length_mm_media\n  &lt;fct&gt;                    &lt;dbl&gt;               &lt;dbl&gt;                   &lt;dbl&gt;\n1 Adelie                    38.8                18.3                    190.\n2 Chinstrap                 48.8                18.4                    196.\n3 Gentoo                    47.5                15.0                    217.\n\n\n\nUtilizar group_by(), seguido de summarise() y ungroup() refleja el paradigma dividir-aplicar-combinar del análisis de datos: Dividir los datos en particiones, aplicar alguna función a los datos y luego combinar los resultados.\n\n\n.add = T permite añadir nuevas variables de agrupación (si no, se anula la primera)\n\n\npenguins %&gt;% \n  group_by(species) %&gt;% \n  group_by(year, .add = T)   # equivalente a: group_by(species, year)\n\n# A tibble: 344 × 8\n# Groups:   species, year [9]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\npenguins %&gt;%\n  group_by(species) %&gt;%\n  summarise(\n    across(\n      contains(\"mm\"),\n      list(media = ~ mean(., na.rm = T), sd = ~ sd(., na.rm = T)),\n      .names = \"{.col}_{.fn}\"\n    ),\n    .groups = \"drop\"\n  )\n\n# A tibble: 3 × 7\n  species   bill_length_mm_media bill_length_mm_sd bill_depth_mm_media\n  &lt;fct&gt;                    &lt;dbl&gt;             &lt;dbl&gt;               &lt;dbl&gt;\n1 Adelie                    38.8              2.66                18.3\n2 Chinstrap                 48.8              3.34                18.4\n3 Gentoo                    47.5              3.08                15.0\n# ℹ 3 more variables: bill_depth_mm_sd &lt;dbl&gt;, flipper_length_mm_media &lt;dbl&gt;,\n#   flipper_length_mm_sd &lt;dbl&gt;\n\n\nEjemplo: Las funciones de resumen, por ejemplo, mean() o sd() operan en particiones de los datos en lugar de en los datos completos\n\npenguins %&gt;%\n  group_by(species) %&gt;% \n  mutate(stand_bm = (body_mass_g - mean(body_mass_g, na.rm = T))\n         /sd(body_mass_g, na.rm = T)) %&gt;% \n  glimpse\n\nRows: 344\nColumns: 9\nGroups: species [3]\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n$ stand_bm          &lt;dbl&gt; 0.107591350, 0.216626878, -0.982763938, NA, -0.54662…\n\n\nEjemplo: Calcular franjas para la masa corporal de acuerdo a la cantidad de desviaciones estándar de la media. Agrupar los datos según estos intervalos.\n\nbm_breaks &lt;- mean(penguins$body_mass_g, \n                  na.rm = T) - (-3:3) *\n  sd(penguins$body_mass_g,na.rm = T)\n\npenguins %&gt;% \n  group_by(species, bm_bin = cut(body_mass_g, breaks = bm_breaks)) %&gt;%\n  summarise(count = n(), .groups = \"drop\")\n\n\n\n# A tibble: 12 × 3\n   species   bm_bin              count\n   &lt;fct&gt;     &lt;fct&gt;               &lt;int&gt;\n 1 Adelie    (2.6e+03,3.4e+03]      39\n 2 Adelie    (3.4e+03,4.2e+03]      87\n 3 Adelie    (4.2e+03,5e+03]        25\n 4 Adelie    &lt;NA&gt;                    1\n 5 Chinstrap (2.6e+03,3.4e+03]      11\n 6 Chinstrap (3.4e+03,4.2e+03]      50\n 7 Chinstrap (4.2e+03,5e+03]         7\n 8 Gentoo    (3.4e+03,4.2e+03]       6\n 9 Gentoo    (4.2e+03,5e+03]        56\n10 Gentoo    (5e+03,5.81e+03]       52\n11 Gentoo    (5.81e+03,6.61e+03]     9\n12 Gentoo    &lt;NA&gt;                    1\n\n\nEjemplo: Filtrar en las particiones en lugar de en la totalidad de los datos\n\npenguins %&gt;% \n  group_by(species, island) %&gt;% \n  filter(flipper_length_mm == max(flipper_length_mm, na.rm = T))\n\n# A tibble: 5 × 8\n# Groups:   species, island [5]\n  species   island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;     &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie    Dream               40.8          18.9               208        4300\n2 Adelie    Biscoe              41            20                 203        4725\n3 Adelie    Torgersen           44.1          18                 210        4000\n4 Gentoo    Biscoe              54.3          15.7               231        5650\n5 Chinstrap Dream               49            19.6               212        4300\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nEjemplo: Utilizar group_by() seguido de nest() para producir un data frame anidado\n\npenguins %&gt;% \n  group_by(species, year) %&gt;% \n  tidyr::nest()\n\n# A tibble: 9 × 3\n# Groups:   species, year [9]\n  species    year data             \n  &lt;fct&gt;     &lt;int&gt; &lt;list&gt;           \n1 Adelie     2007 &lt;tibble [50 × 6]&gt;\n2 Adelie     2008 &lt;tibble [50 × 6]&gt;\n3 Adelie     2009 &lt;tibble [52 × 6]&gt;\n4 Gentoo     2007 &lt;tibble [34 × 6]&gt;\n5 Gentoo     2008 &lt;tibble [46 × 6]&gt;\n6 Gentoo     2009 &lt;tibble [44 × 6]&gt;\n7 Chinstrap  2007 &lt;tibble [26 × 6]&gt;\n8 Chinstrap  2008 &lt;tibble [18 × 6]&gt;\n9 Chinstrap  2009 &lt;tibble [24 × 6]&gt;\n\n\nPuedes encontrar más información de group_by() ejecutando vignette(\"grouping\").\n\n\n1.7.7.4 Otras operaciones con dlpyr\n\ndistinct() selecciona sólo filas únicas\n\n\npenguins %&gt;% \n  distinct(species, island)\n\n# A tibble: 5 × 2\n  species   island   \n  &lt;fct&gt;     &lt;fct&gt;    \n1 Adelie    Torgersen\n2 Adelie    Biscoe   \n3 Adelie    Dream    \n4 Gentoo    Biscoe   \n5 Chinstrap Dream    \n\n\n\npull() extrae columnas individuales como vectores\n\n\npenguins %&gt;% \n  pull(year)  # equivalente a: penguins$year\n\n\nif_else() sentencia if-else vectorizada.\n\n\npenguins %&gt;% select(species, island, body_mass_g) %&gt;% \n  mutate(penguin_size = if_else(body_mass_g &lt; 3500,\n                                \"pequeño\",\n                                \"grande\"))\n\n# A tibble: 344 × 4\n   species island    body_mass_g penguin_size\n   &lt;fct&gt;   &lt;fct&gt;           &lt;int&gt; &lt;chr&gt;       \n 1 Adelie  Torgersen        3750 grande      \n 2 Adelie  Torgersen        3800 grande      \n 3 Adelie  Torgersen        3250 pequeño     \n 4 Adelie  Torgersen          NA &lt;NA&gt;        \n 5 Adelie  Torgersen        3450 pequeño     \n 6 Adelie  Torgersen        3650 grande      \n 7 Adelie  Torgersen        3625 grande      \n 8 Adelie  Torgersen        4675 grande      \n 9 Adelie  Torgersen        3475 pequeño     \n10 Adelie  Torgersen        4250 grande      \n# ℹ 334 more rows\n\n\n\nlag() desplaza los valores de las columnas n posiciones hacia adelante\n\n\npenguins %&gt;% select(species, body_mass_g) %&gt;% \n  mutate(lagged_bm = lag(body_mass_g, n = 1))\n\n# A tibble: 344 × 3\n   species body_mass_g lagged_bm\n   &lt;fct&gt;         &lt;int&gt;     &lt;int&gt;\n 1 Adelie         3750        NA\n 2 Adelie         3800      3750\n 3 Adelie         3250      3800\n 4 Adelie           NA      3250\n 5 Adelie         3450        NA\n 6 Adelie         3650      3450\n 7 Adelie         3625      3650\n 8 Adelie         4675      3625\n 9 Adelie         3475      4675\n10 Adelie         4250      3475\n# ℹ 334 more rows\n\n\n\nCombinar diferentes data frames haciendo coincidir las filas en función de la “key”\n\n\n\n\n\n\n\n\n\n\nNota\n\nEn el enlace de Tidydatatutor puedes escribir código R y Tidyverse en tu navegador y ver cómo cambia el data frame en cada paso del pipeline que has escrito.\n\nAquí os dejamos el enlace para visualizar las instrucciones más utilizadas de tidyverse con el ejemplo de los pingüinos:\n\narrange()\nfilter()\nmutate()\nselect()\ngroup_by() %&gt;% slice()\ngroup_by() %&gt;% summarize()\n\n\n\n\n1.7.8 Visualización de datos ggplot2\nggplot2 es un sistema para crear gráficos de forma declarativa, basado en The Grammar of Graphics. Permite producir “gráficos modernos para el análisis de datos”.\nLa sintaxis de ggplot2 ayuda a pensar los gráficos de una manera nueva y más general que R base, por ejemplo, facilita el agregar las leyendas, los ejes, los colores, en comparación con R base.\nEn R gallery, R charts y en ggplot2 extensions puedes encontrar muchos ejemplos de gráficos con los códigos que os servirán de inspiración.\n\n\n1.7.9 Gramática básica de ggplot2\nUn ggplot necesita al menos tres cosas que hay que especificar: \n\nDatos: Normalmente un tibble del que se seleccionan las variables a visualizar. Siempre empezamos con ggplot(data = df) que le dice a {ggplot2} que vamos a trabajar con los datos df.\nEstética: Propiedades visuales que deseamos tenga nuestro gráfico. Por ejemplo, si deseamos visualizar la relación entre dos variables de df: var1 en el eje x y var2 en el eje y, debemos especificar: aes(x = var1, y = var2).\nGeometría: Forma geométrica que deseamos utilizar para representar los datos: puntos, líneas continuas, curvas, entre otras. Se especifica con geom_*(). Por ejemplo, geom_point() para hacer un diagrama de puntos.\n\nEjemplo:\n\npenguins %&gt;% \n  ggplot(aes(x=bill_length_mm, y = flipper_length_mm)) +\n  geom_point(na.rm = TRUE)\n\n\n\n\n\n\n\n\n\n\n1.7.10 Más elementos de la gramática …\n\nEscalas: Permiten asignar los valores que hay en los datos a los valores visuales de una estética (anulando los valores por defecto de esa estética). La sintáxis es scale_*(). Existen escalas para asignar valores de manera discreta, continua o manual. También escalas de localización, para color, relleno; tamaño y figuras.\n\n\n\n\n\n\n\n\n\n\n\nResúmenes: Permiten construir nuevas variables de resumen para hacer un gráfico. Por ejemplo: conteo, cuantiles, proporciones, curvas ajustadas. Se especifica con stat_*().\n\n\n\n\n\n\n\n\n\n\n\nSistema de coordenadas: Permite especificar cómo deseamos las coordenadas del gráfico. La sintáxis es coord_*.\n\n\n\n\n\n\n\n\n\n\n\nFacetas: Dividen un gráfico en múltiples subgráficos de acuerdo a una o varias variables discretas. La sintáxis es facet_*.\n\n\n\n\n\n\n\n\n\n\n\nAjustes de las posiciones: Permite indicar qué hacer con geometrías (geoms) que ocuparían la misma posición en el gráfico.\n\n\n\n\n\n\n\n\n\n\n\nTema: Valores visuales generales de un gráfico, como el fondo, las cuadrículas, los ejes, el tipo de letra predeterminado, los tamaños y los colores.\n\n\n\n\n\n\n\n\n\n\nHay muchas cosas más, puedes descargar la hoja de trucos de ggplot2 en castellano.\n\n\n1.7.11 Visualizando una variable cualitativa con ggplot2\nVamos a visualizar una variable cualitativa, con un tema que no es el que trae por defecto ggplot con fondo gris.\n\npenguins %&gt;% \n  ggplot(aes(x = species)) +\n  geom_bar(fill=\"blue\") + \n  labs(x=\"Especie\", y=\"Número de pingüinos\") +\n  theme_bw() +\n  theme(axis.text = element_text(size=20),\n        axis.title = element_text(size=20, face = \"bold\")) \n\n\n\n\n\n\n\n\n\n\n1.7.12 Cruzando dos variables cualitativas con ggplot2\nPara observar la distribución conjunta de dos variables cualitativas podemos emplear un gráfico de barras. Las siguientes instrucciones nos ayudan a visualizar la relación entre las variables species e island del conjunto de datos de los pingüinos.\n\npenguins %&gt;% ggplot() + \n  geom_bar(aes(species, fill=island),\n           position=\"dodge\") + coord_flip() +\n  guides(fill = guide_legend(title = \"Isla\")) +\n  labs(x=\"Número de pingüinos\", y=\"Especie\") +\n  theme_bw() +\n  theme(axis.text = element_text(size=20),\n        axis.title = element_text(size=20, face = \"bold\"),\n        legend.title = element_text(size=20)) \n\n\n\n\n\n\n\n\n\n\nSi deseamos que cada barra represente el 100% de la categoría, se indica en el argumento de geom_bar.\n\npenguins %&gt;% ggplot() + \n  geom_bar(aes(species, fill=island),\n           position=\"fill\") + coord_flip() +\n  guides(fill = guide_legend(title = \"Isla\")) +\n  labs(y=\"Proporción de pingüinos\", x=\"Especie\") +\n  theme_bw() +\n  theme(axis.text = element_text(size=20),\n        axis.title = element_text(size=20, face = \"bold\"),\n        legend.title = element_text(size=20))\n\n\n\n\n\n\n\n\n\n\n1.7.13 Visualizando una variable cuantitativa con ggplot2\n\npenguins %&gt;% \n  ggplot(aes(x = flipper_length_mm)) +\n  geom_histogram(na.rm = TRUE) +\n    labs(x=\"Longitud de la aleta en mm\", \n         y=\"Frecuencia absoluta\") + \n  theme_bw() +\n  theme(axis.text = element_text(size=20),\n        axis.title = element_text(size=20, face = \"bold\"))\n\n\n\n\n\n\n\n\n\n\n1.7.14 Cruzando una variable cuantitativa con una cualitativa\n\nggplot(data = penguins, aes(x = flipper_length_mm)) +\n  geom_histogram(aes(fill = species), \n                 alpha = 0.5, \n                 position = \"identity\",\n                 na.rm = TRUE) +\n  scale_fill_manual(values = c(\"darkorange\",\"purple\",\"cyan4\")) +\n  labs(x = \"Longitud de la aleta en mm\",\n       y = \"Frecuencia absoluta\") +\n  guides(fill = guide_legend(title = \"Especie\")) +\n  theme_bw() +\n  theme(axis.text = element_text(size=20),\n        axis.title = element_text(size=20, face = \"bold\"),\n        legend.title = element_text(size=20))\n\n\n\n\n\n\n\n\n\n\nPara que el gráfico sea más claro, se los puede separar con facetas: facet_grid(.~species) justo después de la capa geom_histogram.\n\nggplot(data = penguins, aes(x = flipper_length_mm)) +\n  geom_histogram(aes(fill = species), \n                 alpha = 0.5, \n                 position = \"identity\",\n                 na.rm = TRUE) +\n  facet_grid(.~species) +\n  scale_fill_manual(values = c(\"darkorange\",\"purple\",\"cyan4\")) +\n  labs(x = \"Longitud de la aleta en mm\",\n       y = \"Frecuencia absoluta\") +\n  guides(fill = guide_legend(title = \"Especie\")) +\n  theme_bw() +\n  theme(axis.text = element_text(size=20),\n        axis.title = element_text(size=20, face = \"bold\"),\n        legend.title = element_text(size=20))\n\n\n\n\n\n\n\n\n\nLa capa facet_grid admite dos variables como argumento, separadas por ~, las categorías de la primera definen las filas y las de la segunda, las columnas. Si solo se usa una, se ubica un punto en el lugar de la otra.\nOtra forma muy conveniente de cruzar una variable cuantitativa con otra cualitativa es usando boxplots.\n\n\nggplot(data = penguins, aes(x = species, y = flipper_length_mm)) +\n  geom_boxplot(aes(color = species), width = 0.3, \n               show.legend = FALSE) + \n  geom_jitter(aes(color = species), alpha = 0.5, \n              show.legend = FALSE, \n              position = position_jitter(width = 0.2, seed = 0)) +\n  scale_color_manual(values = c(\"darkorange\",\"purple\",\"cyan4\")) +\n  labs(x = \"Epecie\", y = \"Longitud de la aleta en mm\")\n\n\n\n\n\n\n\n\n\n\n\n1.7.14.1 Cruzando dos variables cuantitativas\n\nggplot(penguins) +\n  geom_point(mapping = aes(x = flipper_length_mm,\n                           y = body_mass_g,\n                           color = sex), size=3)+ theme_bw() +\n  theme(axis.text = element_text(size=20),\n        axis.title = element_text(size=20, face = \"bold\"),\n        legend.title = element_text(size=20)) + \n  guides(fill = guide_legend(title = \"Sexo\"))\n\n\n\n\n\n\n\n\n\n\n\n\n1.7.14.2 Códigos de color html\n\nPara utilizar colores personalizados, puedes consultar los códigos de color HTML (también llamados códigos hexadecimales, por ejemplo, #ff0000 para el rojo) en lugar de especificar los colores por su nombre predefinido en R.\nLa familia de funciones scale_colour_*() permite ajustar los valores de la estética color (por ejemplo, scale_colour_brewer() selecciona una paleta del famoso proyecto ColorBrewer).\n\n\n\n\n1.7.15 Ejemplos integrando las herramientas de tidyverse\n\nEjemplo 1:\n\n\npenguins_long &lt;- penguins %&gt;% \n  tidyr::pivot_longer(\n    cols = contains(\"mm\"),\n    names_to = \"var\", values_to = \"val\") %&gt;% \n  tidyr::drop_na()\n\npenguins_long %&gt;% \n  ggplot(aes(x = var, y = val, fill=var)) +\n  geom_boxplot() +\n  theme_bw() +\n  theme(legend.position=\"none\") +\n  labs(x=\"\", y=\"Medida en mm\") +\n  theme(axis.text = element_text(size=20),\naxis.title = element_text(size=20, face = \"bold\"),\nlegend.title = element_text(size=20))\n\n\n\n\n\n\n\n\n\nEjemplo 2:\n\n\npenguins %&gt;%\n  dplyr::count(species) %&gt;%\n  dplyr::mutate(prop = n / sum(n)) %&gt;%\n  ggplot() + geom_col(aes(x = prop, y = species)) +\n  labs(x=\"Proporción\", y=\"\") + \n  theme(axis.text = element_text(size=20),\naxis.title = element_text(size=20, face = \"bold\"),\nlegend.title = element_text(size=20)) \n\n\n\n\n\n\n\n\n\n\n1.7.16 Extensiones de ggplot\nHay muchas extensiones de ggplot, algunos ejemplos son:\n\nGGally\n\n\nlibrary(GGally)\nlibrary(gapminder)\ngapminder %&gt;% select(-country,-year) %&gt;% \n  ggpairs(aes(color=continent))\n\n\n\n\n\n\n\n\n\n\n\nggpubr proporciona algunas funciones fáciles de usar para crear y personalizar gráficos útiles para una publicación.\n\n\n\n\n\n\n\n\n\n\n\nPuedes ver más ejemplos en ‘ggplot2’ Based Publication Ready Plots.\nEn la cuenta de gitHub de Allison Horst puedes encontrar otros ejemplos de gráficos con los datos de los pingüinos.\n\n\n\n1.7.17 Gráficos interactivos\nMencionaremos algunas librerías que podéis utilizar tanto en combinación con ggplot2 o por sí solas para crear visualizaciones interactivas en R.\n\n1.7.17.1 Combinando ggplot2y shiny\nshiny es un paquete de RStudio que hace fácil construir aplicaciones web interactivas con R. Para una introducción y ejemplos en vivo, visita la página web de Shiny.\nPara ver el uso potencial, puedes consultar los ejemplos de Hello Shiny, ejecuta las siguientes instrucciones en la consola de tu RStudio.\n\nlibrary(shiny)\nrunExample(\"01_hello\")\nrunExample(\"04_mpg\")\n\n\n\n1.7.17.2 Plotly\nPlot.ly es una herramienta para crear gráficos interactivos en línea y aplicaciones web.\nEl paquete plotly permite crear los gráficos interactivos a partir de tus gráficos ggplot desde R.\nPara ilustrar las herramientas de esta librería utilizaremos datos de los precios de Google desde enero de 2018 hasta el 31 de diciembre de 2019 que están disponibles en la librería tidyquant.\n\nlibrary(tidyquant)\nlibrary(plotly)\ngetSymbols(\"GOOG\",\n             from = \"2018-01-01\",\n             to = \"2019-12-31\")\n\n[1] \"GOOG\"\n\nstock &lt;- data.frame(GOOG$GOOG.Adjusted)\nstock$GOOG.Adjusted &lt;- stock$GOOG.Adjusted/stock$GOOG.Adjusted[1]\nstock &lt;- data.frame(stock,rownames(stock))\ncolnames(stock) &lt;- append('GOOG','date')\n\nfig &lt;- plot_ly(stock, type = 'scatter', mode = 'lines')%&gt;%\n  add_trace(x = ~date, y = ~GOOG, name = 'GOOG')%&gt;%\n  layout(showlegend = F)\noptions(warn = -1)\n\nfig &lt;- fig %&gt;%\n  layout(\n         xaxis = list(zerolinecolor = '#ffff',\n                      zerolinewidth = 2,\n                      gridcolor = 'ffff'),\n         yaxis = list(zerolinecolor = '#ffff',\n                      zerolinewidth = 2,\n                      gridcolor = 'ffff'),\n         plot_bgcolor='#e5ecf6', width = 900)\n\n\nfig\n\n\n\n\n\n\n\n1.7.17.3 ggiraph\nggiraph es un paquete de R que permite crear gráficos dinámicos ggplot2.\nPermite añadir tooltips (globo /herramienta de ayuda visual), animaciones y acciones JavaScript a los gráficos.\n\n\n\n1.7.18 Recursos adicionales\n\nggplot2: Elegant Graphics for Data Analysis por Hadley Wickham, disponible en acceso abierto.\nFundamentals of Data Visualization por Claus O. Wilke sobre la visualización de datos en general pero utilizando ggplot2. Puedes encontrar los códigos en su perfil de GitHub.\nCookbook for R por Winston Chang con recetas para producir gráficos en R.\nGalería 50 mejores visualizaciones de ggplot2.\nA ggplot2 Tutorial for Beautiful Plotting in R"
  },
  {
    "objectID": "intro.html#práctica-2",
    "href": "intro.html#práctica-2",
    "title": "1  Introducción al Análisis de Datos",
    "section": "1.8 Práctica 2",
    "text": "1.8 Práctica 2\nLa base de datos mpg (ya precargada en tidyverse) contiene datos de vehículos y sus rendimientos tanto en ciudad como en carretera. La descripción de las variables es la siguiente:\n\nmanufacturer: nombre del fabricante.\nmodel: nombre del modelo.\ndispl: desplazamiento del motor, en litros.\nyear: año de fabricación.\ncyl: número de cilindros.\ntrans: tipo de transmisión (manual o automática)\ndrv: tipo de tracción, con f = tracción delantera, r = tracción trasera, 4 = tracción en cuatro ruedas.\ncty: rendimiento en ciudad, en millas por galón.\nhwy: rendimiento en carretera, en millas por galón.\nfl: tipo de combustible, con e = etanol, d = diesel, r = regular, p = premium, c = CNG (gas natural).\nclass: tipo de vehículo.\n\n\nDibujad un gráfico que permita visualizar el número de vehículos de cada fabricante del conjunto de datos.\nDibujad un gráfico para mostrar el rendimiento medio en ciudad para cada clase.\nConstruid un gráfico para mostrar el rendimiento medio en ciudad para cada clase y tipo de tracción simultáneamente.\nUtilizad un gráfico para mostrar la relación entre el tamaño del motor y el rendimiento en carretera, para cada clase de vehículo. ¿Qué podéis observar?\nComparad la distribución del rendimiento en ciudad para distintos tipos de tracción."
  },
  {
    "objectID": "EM.html#poblaciones-vectores-aleatorios",
    "href": "EM.html#poblaciones-vectores-aleatorios",
    "title": "2  Análisis Multivariante",
    "section": "2.1 Poblaciones: vectores aleatorios",
    "text": "2.1 Poblaciones: vectores aleatorios\nUn vector aleatorio de dimensión \\(p\\) es un vector formado por \\(p\\) variables aleatorias \\[\n\\underline{X}=(X_1,X_2,\\ldots,X_p).\n\\] Una realización de \\(\\underline{X}\\) es un vector \\((x_1,\\ldots,x_p)\\) formado por los valores \\(X_1,\\ldots,X_p\\) sobre un individuo.\nUna muestra de \\(\\underline{X}\\) es un conjunto de realizaciones.\nUsualmente, organizamos una muestra de \\(\\underline{X}\\) por medio de una tabla de datos con las columnas definidas por las variables \\(X_1,\\ldots,X_p\\) y donde cada fila es una realización de estas variables, es decir, un vector formado por los valores de \\(X_1,\\ldots,X_p\\) sobre un individuo de la muestra.\nEjemplo: Sea \\(\\textit{BMI}\\) la variable aleatoria que registra el índice de masa corporal (BMI) de una persona, \\(C\\) la variable aleatoria que registra el nivel de colesterol en mg/dl de una persona y \\(E\\) la variable aleatoria que da la edad de una persona en años, entonces \\[\n\\underline{X}=(\\textit{BMI},C,E)\n\\] es un vector aleatorio de dimensión 3. Cada vez que tomamos una persona y anotamos el BMI, el nivel de colesterol y la edad y organizamos estas medidas en este orden en un vector, obtenemos una realitzación de este vector aleatorio \\(\\underline{X}\\). Entonces,una muestra de \\(\\underline{X}\\) será un conjunto de vectores con el BMI, el nivel de colesterol y la edad de un grupo de personas de la población. Por ejemplo\n\n\n\n\n\nBMI\nC\nE\n\n\n\n\n18.3\n170\n49\n\n\n24.4\n202\n39\n\n\n24.6\n215\n50\n\n\n24.4\n218\n44\n\n\n22.2\n210\n40\n\n\n19.5\n210\n36\n\n\n\n\n\n\n\nSean \\(\\underline{X}=(X_1,X_2,\\ldots,X_p)\\) un vector aleatorio y, \\(\\mu_i\\) y \\(\\sigma_i\\) la media y la desviación típica, respectivamente, de cada \\(X_i\\).\n\nEl valor esperado, o vector de medias, de \\(\\underline{X}\\) es el vector formado por los valores esperados, o medias de sus componentes: \\[\nE(\\underline{X})=(\\mu_1,\\ldots,\\mu_p).\n\\] Para abreviar, a veces indicaremos este vector simplemente con \\(\\boldsymbol\\mu\\).\nEl vector de varianzas de \\(\\underline{X}\\) es el vector formado por las varianzas de sus componentes: \\[\n\\sigma^2(\\underline{X})=(\\sigma_1^2,\\ldots,\\sigma_p^2).\n\\]\nEl vector de desviaciones típicas de \\(\\underline{X}\\) es el vector formado por las desviaciones típicas de sus componentes: \\[\n\\sigma(\\underline{X})=(\\sigma_1,\\ldots,\\sigma_p).\n\\]"
  },
  {
    "objectID": "EM.html#covarianza",
    "href": "EM.html#covarianza",
    "title": "2  Análisis Multivariante",
    "section": "2.2 Covarianza",
    "text": "2.2 Covarianza\nLa covarianza de dos variables \\(X\\) e \\(Y\\) es una medida del comportamiento conjunto de estas dos variables.\nFormalmente, dadas dos variables aleatorias \\(X,Y\\) con medias \\(\\mu_X\\) y \\(\\mu_Y\\), respectivamente, su covarianza es \\[\n\\sigma_{X,Y}=E\\left((X-\\mu_X)\\cdot ( Y-\\mu_Y)\\right).\n\\] Es fácil comprobar que \\[\n\\sigma_{X,Y}=E(X\\cdot Y) -\\mu_X\\cdot \\mu_Y.\n\\]\nEn efecto, \\[\n\\begin{array}{rl}\n\\sigma_{X,Y} & =E((X-\\mu_X) ( Y-\\mu_Y))=\nE(XY-\\mu_XY-\\mu_YX+\\mu_X\\mu_Y)\\\\ &\n=E(XY)-\\mu_XE(Y)-\\mu_YE(X)+\\mu_X\\mu_Y\\\\ &=\nE(XY)-\\mu_X\\mu_Y-\\mu_Y\\mu_X+\\mu_X\\mu_Y\n=E(XY)-\\mu_X\\mu_Y\n\\end{array}\n\\]\nLa covarianza de \\(X\\) e \\(Y\\) puede tomar cualquier valor real (no como la varianza, que siempre es positiva), y mide el grado de variación conjunta de las variables en el sentido sigüiente:\n\n\\(\\sigma_{X,Y}&gt;0\\) significa que cuando \\(X\\) es más grande en un individuo 1 que en un individuo 2, \\(Y\\) tiende también a ser más gran en el individuo 1 que en el individuo 2.\n\n\n\n\n\n\n\n\\(\\sigma_{X,Y}&lt;0\\) significa que cuando \\(X\\) es más grande en un individuo 1 que en un individuo 2, \\(Y\\) tiende a ser más pequeña en el individuo 1 que en el individuo 2.\n\n\n\n\n\n\n\n\\(\\sigma_{X,Y}=0\\) significa que no hay ninguna tendencia en este sentido.\n\n\n\n\n\n\nEl signo de la covarianza refleja la “tendencia del crecimiento conjunto” de las variables:\n\nCovarianza positiva significa la misma tendencia: Si \\(X\\) aumenta, \\(Y\\) tiende a aumentar. Esto suele expresarse diciendo que hay asociación positiva entre \\(X\\) e \\(Y\\).\nCovarianza negativa significa la tendencia inversa: Si \\(X\\) aumenta, \\(Y\\) tiende a disminuir. Esto suele expresarse diciendo que hay asociación negativa entre \\(X\\) e \\(Y\\).\n\nSi \\(X\\) e \\(Y\\) son variables independientes, su covarianza es 0, porque en este caso \\(E(X\\cdot Y) =\\mu_X\\mu_Y\\) y por tanto \\[\n\\sigma_{X,Y}=E(X\\cdot Y) -\\mu_X\\cdot \\mu_Y=\\mu_X\\cdot \\mu_Y-\\mu_X\\cdot \\mu_Y=0.\n\\] Intuitivamente, si \\(X\\) e \\(Y\\) son independientes, significa que el hecho de que el valor de \\(X\\) aumente de un individuo a otro no tiene ningún efecto sobre el valor de \\(Y\\).\n¿Por qué, si \\(X\\) e \\(Y\\) son independientes, \\(E(X\\cdot Y) =\\mu_X\\mu_Y\\)?\nOs lo demostraremos en el caso discreto; el argumento en el caso continuo es lo mismo cambiando sumatorios por integrales. \\[\n\\begin{array}{rl}\nE(X\\cdot Y)\\!\\!\\! &\\displaystyle =\\sum_{x\\in D_X,y\\in D_Y} xyP(X=x,Y=y)\\\\\n&\\displaystyle =\\sum_{x\\in D_X,y\\in D_Y} xyP(X=x)P(Y=y)\\\\\n&\\text{(por la independencia de $X$ e $Y$)}\\\\\n&\\displaystyle =\\Big(\\sum_{x\\in D_X}xP(X=x)\\Big)\\Big(\\sum_{y\\in D_Y} yP(Y=y)\\Big)=E(X)E(Y)\n\\end{array}\n\\]\nEs importante remarcar que la igualdad \\(E(X\\cdot Y) =\\mu_X\\mu_Y\\) es equivalente a la igualdad \\(\\sigma(X+Y)^2 =\\sigma(X)^2+\\sigma(Y)^2\\) que decíamos que satisfacen las variables independientes. En efecto \\[\n\\begin{array}{l}\n\\sigma(X+Y)^2 -(\\sigma(X)^2+\\sigma(Y)^2)\\\\\n\\quad = E((X+Y)^2)-E(X+Y)^2-(E(X^2)-E(X)^2+E(Y^2)-E(Y)^2)\\\\\n\\quad = E(X^2+2XY+Y^2)-(E(X)+E(Y))^2\\\\\n\\qquad\\qquad -E(X^2)+E(X)^2-E(Y^2)+E(Y)^2\\\\\n\\quad = E(X^2)+2E(XY)+E(Y^2)-E(X)^2-2E(X)E(Y)-E(Y)^2\\\\\n\\qquad\\qquad -E(X^2)+E(X)^2-E(Y^2)+E(Y)^2\\\\\n\\quad = 2E(XY)-2E(X)E(Y)=2(E(XY)-\\mu_X\\mu_Y)\n\\end{array}\n\\] y por tanto \\[\n\\sigma^2(X+Y) -(\\sigma^2(X)+\\sigma^2(Y))=0 \\Longleftrightarrow E(XY)-\\mu_X\\mu_Y=0\n\\]\nSi \\(X\\) e \\(Y\\) son variables independendientes, su covarianza es 0, pero la implicación al contrario es falsa: Dos variables aleatorias pueden tener covarianza 0 y no ser independientes.\nVeamos un ejemplo de este hecho:\nSupongamos que tenemos un dado tetraédrico no trucado con las caras marcadas con los valores -2, -1, 1 y 2. Sean \\(X\\) la variable aleatoria que consiste en llanzar el dado y anotar el resultado (la cara que queda en tierra), e \\(Y\\) la variable aleatoria que consite en lanzar el dado y anotar el cuadrado del resultado obtenido. Como las cuatro caras del dado son equiprobables, \\[\n\\begin{array}{l}\n\\displaystyle P(X=-2)=P(X=-1)=P(X=1)=P(X=2)=\\frac{1}{4}\\\\\n\\displaystyle P(Y=1)=P(Y=4)=\\frac{1}{2}\n\\end{array}\n\\]\nComp \\(Y\\) es función de \\(X\\), ya que \\(Y=X^2\\), \\(X\\) e \\(Y\\) no pueden ser independientes. Veamos que, en efecto, no lo son. Observad que los únicos posibles valores para el vector \\((X,Y)\\) en una tirada del dado son (-2,4), (-1,1), (1,1) y (2,4), cada uno con probabilidad 1/4. Entonces, por ejemplo, la probabilidad de obtener en una tirada \\(X=-1\\) i \\(Y=4\\) es 0, porque es imposible, mientras que \\[\nP(X=-1)\\cdot P(Y=4)=\\frac{1}{4}\\cdot\\frac{1}{2}=\\frac{1}{8}\\neq 0.\n\\]\nVeamos ahora que la covarianza de \\(X\\) e \\(Y\\) es 0. Para calcularla, primero necesitamos calcular los valores esperados de las variables: \\[\n\\begin{array}{l}\n\\displaystyle \\mu_X=(-2)\\cdot \\frac{1}{4}+(-1)\\cdot \\frac{1}{4}+1\\cdot \\frac{1}{4}+2\\cdot \\frac{1}{4}=0\\\\\n\\displaystyle \\mu_Y=1\\cdot \\frac{1}{2}+4\\cdot \\frac{1}{2}=2.5\n\\end{array}\n\\] Por tanto \\[\n\\begin{array}{l}\n\\sigma_{X,Y}=E\\big(X\\cdot Y\\big)-\\mu_X\\cdot \\mu_Y=E\\big(X\\cdot Y\\big)-0\\cdot 2.5=E\\big(X\\cdot Y\\big)\\\\\n\\qquad =P\\big(X=-2,Y=4\\big)\\cdot (-2\\cdot 4)+P\\big(X=-1,Y=1\\big)\\cdot (-1\\cdot 1)\\\\\n\\qquad\\qquad\\qquad +P\\big(X=1,Y=1\\big)\\cdot (1\\cdot 1)+P\\big(X=2,Y=4\\big)\\cdot (2\\cdot 4)\\\\\n\\qquad =\\displaystyle \\frac{1}{4}\\cdot (-8)+\\frac{1}{4}\\cdot (-1)+\\frac{1}{4}\\cdot 1+\\frac{1}{4}\\cdot 8=0.\n\\end{array}\n\\] Así pues, \\(X\\) e \\(Y\\) son variables dependientes, pero su covarianza es 0.\nDos propiedades importantes más de la covarianza:\n\nLa covarianza es simétrica: \\[\n\\begin{array}{rl}\n\\sigma_{X,Y}\\!\\!\\! & =E((X-\\mu_X)\\cdot ( Y-\\mu_Y))\\\\\n& =E(( Y-\\mu_Y)\\cdot (X-\\mu_X))=\\sigma_{Y,X}\n\\end{array}\n\\]\nLa covarianza de una variable aleatoria con ella misma es su varianza: \\[\n\\sigma_{X,X}=E((X-\\mu_X)^2)=\\sigma^2(X)\n\\]\n\nLa matriz de covarianzas de un vector aleatorio \\(\\underline{X}=(X_1,\\ldots,X_p)\\) es la matriz formada por las covarianzas de los pares de variables que la formen: \\[\n\\sigma_{\\underline{X},\\underline{X}}=\\begin{pmatrix} \\sigma_{X_1,X_1} & \\sigma_{X_1,X_2} & \\ldots & \\sigma_{X_1,X_p}\\\\\n\\sigma_{X_2,X_1} & \\sigma_{X_2,X_2} & \\ldots & \\sigma_{X_2,X_p}\\\\\n\\vdots & \\vdots &\\ddots  & \\vdots\\\\\n\\sigma_{X_p,X_1} & \\sigma_{X_p,X_2} & \\ldots & \\sigma_{X_p,X_p}\\\\\n\\end{pmatrix}\n\\]\nEsta matriz es simétrica y las entradas de la diagonal son las varianzas de las variables del vector, porque \\(\\sigma_{X_i,X_i}=\\sigma^2_{X_i}\\)."
  },
  {
    "objectID": "EM.html#correlación",
    "href": "EM.html#correlación",
    "title": "2  Análisis Multivariante",
    "section": "2.3 Correlación",
    "text": "2.3 Correlación\nComo hemos dicho, el signo de la covarianza tiene una interpretación sencilla, puesto que refleja la tendencia del crecimiento conjunto de las variables. Pero, su magnitud no tiene una interpretación sencilla.\nComo alternativa, se puede medir la tendencia de que haya una relación lineal entre dos variables aleatorias continuas empleando el llamado coeficiente de correlación lineal de Pearson (o, para abreviar, la correlación), que viene a ser una versión normalizada de la covarianza.\nEn concreto, la correlación de las variables \\(X\\) e \\(Y\\) se define como el cociente de su covarianza entre el producto de sus desviaciones típicas: \\[\n\\rho_{X,Y}=\\frac{\\sigma_{X,Y}}{\\sigma_{X} \\sigma_{Y}}\n\\]\nLa correlación tiene las propiedades importantes siguientes:\n\nNo tiene unidades (porque las unidades de \\(\\sigma_X\\) son las de \\(X\\), las unidades de \\(\\sigma_Y\\) son las de \\(Y\\), y las unidades de \\(\\sigma_{X,Y}\\) son las de \\(X\\) por las de \\(Y\\))\nToma valores entre -1 y 1: \\(-1\\leqslant \\rho_{X,Y}\\leqslant 1\\)\nEs simétrica, \\(\\rho_{X,Y}= \\rho_{Y,X}\\)\nLa correlación de una variable con ella misma es 1: \\(\\rho_{X,X}=1\\)\n\\(\\rho_{X,Y}=\\pm 1\\) si, y solo si, las variables \\(X,Y\\) tienen una relación lineal perfecta. Es decir, \\(\\rho_{X,Y}=\\pm 1\\) si, y solo si, existen \\(a,b\\in \\mathbb{R}\\) con \\(a\\neq 0\\) y tales que \\(Y= X+b\\). La pendiente \\(a\\) de esta recta tiene el mismo signo que \\(\\rho_{X,Y}\\).\nCuanto más se acerca \\(|\\rho_{X,Y}|\\) a 1, más se acerca \\(Y\\) a ser función lineal de \\(X\\).\n\n\nSi \\(\\rho_{X,Y}&gt;0\\), la función es creciente\nSi \\(\\rho_{X,Y}&lt;0\\), la función es decreciente\n\n\nSi \\(\\rho_{X,Y}=0\\), decimos que las variables \\(X\\) e \\(Y\\) son incorreladas. Notamos que la correlación es 0 si, y solo si, la covarianza es 0. Por lo tanto, si \\(X\\) y \\(Y\\) son independientes, también son incorreladas. El recíproco en general es falso.\n\nSi una de las dos variables tiene desviación típica 0, en la fórmula de la correlación aparece un 0 en el denominador y no la podemos calcular. En este caso, se toma \\(\\rho_{X,Y}=0\\). El motivo intuitivo es que una variable constante es siempre independiente de cualquier otra variable (haga lo que haga la otra, siempre toma el mismo valor) y hemos quedado que las variables independientes son incorreladas.\nLa matriz de correlaciones de un vector aleatorio \\(\\underline{X}=(X_1,\\ldots,X_p)\\) es la matriz formada por las correlaciones de pares de sus variables: \\[\n\\rho(\\underline{X})\n=\\begin{pmatrix} 1 & \\rho_{X_1,X_2} & \\ldots & \\rho_{X_1,X_p}\\\\\n\\rho_{X_2,X_1} & 1 & \\ldots & \\rho_{X_2,X_p}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n\\rho_{X_p,X_1} & \\rho_{X_p,X_2} & \\ldots & 1\\\\\n\\end{pmatrix}.\n\\] Esta matriz es simétrica por la simetría de la correlación.\nLa correlación de Pearson de dos variables continuas mide la tendencia de las variables a variar conjuntamente de manera lineal. En particular, por ejemplo, si \\(\\rho_{X,Y}&gt;0\\), \\(Y\\) tiende a crecer cuando \\(X\\) crece. Pero esto no significa que un aumento del valor de \\(X\\) cause que el valor de \\(Y\\) tienda a aumentar:\n\nCorrelación no implica causalidad!\n\nLa tendencia al crecimiento simultáneo de \\(X\\) y \\(Y\\) se puede deber a una tercera variable que las haga crecer las dos, o puede ser puramente espuria.\nSi entráis a la página web Spurious Correlations podréis explorar un montón de correlaciones espurias. Nuestra preferida es una correlación de 0.947 entre la variable \\(X\\)= “Tomo un año y anoto el consumo per capita* de queso en los EE. UU.” e \\(Y\\)= “Tomo un año y anoto el número de muertes por estrangulamiento accidental con las sábanas de la cama en los EE. UU.”.\n\n\n\n\n\n\n\n\n\nHay un ejemplo de correlación negativa que es importante tener presente para no dejarse engañar.\nTeorema: Si \\(X_1\\), \\(X_2\\) son dos copias independientes de una misma variable aleatoria \\(X\\), \\[\n\\rho_{X_1,X_2-X_1}=-\\frac{1}{\\sqrt{2}}\\approx -0.71\n\\]\nEsto nos dice que, sea cual sea la variable \\(X\\), si la medimos en dos momentos independientes o sobre dos individuos elegidos de manera independiente, la diferencia entre los dos valores tiene una tendencia destacada a decrecer linealmente en el primer valor. Por ejemplo:\n\nHacéis un test y sacáis una nota muy baja (\\(X_1\\)). El día siguiente hacéis otro test similar (\\(X_2\\)) sin haber estudiado más. Lo más probable es que, por puro azar, saques una nota más alta (que \\(X_2-X_1\\) sea grande, por lo tanto positivo).\nHacéis un test y sacáis una nota muy alta (\\(X_1\\)). El día siguiente hacéis otro test (\\(X_2\\)) sin haber estudiado más. Lo más probable es que, por puro azar, saques una nota más baja (que \\(X_2-X_1\\) sea pequeño, por lo tanto negativo).\n\nPor si alguno necesita una demostración del teorema anterior, recordamos que \\[\n\\rho_{X_1,X_2-X_1}=\\dfrac{\\sigma_{X_1,X_2-X_1}}{\\sigma_{X_1}\\sigma_{X_2-X_1}}\n\\] Ahora \\[\n\\begin{array}{l}\n\\sigma_{X_2-X_1}=\\sqrt{\\sigma^2_{X_2-X_1}}\\\\[2ex]\n\\quad =\\sqrt{\\sigma^2_{X_1}+\\sigma^2_{X_2}}\\ \\text{(porque son independientes)}\\\\[2ex]\n\\quad =\\sqrt{\\sigma^2_{X}+\\sigma^2_{X}}\\ \\text{(porque $X_1,X_2$ son copias de $X$)}\\\\[2ex]\n\\quad =\\sqrt{2\\sigma^2_{X}}=\\sigma_{X}\\sqrt{2}\n\\end{array}\n\\] Luego, \\[\n\\begin{array}{l}\n\\sigma_{X_1,X_2-X_1}=E(X_1(X_2-X_1))-E(X_1)E(X_2-X_1)\\\\[1ex]\n\\quad =E(X_1X_2-X_1^2)-E(X_1)(E(X_2-E(X_1))\\\\[1ex]\n\\quad =E(X_1X_2)-E(X_1^2)-E(X_1)E(X_2)+E(X_1)E(X_1)\\\\[1ex]\n\\quad =E(X_1)E(X_2)-E(X_1^2)-E(X_1)E(X_2)+E(X_1)E(X_1)\\\\[1ex]\n\\quad \\text{(porque  $X_1,X_2$ son independents)}\\\\[1ex]\n\\quad =-E(X_1^2)+E(X_1)E(X_1)=-\\sigma^2_{X_1}=-\\sigma^2_{X}\n\\end{array}\n\\]\nCombinando lo anterior: \\[\n\\rho_{X_1,X_2-X_1}=\\dfrac{\\sigma_{X_1,X_2-X_1}}{\\sigma_{X_1}\\sigma_{X_2-X_1}}=\\dfrac{-\\sigma^2_{X}}{\\sigma_{X}\\cdot \\sigma_{X}\\sqrt{2}} =-\\frac{1}{\\sqrt{2}}\n\\]\nComo ejemplo, generaremos una muestra \\(X\\) de 101 “notas” aleatorias entre 0 y 100 con distribución binomial \\(B(100,0.5)\\). Tomaremos como \\(X_1\\) el vector de las primeras 100 notas, \\[\nX_1=(x_1,x_2,\\ldots,x_{100})\n\\] y como \\(X_2-X_1\\) el vector de las diferencias de cada nota \\(x_y\\), \\(y\\geqslant 2\\), con la anterior: \\[\nX_2-X_1=(x_2-x_1,x_3-x_2,\\ldots,x_{101}-x_{100}).\n\\] Calcularemos la correlación entre \\(X_1\\) y \\(X_2-X_1\\), y lo ilustraremos con un gráfico.\n\nX=rbinom(101,100,0.5)\nX1=X[-101]\nX2.menos.X1=diff(X)\nplot(X1,X2.menos.X1,pch=20,xlab=expression(X[1]),\n     ylab=expression(X[2]-X[1]))\n\n\n\ncor(X1,X2.menos.X1)\n\n[1] -0.7068097\n\n\nLa correlación predicha por el teorema anterior es\n\n-1/sqrt(2) \n\n[1] -0.7071068"
  },
  {
    "objectID": "EM.html#estadística-descriptiva-muestras",
    "href": "EM.html#estadística-descriptiva-muestras",
    "title": "2  Análisis Multivariante",
    "section": "2.4 Estadística descriptiva: Muestras",
    "text": "2.4 Estadística descriptiva: Muestras\n\n2.4.1 Covarianza\nSean \\(X=(x_1,\\ldots,x_n)\\) y \\(Y=(y_1,\\ldots,y_n)\\) dos vectores obtenidos de mediciones de dos variables aleatorias cuantitativas sobre una misma muestra ordenada de individuos de tamño \\(n\\) de una población. Sean \\(\\overline{X}\\) y \\(\\overline{Y}\\) sus medias muestrales. Entonces su covarianza muestral es \\[\n\\widetilde{S}_{X,Y} =\\frac{1}{n-1} \\sum_{i =1}^n\\big((x_{i}-\\overline{{X}})(y_i-\\overline{Y})\\big)\n\\] y su covarianza (a secas) es \\[\n{S}_{X,Y} =\\frac{1}{n} \\sum_{i =1}^n\\big((x_{i}-\\overline{{X}})(y_i-\\overline{Y})\\big)=\\frac{n-1}{n}\\widetilde{S}_{X,Y}.\n\\] Es decir, la diferencia entre la versión “muestral” y la versión “a secas” recae en el denominador, \\(n-1\\) y \\(n\\) respectivamente.\n\nLa covarianza de dos vectores solo tiene sentido cuando estos vectores representan los valores de dos variables cuantitativas sobre los mismos individuos, o sobre dos muestras emparejadas, y en el mismo orden. En particular, los dos vectores tienen que tener la misma longitud.\n\nComo en el caso poblacional, la covarianza entre dos vectores mide la tendencia que tienen los datos a variar conjuntamente:\n\nCuando \\(\\widetilde{S}_{X,Y}&gt;0\\), si \\(x_i&gt;x_j\\) entonces \\(y_i\\) tiende a ser más grande que \\(y_j\\)\nCuando \\(\\widetilde{S}_{X,Y}&lt;0\\), si \\(x_i&gt;x_j\\) entonces \\(y_i\\) tiende a ser más pequeño que \\(y_j\\)\nCuando \\(\\widetilde{S}=0\\), no hay ninguna tendencia en este sentido\n\nEs fácil comprobar que:\n\nLas dos covarianzas son simétricas \\[\n\\widetilde{S}_{X,Y}=\\widetilde{S}_{Y,X},\\ {S}_{X,Y}={S}_{Y,X}\n\\]\nLa varianza de un vector es su covarianza con él mismo \\[\n\\widetilde{S}_{X,X}=\\widetilde{S}^2_{X},\\ {S}_{X,X}={S}^2_{X}.\n\\]\n\nEjemplo\nHemos medido el índice de masa corporal, BMI, y el nivel de colesterol en 5 individuos sanos. Guardamos los resultados en un dataframe y calculamos las medias:\n\nBMI= c(18.3,24.4,24.6,24.4,22.2,19.5)\nChol=c(170,202,215,218,210,210)\nDF=data.frame(BMI,Chol)\nmean(BMI)\n\n[1] 22.23333\n\nmean(Chol)\n\n[1] 204.1667\n\n\nEntonces la covarianza muestral de estos dos vectores es \\[\n\\begin{array}{l}\n\\dfrac{1}{5}\\Big((18.3-22.23)(170-204.17)+(24.4-22.23)(202-204.17)\\\\\n\\qquad +(24.6-22.23)(215-204.17)+(24.4-22.23)(218-204.17)\\\\\n\\qquad +(22.2-22.23)(210-204.17)+(19.5-22.23)(210-204.17)\\Big)=33.8333\n\\end{array}\n\\]\nEntonces la covarianza muestral de estos dos vectores es \\[\n\\begin{array}{l}\n\\dfrac{1}{6}\\Big((18.3-22.23)(170-204.17)+(24.4-22.23)(202-204.17)\\\\\n\\qquad +(24.6-22.23)(215-204.17) +(24.4-22.23)(218-204.17)\\\\\n\\qquad +(22.2-22.23)(210-204.17)+(19.5-22.23)(210-204.17)\\Big)=28.1944\n\\end{array}\n\\]\nLa covarianza muestral de dos vectores numéricos de la misma longitud \\(n\\) se calcula en R con la función cov.\n\ncov(BMI,Chol)\n\n[1] 33.83333\n\n\nPara obtener su covarianza a secas, hay que multiplicar el resultado de cov por \\((n-1)/n\\).\n\nn=length(BMI)\ncov(BMI,Chol)*(n-1)/n\n\n[1] 28.19444\n\n\nConsideramos una tabla de datos cuantitativos de la forma \\[\n\\begin{array}{cccc}\nX_1 & X_2 & \\ldots & X_p\\\\ \\hline\nx_{1 1} & x_{1 2} &\\ldots & x_{1 p}\\\\\nx_{2 1} & x_{2 2} &\\ldots & x_{2 p}\\\\\n\\vdots & \\vdots   &   \\ddots    &\\vdots\\\\\nx_{n 1} & x_{n 2} &\\ldots & x_{n p}\n\\end{array}\n\\] donde cada columna representa los valores de cierta variable \\(X_i\\) y cada fila un individuo de una muestra de la población, de forma que la entrada \\(x_{ij}\\) de esta tabla es el valor de \\(X_j\\) sobre el individuo \\(i\\)-ésimo de la muestra.\nLa matriz de covarianzas muestrales de esta tabla es la matriz \\[\n\\widetilde{{S}}=\n\\begin{pmatrix}  \n\\widetilde{S}^2_{X_1} & \\widetilde{S}_{X_1,X_2} & \\ldots & \\widetilde{S}_{X_1,X_p}\\\\\n\\widetilde{S}_{X_2,X_1} &  \\widetilde{S}^2_{X_2} & \\ldots & \\widetilde{S}_{X_2,X_p}\\\\\n  \\vdots & \\vdots  &  \\ddots      & \\vdots\\\\\n\\widetilde{S}_{X_p,X_1} & \\widetilde{S}_{X_p,X_2} & \\ldots &  \\widetilde{S}^2_{X_p}\n\\end{pmatrix}\n\\] y la matriz de covarianzas (a secas) se define de manera similar, pero con las covarianzas a secas: \\[\n{S}=\n\\begin{pmatrix}  \nS^2_{X_1} & S_{X_1,X_2} & \\ldots & S_{X_1,X_p}\\\\\nS_{X_2,X_1} & S^2_{X_2} & \\ldots & S_{X_2,X_p}\\\\\n  \\vdots & \\vdots  &  \\ddots      & \\vdots\\\\\nS_{X_p,X_1} & S_{X_p,X_2} & \\ldots & S^2_{X_p}\n\\end{pmatrix}\n\\] Las dos son simétricas.\nLa matriz de covarianzas muestrales se calcula con la función cov aplicada a la matriz o al data frame de variables numéricas que almacena la tabla de datos. Para calcular la matriz de covarianzas a secas, se multiplica el resultado de cov por \\((n-1)/n\\), donde \\(n\\) es el número de filas de la tabla.\nEjemplo\nAñadiremos a los datos del ejemplo anterior una tercera variable con las edades de los 5 individuos.\n\nDF$Edad=c(49,39,50,44,40,36)\nDF\n\n   BMI Chol Edad\n1 18.3  170   49\n2 24.4  202   39\n3 24.6  215   50\n4 24.4  218   44\n5 22.2  210   40\n6 19.5  210   36\n\n\nLa matriz de covarianzas muestrales de esta tabla es\n\ncov(DF)\n\n           BMI      Chol   Edad\nBMI   7.586667  33.83333   1.14\nChol 33.833333 309.76667 -33.00\nEdad  1.140000 -33.00000  32.00\n\n\nPodréis observar que es simétrica, que la entrada (2,1) coincide con la covarianza de BMI y Chol que hemos calculado antes, y que en la diagonal obtenemos las varianzas muestrales de las variables de la tabla:\n\napply(DF,MARGIN=2,FUN=var)\n\n       BMI       Chol       Edad \n  7.586667 309.766667  32.000000 \n\n\n\n\n2.4.2 Correlación de Pearson\nSean \\(X=(x_1,\\ldots,x_n)\\) e \\(Y=(y_1,\\ldots,y_n)\\) dos vectores obtenidos midiendo dos variables aleatorias continuas sobre una misma muestra de individuos de medida \\(n\\) de una población.\nLa correlación de Pearson de \\(X\\) e \\(Y\\) es su covarianza muestral dividida por el producto de sus desviaciones típicas muestrales: \\[\nR_{X,Y}=\\frac{\\widetilde{S}_{X,Y}}{\\widetilde{S}_X\\cdot \\widetilde{S}_Y}.\n\\]\nLa correlación de Pearson de \\(X\\) e \\(Y\\) también es igual a su covarianza a secas dividida por el producto de sus desviaciones típicas a secas, porque los cambios de denominador se cancelan: \\[\nR_{X,Y}=\\frac{\\widetilde{S}_{X,Y}}{\\widetilde{S}_X\\cdot \\widetilde{S}_Y}=\n\\frac{\\frac{n}{n-1}\\cdot {S}_{X,Y}}{\\sqrt{\\frac{n}{n-1}}\\cdot {S}_X \\cdot\\sqrt{\\frac{n}{n-1}}\\cdot{S}_Y}=\n\\frac{S_{X,Y}}{S_X \\cdot S_Y}=R_{X,Y}.\n\\]\nEjemplo\nVolvemos a la situación del ejemplo anterior. La covarianza muestral y las desviaciones típicas muestrales de los vectores BMI y Chol son\n\ncov(BMI,Chol)\n\n[1] 33.83333\n\nsd(BMI)\n\n[1] 2.75439\n\nsd(Chol)\n\n[1] 17.60019\n\n\ny por tanto su correlación de Pearson es \\[\nR_{BMI,Chol}=\\frac{33.833}{2.754\\cdot 17.6}= 0.698\n\\]\nAlgunas propiedades importantes de la correlación de Pearson:\n\nLa correlación de Pearson es simétrica: \\[\nR_{X,Y}=R_{Y,X}\n\\]\nLa correlación de Pearson toma valores solo entre -1 y 1: \\[\n-1\\leqslant R_{X,Y}\\leqslant 1\n\\]\nLa correlación de Pearson de un vector con él mismo es 1: \\[\nR_{X,X}=1\n\\]\n\\(R_{X,Y}\\) tiene el mismo signo que \\(S_{X,Y}\\), y por tanto este signo tiene el mismo significado que a la covarianza:\nSi \\(R_{X,Y}&gt;0\\) y si \\(x_i&gt;x_j\\), \\(y_i\\) tiende a ser más grande que \\(y_j\\)\nSi \\(R_{X,Y}&lt;0\\) y si \\(x_i&gt;x_j\\), \\(y_i\\) tiende a ser más pequeño que \\(y_j\\)\nSi \\(R=0\\), no hay ninguna tendencia en este sentido\n\\(R_{X,Y}=\\pm 1\\) si, y solo si, todos los puntos \\((x_i,y_i)\\) están sobre una recta \\(y=ax+b\\) con \\(a\\neq 0\\). La pendiente \\(a\\) de esta relación lineal tiene el mismo signo que \\(R_{X,Y}\\). Por lo tanto, la recta es creciente si \\(R_{X,Y}=1\\) y decreciendo si \\(R_{X,Y}=- 1\\).\nEl coeficiente de determinación \\(R^2\\) de la regresión lineal por mínimos cuadrados de \\(Y\\) respecto de \\(X\\) es igual al cuadrado de su correlación de Pearson: \\[\nR^2=R_{X,Y}^2\n\\]\n\n\nPor lo tanto, cuanto más se acerca la correlación de Pearson de \\(X\\) e \\(Y\\) a 1 o a -1, más se acercan los puntos \\((x_i,y_i)\\) a estar sobre una recta. El signo de \\(R_{X,Y}\\) indica si esta recta es creciente (\\(R_{X,Y}&gt;0\\)) o decreciendo (\\(R_{X,Y}&lt;0\\)).\n\nComo en el caso poblacional, cuando uno de los vectores es constante, la correlación es igual a 0.\nCon R, la correlación de Pearson de dos vectores se puede calcular con la función cor. Por ejemplo, la correlación del Pearson de los vectores BMI y Chol se obtiene con\n\ncor(BMI,Chol)\n\n[1] 0.6979141\n\n\nVeamos que su cuadrado es igual al \\(R^2\\) de la regresión lineal de Chol en función de BMI:\n\ncor(BMI,Chol)^2\n\n[1] 0.487084\n\nsummary(lm(Chol~BMI))$r.squared\n\n[1] 0.487084\n\n\nPara hacernos una idea de qué representa este valor de la correlación, veamos el gráfico de los puntos (BMI,Chol) con su recta de regresión lineal:\n\nplot(BMI,Chol,pch=20)\nabline(lm(Chol~BMI),col=\"red\",lwd=1.5)\n\n\n\n\nPodemos observar como Chol tiende a crecer cuando BMI crece, pero los puntos (BMI,Chol) no tienden a estar sobre una recta.\nCuidado Es conveniente acompañar el cálculo de correlación o covarianza de dos vectores \\(x,y\\) con un gráfico de los puntos \\((x_i,y_i)\\), porque conjuntos muy diferentes de puntos pueden dar lugar a la misma correlación.\nUn ejemplo clásico de este hecho son los cuatro conjuntos de datos \\((x_{1,i},y_{1,i})_{i=1,\\ldots,11}\\), \\((x_{2,i},y_{2,i})_{i=1,\\ldots,11}\\), \\((x_{3,i},y_{3,i})_{i=1,\\ldots,11}\\), \\((x_{4,i},y_{4,i})_{i=1,\\ldots,11}\\) que forman el dataframe anscombe de R:\n\nstr(anscombe)\n\n'data.frame':   11 obs. of  8 variables:\n $ x1: num  10 8 13 9 11 14 6 4 12 7 ...\n $ x2: num  10 8 13 9 11 14 6 4 12 7 ...\n $ x3: num  10 8 13 9 11 14 6 4 12 7 ...\n $ x4: num  8 8 8 8 8 8 8 19 8 8 ...\n $ y1: num  8.04 6.95 7.58 8.81 8.33 ...\n $ y2: num  9.14 8.14 8.74 8.77 9.26 8.1 6.13 3.1 9.13 7.26 ...\n $ y3: num  7.46 6.77 12.74 7.11 7.81 ...\n $ y4: num  6.58 5.76 7.71 8.84 8.47 7.04 5.25 12.5 5.56 7.91 ...\n\n\nLas correlaciones de los cuatro pares de vectores son muy parecidas:\n\ncor(anscombe$x1,anscombe$y1)\n\n[1] 0.8164205\n\ncor(anscombe$x2,anscombe$y2)\n\n[1] 0.8162365\n\ncor(anscombe$x3,anscombe$y3)\n\n[1] 0.8162867\n\ncor(anscombe$x4,anscombe$y4)\n\n[1] 0.8165214\n\n\nPero si los dibujamos veréis que los cuatro conjuntos de puntos son muy diferentes:\n\nplot(anscombe$x1,anscombe$y1,pch=20,main=\"Conjunto de datos 1\",cex=1.25)\nabline(lm(y1~x1,data=anscombe),col=\"red\",lwd=1.5)\nplot(anscombe$x2,anscombe$y2,pch=20,main=\"Conjunto de datos 2\",cex=1.25)\nabline(lm(y2~x2,data=anscombe),col=\"red\",lwd=1.5)\nplot(anscombe$x3,anscombe$y3,pch=20,main=\"Conjunto de datos 3\",cex=1.25)\nabline(lm(y3~x3,data=anscombe),col=\"red\",lwd=1.5)\nplot(anscombe$x4,anscombe$y4,pch=20,main=\"Conjunto de datos 4\",cex=1.25)\nabline(lm(y4~x4,data=anscombe),col=\"red\",lwd=1.5)\n\n\n\n\n\n\nEjemplos más espectaculares se pueden obtener con las funciones del paquete datasaurus, que permiten crear conjuntos de puntos de “formas” diferentes con los mismos estadísticos, y en particular la misma correlación. Empleando este paquete, hemos creado dos pares de vectores de datos dino y star, que hemos recogido en la siguiente tabla de datos. Esta tabla de datos tiene tres variables: una variable dataset que indica el conjunto de datos, y las variables x e y que dan las coordenadas de los puntos que forman cada conjunto de datos. Comprobaremos que los dos pares de vectores de datos tienen el mismo coeficiente de correlación (al menos hasta la séptima cifra decimal) y los dibujaremos.\n\ndatasaure=read.table(\"https://raw.githubusercontent.com/AprendeR-UIB/MatesII/master/Dades/Datasaurus.txt\",header=TRUE,sep=\"\\t\")\nstr(datasaure)\n\n'data.frame':   284 obs. of  3 variables:\n $ dataset: chr  \"dino\" \"dino\" \"dino\" \"dino\" ...\n $ x      : num  55.4 51.5 46.2 42.8 40.8 ...\n $ y      : num  97.2 96 94.5 91.4 88.3 ...\n\ndino=datasaure[datasaure$dataset==\"dino\",2:3]\nstar=datasaure[datasaure$dataset==\"star\",2:3]\ncor(dino$x,dino$y)\n\n[1] -0.0629611\n\ncor(star$x,star$y)\n\n[1] -0.0629611\n\n\n\nplot(dino,pch=20,cex=1.25)\nplot(star,pch=20,cex=1.25)\n\n\n\n\n\n\nAhora supongamos que tenemos una tabla de datos numéricos de la forma \\[\n\\begin{array}{cccc}\nX_1 & X_2 & \\ldots & X_p\\\\ \\hline\nx_{1 1} & x_{1 2} &\\ldots & x_{1 p}\\\\\nx_{2 1} & x_{2 2} &\\ldots & x_{2 p}\\\\\n\\vdots & \\vdots   &   \\ddots    &\\vdots\\\\\nx_{n 1} & x_{n 2} &\\ldots & x_{n p}\n\\end{array}\n\\]\ndonde cada columna representa los valores de cierta variable \\(X_i\\) y cada fila un individuo de una muestra de la población, de forma que la entrada \\(x_{ij}\\) de esta tabla es el valor de \\(X_j\\) sobre el individuo \\(i\\)-ésimo de la muestra.\nSu matriz de correlaciones de Pearson es la matriz simétrica \\[\n\\begin{pmatrix}  \n1 & R_{X_1,X_2} & \\ldots & R_{X_1,X_p}\\\\\nR_{X_2,X_1} & 1 & \\ldots & R_{X_2,X_p}\\\\\n  \\vdots & \\vdots  &  \\ddots      & \\vdots\\\\\nR_{X_p,X_1} & R_{X_p,X_2} & \\ldots & 1\n\\end{pmatrix}\n\\]\nEsta matriz de correlaciones se calcula con la función cor aplicada a la matriz o el data frame de variables numéricas que almacena la tabla de datos. Por ejemplo, la matriz de correlaciones de Pearson de la tabla de datos DF del Ejemplo que venimos trabajando es\n\ncor(DF)\n\n            BMI       Chol        Edad\nBMI  1.00000000  0.6979141  0.07316517\nChol 0.69791406  1.0000000 -0.33145274\nEdad 0.07316517 -0.3314527  1.00000000\n\n\n\n\n2.4.3 Estimación\nLas covarianzas de dos vectores obtenidos midiendo dos variables \\(X,Y\\) sobre una muestra aleatoria simple de sujetos de una población estiman la covarianza poblacional de las variables \\(X,Y\\) que han producido los vectores:\n\nLa covarianza muestral \\(\\widetilde{S}_{X,Y}\\) siempre es un estimador no sesgado de la covarianza poblacional \\(\\sigma_{X,Y}\\)\nLa covarianza \\({S}_{X,Y}=\\frac{n-1}{n}\\widetilde{S}_{X,Y}\\) es un estimador sesgado de la covarianza poblacional \\(\\sigma_{X,Y}\\), con sesgo que tiende a 0, y es más eficiente que \\(\\widetilde{S}_{X,Y}\\)\nLa covarianza \\(S_{X,Y}\\) es el estimador máximo verosímil de \\(\\sigma_{X,Y}\\) cuando la distribución conjunta de las variables \\(X,Y\\) es la normal bivariante que estudiaremos próximamente.\n\nLa correlación de Pearson de dos vectores obtenidos midiendo dos variables continuas \\(X,Y\\) sobre una muestra aleatoria simple de sujetos de una población estima la correlación poblacional de las variables \\(X,Y\\). En concreto:\n\n\\(R_{X,Y}\\) es un estimador máximo verosímil de \\(\\rho_{X,Y}\\) cuando la distribución conjunta de \\(X,Y\\) se normal bivariante. se un estimador sesgado, pero su sesgo tiende a 0.\n\n\n\n2.4.4 Correlación de Spearman\nLa correlación de Pearson mide específicamente la tendencia de dos variables continuas a depender linealmente la una de la otra. Si no esperamos que esta dependencia lineal exista, o si nuestras variables son discretas o simplemente ordinales, emplear la correlación de Pearson para analizar la relación entre dos variables no es lo más adecuado. Entre las propuestas alternativas, la más popular es la correlación de Spearman.\nIntuitivamente, la correlación de Spearman mide la tendencia que si \\(x_i&gt;x_j\\), pase que \\(y_i&gt;y_j\\). Su valor es 1 si, para todos \\(y,j\\), \\[\nx_i&gt;x_j\\Longleftrightarrow y_i&gt;y_j\n\\] y su valor es -1 si, para todo \\(i,j\\), \\[\nx_i&gt;x_j\\Longleftrightarrow y_i&lt;y_j\n\\] Cuanto más se acerca la correlación de Spearman a 1 (o -1), para más parejas de índices \\((i,j)\\) tales que \\(x_i&gt;x_j\\) se tiene que \\(y_i&gt;y_j\\) (\\(y_i&lt;y_j\\), si se acerca a -1).\nFormalmente, la correlación de Spearman de dos vectores \\(X\\) e \\(Y\\) se define como la correlación de Pearson de los vectores de rangos de \\(X\\) e \\(Y\\). El vector de rangos de un vector \\(X\\) se obtiene sustituyendo cada valor de \\(X\\) por su posición en el vector ordenado de menor a mayor, y en caso de empates asignando a grupos de valores empatados la media de las posiciones que ocuparían. Por ejemplo, el vector de rangos de\n\\[\nx=(4,5,1,5,1,3,4,4)\n\\] es \\[\n(5,7.5,1.5,7.5,1.5,3,5,5)\n\\] ¿Cómo hemos calculado este vector?\n\nPrimero asignamos a cada valor del vector su posición si estuvieran ordenados de menor a mayor, y en caso de empate por ahora los ordenaremos de izquierda a derecha: \\[\n\\begin{array}{r|cccccccc}\nx & 4& 5 & 1 & 5 & 1 & 3 & 4 & 4\\\\ \\hline\n\\text{Posició} & 4 & 7 & 1 & 8 & 2 & 3 & 5 & 6\\\\\n\\end{array}\n\\]\nAhora, para asignar los rangos finales:\n\n\nEl rango de los dos elementos 1 de \\(x\\) es la media de las posiciones 1, 2 del vector ordenado: 1.5.\nComo que solo hay un 3 en \\(x\\), su rango es su posición en el vector ordenado: 3.\nEl rango de los tres elementos 4 es la media de las posiciones 4, 5 y 6 del vector ordenado: 5.\nFinalmente, el rango de los dos elementos 5 es la media de las posiciones 7, 8 del vector ordenado: 7.5.\n\n\\[\n\\begin{array}{r|cccccccc}\nx & 4& 5 & 1 & 5 & 1 & 3 & 4 & 4\\\\ \\hline\n\\text{Posició} & 4 & 7 & 1 & 8 & 2 & 3 & 5 & 6\\\\ \\hline\n\\text{Rang} & 5 & 7.5 & 1.5 & 7.5 & 1.5 & 3 & 5 & 5\n\\end{array}\n\\]\nPor cierto, con R el vector de rangos se calcula con la función rank:\n\nrank(c(4,5,1,5,1,3,4,4))\n\n[1] 5.0 7.5 1.5 7.5 1.5 3.0 5.0 5.0\n\n\nCon R, la correlación de Spearman se calcula directamente con la función cor especificando el parámetro method=\"spearman\". (El valor por defecto del parámetro method es \"pearson\" y por eso no lo indicamos cuando calculamos la correlación de Pearson.)\nEjemplo Consideremos los vectores BMI y Chol del ejemplo que venimos trabajando. Lo primero que haremos será calcular sus vectores de rangos:\n\\[\n\\begin{array}{|c|c||c|c|}\n\\hline\nBMI & \\text{Rangs} & Chol&  \\text{Rangs}\n\\\\\\hline\\hline\n18.3& 1 & 170& 1 \\\\\n24.4&4.5 & 202 & 2\\\\\n24.6&6 & 215& 5 \\\\\n24.4&4.5 & 218&  6\\\\\n22.2&3 & 210&  3.5\\\\\n19.5&2 & 210&  3.5\\\\\\hline\n\\end{array}\n\\]\nPor tanto, la correlación de Spearman de \\[\n\\mathit{BMI}=(18.3, 24.4, 24.6, 24.4, 22.2, 19.5)\\mbox{ i }\\mathit{Chol}=(170, 202, 215, 218, 210, 210)\n\\] es la correlación de Pearson de \\[\n(1, 4.5, 6, 4.5, 3, 2)\\mbox{ i }(1, 2, 5, 6, 3.5, 3.5)\n\\]\nComprobémoslo:\n\ncor(BMI,Chol,method=\"spearman\")\n\n[1] 0.6470588\n\ncor(c(1,4.5,6,4.5,3,2),c(1,2,5,6,3.5,3.5))\n\n[1] 0.6470588"
  },
  {
    "objectID": "EM.html#contrastes-de-correlación",
    "href": "EM.html#contrastes-de-correlación",
    "title": "2  Análisis Multivariante",
    "section": "2.5 Contrastes de correlación",
    "text": "2.5 Contrastes de correlación\nEn un contraste de correlación de dos variables poblacionales continuas \\(X\\) e \\(Y\\), la hipótesis nula es que no hay correlación entre las dos variables, lo cual traduce que no hay ninguna relación entre ellas. \\[\n\\left\\{\n\\begin{array}{ll}\nH_0: & \\rho_{XY}=0\\\\\nH_1: & \\rho_{XY}&gt; 0\\text{ o }\\rho_{XY}&lt; 0\\text{ o }\\rho_{XY}\\neq 0\n\\end{array}\\right.\n\\]\n\nSi en un contraste de correlación rechazamos la hipótesis nula, en particular concluimos que las variables \\(X\\) e \\(Y\\) son dependientes (porque si fueran independientes, su correlación seria 0).\n\nNo explicaremos cómo se hace a mano este contraste ni qué hipótesis tienen que satisfacer las variables poblacionales para que el resultado sea fiable. Si estáis interesados en el detalle, podéis consultar la correspondiente entrada de la Wikipedia. Simplemente tenéis que saber que se efectúa con la función cor.plot. Su sintaxis es similar a la de las otras funciones que efectúan contrastes.\nEjemplo Queremos contrastar si hay correlación positiva entre el BMI y el nivel de colesterol de un adulto sano, con un nivel de significación del 5%.\nVariables poblacionales de interés:\n\n\\(\\mathit{BMI}\\): “Tomamos un adulto sano y anotamos su BMI”\n\\(\\mathit{Chol}\\): “Tomamos un adulto sano y anotamos el nivel de colesterol en mg/l”\n\nContrast: \\[\n\\left\\{\n\\begin{array}{ll}\nH_0: & \\rho_{\\textit{BMI,Chol}}=0\\\\\nH_1: & \\rho_{\\textit{BMI,Chol}}&gt;0\n\\end{array}\\right.\n\\]\nEmpleamos las muestras de BMI y Chol del Ejemplo que venimos trabajando.\n\ncor.test(BMI,Chol,alternative=\"greater\")\n\n\n    Pearson's product-moment correlation\n\ndata:  BMI and Chol\nt = 1.949, df = 4, p-value = 0.06155\nalternative hypothesis: true correlation is greater than 0\n95 percent confidence interval:\n -0.08621998  1.00000000\nsample estimates:\n      cor \n0.6979141 \n\n\nConclusión: No hemos obtenido evidencia estadísticamente significativa que el BMI y el nivel de colesterol de un adulto sano tengan correlación positiva (test de correlación, p-valor 0.06, IC para \\(\\rho\\) 95% [-0.086 a 1]).\nCuidado La conclusión es que no hemos obtenido evidencia que el BMI y el nivel de colesterol tengan correlación positiva en la población de los adultos sanos. No en nuestra muestra, que sí que ha dado correlación positiva. Recordad que los contrastes siempre se refieren a la población."
  },
  {
    "objectID": "EM.html#gráficos-para-datos-multidimensionales",
    "href": "EM.html#gráficos-para-datos-multidimensionales",
    "title": "2  Análisis Multivariante",
    "section": "2.6 Gráficos para datos multidimensionales",
    "text": "2.6 Gráficos para datos multidimensionales\n\n2.6.1 Otro gráfico para datos bivariantes\nEl boxplot bivariante es un gráfico de dispersión que incluye dos elipses estimadas, la interior que contiene aproximadamente el 50% de los datos y la exterior que contiene aproximadamente el 95% de los datos. Este tipo de gráficos nos ayuda a localizar datos atípicos. Veamos un ejemplo con los datos de los pingüinos.\n\nlibrary(tidyverse)\nlibrary(MVA)\nlibrary(palmerpenguins)\n\na2&lt;- penguins %&gt;%\n  select(body_mass_g,bill_length_mm) %&gt;%\n  na.omit %&gt;% as.matrix()\n\nbvbox(a2,xlab = \"Peso del pingüino en gr\", \n           ylab = \"Longitud del pico en mm\",\n      pch = 19, cex = 1.25, col = \"red\")\n\n\n\n\nLas dos rectas dentro de las elipses del gráfico anterior son estimaciones de la recta de regresión.La recta más oscura es la habitual de mínimos cuadrados utilizando todas las observaciones. La recta más clara es una estimación más robusta que reduce la influencia de cualquier valor extremo.\n\n\n2.6.2 Matriz de dispersión\nEs posible ver los gráficos de dispersión por pares entre diversas variables cuantitativas utilizando una matriz. Se puede generar utilizando la función “pairs()” de R base. Veamos un ejemplo con nuestros datos de pingüinos.\n\na&lt;-penguins %&gt;%\n  select(3:7) %&gt;%\n  na.omit\n\npairs(a,\n      col = c(\"red\", \"blue\")[as.integer(a$sex)], \n      pch = 18)\n\n\n\n\nExisten muchas otras funciones para crear matrices de gráficos de dispersión. Una que nos gusta es ggpairs() del paquete GGally.\n\nlibrary(GGally)\nggpairs(a)\n\n\n\n\nPor defecto, la diagonal principal de la matriz contiene la curva de densidad para cada variable. Por debajo de la diagonal principal se muestran los gráficos de dispersión y la correlación (para las variables cuantitativas) o el cruce entre variables si se trata de una variable categórica.\n\n\n2.6.3 Caras de Chernoff\nCada variable del conjunto de datos se usa para representar una característica de la cara. Chernoff usó hasta 18 variables para representar diferentes rasgos faciales como cabeza, nariz, ojos, cejas, boca y orejas. El gráfico de Chernoff tiene como ventaja la facilidad humana para reconocer patrones de caras. El inconveniente es que la representación es muy dependiente de las variables escogidas para representar cada rasgo. Por ejemplo: la boca y la forma de la cabeza son rasgos más llamativos que las orejas o la longitud de la nariz, por tanto, el mismo conjunto puede sugerir distintos patrones de similitud entre las observaciones. Veamos un ejemplo.\n\nlibrary(\"aplpack\")\nb&lt;- penguins %&gt;% \n      filter(species==\"Adelie\", \n             island==\"Torgersen\",\n             sex==\"female\",\n             year==2007)\n\nfaces(b[,3:6],face.type = 1, scale =TRUE,print.info = TRUE)\n\n\n\n\neffect of variables:\n modified item       Var                \n \"height of face   \" \"bill_length_mm\"   \n \"width of face    \" \"bill_depth_mm\"    \n \"structure of face\" \"flipper_length_mm\"\n \"height of mouth  \" \"body_mass_g\"      \n \"width of mouth   \" \"bill_length_mm\"   \n \"smiling          \" \"bill_depth_mm\"    \n \"height of eyes   \" \"flipper_length_mm\"\n \"width of eyes    \" \"body_mass_g\"      \n \"height of hair   \" \"bill_length_mm\"   \n \"width of hair   \"  \"bill_depth_mm\"    \n \"style of hair   \"  \"flipper_length_mm\"\n \"height of nose  \"  \"body_mass_g\"      \n \"width of nose   \"  \"bill_length_mm\"   \n \"width of ear    \"  \"bill_depth_mm\"    \n \"height of ear   \"  \"flipper_length_mm\"\n\n\nAlternativamente, podemos representar cada elemento por una figura geométrica, donde las similitudes entre figuras indican las similitudes entre los elementos.\n\nstars(a[,1:4], key.loc = c(44, 1.5),cex=0.45,\n      labels=row.names(a[,1:4]), draw.segments=TRUE)\n\n\n\n\nPor supuesto que ggplot2 nos da opciones más modernas de este tipo de gráficos. Ver, por ejemplo:\n\nggradar\nfmsb library\n\n\n\n2.6.4 Matriz de correlaciones\nLas correlaciones por pares entre varias variables se muestran visualmente mediante un mapa de calor de la matriz de correlaciones. La función ggcorrplot() del paquete “ggcorrplot” de R puede utilizarse para construirlo. A continuación se muestra un ejemplo que utiliza el conjunto de datos de los pingüinos:\n\nlibrary(ggcorrplot)\npenguins %&gt;%\n  select(3:6) %&gt;%\n  na.omit(.) %&gt;% \n  cor(.) %&gt;% \n  ggcorrplot(., hc.order = TRUE,\n        type = \"lower\",\n        colors = c(\"#6D9EC1\",\n                    \"yellow\", \"#E46726\"))\n\n\n\n\nEn el gráfico, el color naranja denota correlaciones positivas y el gris correlaciones negativas. Al especificar hc.order,las variables también se ordenan.\n\n\n2.6.5 Gráficos de mosaico\nHasta ahora, hemos explorado métodos para visualizar relaciones entre variables cuantitativas. Pero, ¿qué hacemos si hay más de dos variables categóricas?\nUn método consiste en utilizar gráficos de mosaico, en los que las frecuencias de una tabla de contingencia multidimensional se representan mediante regiones rectangulares anidadas que son proporcionales a su frecuencia de celda. La función mosaicplot() de la librería vcd proporciona características más amplias que la de R base.\nSi se añade la opción opción shade=TRUE colorea la figura basándose en los residuos estandarizados de un modelo loglineal para la tabla. La idea central detrás de un modelo loglineal es modelar las probabilidades de ocurrencia de cada categoría en la tabla de contingencia y el modelo se ajusta utilizando técnicas de regresión logística.\n\nlibrary(vcd)\na&lt;- penguins %&gt;%\n  select(island,species,sex) %&gt;%\n  na.omit()\na2&lt;- table(a)\n\nmosaicplot(a2,shade=TRUE, main=\"\")"
  },
  {
    "objectID": "EM.html#práctica-3",
    "href": "EM.html#práctica-3",
    "title": "2  Análisis Multivariante",
    "section": "2.7 Práctica 3",
    "text": "2.7 Práctica 3\nDe las fuentes de datos que se citan abajo o de alguna otra fuente que sea de libre acceso, seleccionad un conjunto de datos que os guste y pensad en responder una pregunta a partir de estos. El conjunto de datos debe cumplir con las siguientes condiciones:\n\nQue contenga al menos un par de variables cualitativas (una ordinal y otra nominal).\nQue contenga al menos cinco variables cuantitativas, cuantas más mejor.\nSi tenéis que unir varias tablas de datos, usad las herramientas que hemos presentado de tidyverse.\nAlgunas fuentes de datos:\n\nCompendium of Data Sources for Data Science, Machine Learning, and Artificial Intelligence.\ndatos.gob.es\nIBESTAT\nDatasets 2023 Makeover Mondey\n\n\n\nCread un repositorio en Github para vuestro grupo con un nombre que sea fácilmente identificable para los profesores de la asignatura. Cread un proyecto nuevo en RStudio conectado al repositorio anterior que contenga los documentos/scripts y presentación del proyecto.\nEl proyecto debe incluir:\n\nResumen del problema que trataréis, su contexto y la fuente de datos. Describid cada una de las variables que conforman la base de datos del estudio. Plantead una pregunta u objetivo del proyecto.\nTibble o dataframe con la base de datos limpia. Indicad los pasos e instrucciones que habéis utilizado para lograr convertir vuestros datos en tidy data (si aplica).\nUn análisis multivariante exploratorio de los datos con visualizaciones apropiadas y su interpretación en el contexto del problema.\nLa estimación del vector de medias y la matriz de covarianza/correlación de las variables cuantitativas, así como un contraste de correlación.\nEscribid una conclusión con los resultados importantes del análisis.\nPreparad una presentación de máximo 15 minutos en la que cada miembro del grupo exponga una parte del trabajo.\nCada grupo tiene 15 minutos para exponer y 5 para responder las dudas y preguntas de los compañeros y de los profesores de la asignatura. La calificación se realizará por medio de la rúbrica publicada en Aula Digital que podéis encontrar en la sección “Bloque I”.\nEl peso de la evaluación es el siguiente: 0.8 \\(\\cdot\\) Nota de la Exposición + 0.2 \\(\\cdot\\) Nota del repositorio$. Para calcular la nota de la Exposición, se tomará la calificación media de vuestros compañeros (excluidos los integrantes del propio grupo) y la asignada por los profesores. Si la diferencia de notas entre los profesores y los compañeros es mayor a 2 puntos, contará únicamente la nota de los profesores."
  },
  {
    "objectID": "EM.html#distancias",
    "href": "EM.html#distancias",
    "title": "2  Análisis Multivariante",
    "section": "2.8 Distancias",
    "text": "2.8 Distancias\nComo hemos mencionado, la información multivariante es una matriz de datos de orden \\(n \\times p\\), con \\(n\\) el número de filas y \\(p\\) el número de variables consideradas. A veces, para determinar cuáles observaciones multivariantes son semejantes y cuáles no, se emplean matrices de distancias o similaridades. Dependiendo del contexto, hay que seleccionar la más apropiada para obtener resultados confiables. A continuación, presentamos algunas de las distancias más utilizadas.\n\n2.8.1 Distancia euclídea\nLa distancia euclídea entre dos realizaciones \\(\\underline{X}=(x_1,\\ldots,x_p)\\) e \\(\\underline{Y}=(y_1,\\ldots,y_p)\\) se define como\n\\[d_E(\\underline{X},\\underline{Y})=\\sqrt{\\displaystyle\\sum_{k=1}^p (x_k-y_k)^2}\\]\nEsta distancia equivale a la suma de las longitudes de los segmentos que unen cada par de posiciones. La siguiente imagen muestra el perfil de dos usuarios en base a las valoraciones que han hecho de 10 ítems.\n\nlibrary(tidyverse)\nusuario_a &lt;- c(4, 4.5, 4, 7, 7, 6, 5, 5.5, 5, 6)\nusuario_b &lt;- c(4, 4.5, 4, 7, 7, 6, 5, 5.5, 5, 6) + 3\ndatos &lt;- data.frame(usuario = rep(c(\"a\", \"b\"), each = 10),\n                    valoracion = c(usuario_a, usuario_b),\n                    item = 1:10)\nggplot(data = datos, aes(x = as.factor(item), y = valoracion,\n                         colour = usuario)) +\n  geom_path(aes(group = usuario)) +\n  geom_point() +\n  geom_line(aes(group = item), colour = \"firebrick\", linetype = \"dashed\") +\n  labs(x = \"item\") +\n  theme_bw() + theme(legend.position = \"bottom\")\n\n\n\n\nLa distancia euclideana entre los dos usuarios es:\n\nsqrt(sum((usuario_a-usuario_b)^2))\n\n[1] 9.486833\n\n\nLas funciones dist() de R base y get_dist de la librería factoextra se pueden utilizar para calcular las distancias entre todas las filas (observaciones) de una matriz o data frame. Por defecto, las funciones devuelven una matriz triangular inferior.\nEjemplo: Para el data frame de los pingüinos, calculamos las distancias euclideanas entre las ocho hembras de la especie Adelie que fueron registradas en Torgensen el año 2007.\n\np_ATf&lt;- penguins %&gt;% \n  filter(species==\"Adelie\", \n         island==\"Torgersen\",\n         sex==\"female\",\n         year==2007) %&gt;% select(3:5)\n\ndist(p_ATf)\n\n          1         2         3         4         5         6         7\n2  9.055385                                                            \n3  7.774960  4.318565                                                  \n4  5.051732 14.071247 12.291867                                        \n5  4.312772 13.030733 11.968709  2.424871                              \n6  3.093542 10.664427  8.140025  4.614109  5.412024                    \n7  9.176056  1.886796  2.844293 14.052758 13.293607 10.288343          \n8  5.568662 12.488795  9.332738  5.441507  7.037755  2.489980 11.825819\n\nlibrary(factoextra)\nget_dist(p_ATf, stand=TRUE)\n\n         1        2        3        4        5        6        7\n2 1.819868                                                      \n3 3.293836 2.530682                                             \n4 1.077437 2.510238 3.181155                                    \n5 1.039894 2.343332 3.710845 1.048543                           \n6 1.443639 2.415665 2.600833 1.244814 2.113671                  \n7 2.840218 1.637506 1.061075 2.989495 3.228280 2.641200         \n8 2.750331 3.317954 2.286750 2.271394 3.259136 1.338307 2.850519\n\n\nLas distancias grandes indican mayores disimilitudes entre las observaciones.\nObserva que hemos calculado la distancia euclideana con la función get_dist pero estandarizando los datos especificando stand=TRUE, es decir, para cada variable (columna), restamos el valor medio de la columna y dividimos por la desviación típica de la columna, lo cual tiene sentido hacerlo cuando las variables del conjunto de datos están en escalas diferentes. El problema con la distancia euclideana es que no toma en cuenta la correlación entre las variables consideradas.\n\n\n2.8.2 Distancia de Minkowski\nOtra distancia que podría ser considerada es la de Minkowski, cuya definición es la siguiente:\n\\[d_{M_q}(\\underline{X}_i,\\underline{X}_j)=\\left(\\displaystyle\\sum_{k=1}^p|x_{ik}-x_{jk}|^q \\right)^{1/q}, \\;\\; q&gt;0\\] Presenta los mismos inconvenientes que \\(d_E\\) \\((d_E = d_{M_2})\\).\n\nSi \\(q=1\\), se le llama distancia de Manhattan\nSi \\(q \\rightarrow \\infty\\), se le llama distancia dominante \\[d_{M_\\infty}(\\underline{X}_i,\\underline{X}_j)=\\max\\{ |x_{i1}-x_{j1}|,\\ldots,|x_{ip}-y_{jp}|\\}\\]\n\nLa siguiente imagen muestra una comparación entre la distancia euclídea (segmento azul) y la distancia de Manhattan (segmento rojo y verde) en un espacio bidimensional. Existen múltiples caminos para unir dos puntos con el mismo valor de distancia de Manhattan, ya que su valor es igual al desplazamiento total en cada una de las dimensiones.\n\ndatos &lt;- data.frame(observacion = c(\"a\", \"b\"), x = c(2,7), y = c(2,7))\nmanhattan &lt;- data.frame(\n              x = rep(2:6, each = 2),\n              y = rep(2:6, each = 2) + rep(c(0,1), 5),\n              xend = rep(2:6, each = 2) + rep(c(0,1), 5),\n              yend = rep(3:7, each = 2))\n\nmanhattan_2 &lt;- data.frame(\n                x = c(2, 5, 5, 7),\n                y = c(2, 2, 4, 4),\n                xend = c(5, 5, 7, 7),\n                yend = c(2, 4, 4, 7))\n\nggplot(data = datos, aes(x = x, y = y)) +\ngeom_segment(aes(x = 2, y = 2, xend = 7, yend = 7), color = \"blue\", size = 1.2) +\ngeom_segment(data = manhattan, aes(x = x, y = y, xend = xend, yend = yend),\n             color = \"red\", size = 1.2) +\ngeom_segment(data = manhattan_2, aes(x = x, y = y, xend = xend, yend = yend),\n             color = \"green3\", size = 1.2) +\ngeom_point(size = 3) +\ntheme(panel.grid.minor = element_blank(),\n      panel.grid.major = element_line(size = 2),\n      panel.background = element_rect(fill = \"gray\",\n                                      colour = \"white\",\n                                      size = 0.5, linetype = \"solid\"))"
  },
  {
    "objectID": "EM.html#distancias-basadas-en-la-correlación",
    "href": "EM.html#distancias-basadas-en-la-correlación",
    "title": "2  Análisis Multivariante",
    "section": "2.9 Distancias basadas en la correlación",
    "text": "2.9 Distancias basadas en la correlación\nLas distancias basadas en la correlación son útiles utilizadas cuando la similitud queremos establecerla en términos del patrón o forma y no de desplazamiento o magnitud; por ejemplo en el análisis de datos de expresión génica.\n\\[d_{cor}(\\underline{X}_i,\\underline{X}_j)=1-Cor(\\underline{X}_i,\\underline{X}_j)\\]\ndonde la correlación puede ser de distintos tipos (Pearson, Spearman, Kendall…)\nEn la siguiente imagen mostramos el perfil de 3 usuarios. Veamos qué ocurre si utilizamos la distancia euclideana y la de correlación de Pearson para compararlos.\n\nusuario_a &lt;- c(4, 4.5, 4, 7.5, 7, 6, 5, 5.5, 5, 6)\nusuario_b &lt;- c(4, 4.5, 4, 7.5, 7, 6, 5, 5.5, 5, 6) + 4\nusuario_c &lt;- c(5, 5.5, 4.8, 5.4, 4.7, 5.6, 5.3, 5.5, 5.2, 4.8)\n\ndatos &lt;- data.frame(usuario = rep(c(\"a\", \"b\", \"c\"), each = 10),\n                    valoracion = c(usuario_a, usuario_b, usuario_c),\n                    item = 1:10)\n\nggplot(data = datos, aes(x = as.factor(item),\n                         y = valoracion, \n                         colour = usuario)) +\n  geom_path(aes(group = usuario)) +\n  geom_point() +\n  labs(x = \"item\") +\n  theme_bw() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\ndist(x = rbind(usuario_a, usuario_b, usuario_c), method = \"euclidean\")\n\n          usuario_a usuario_b\nusuario_b  12.64911          \nusuario_c   3.75100  13.98821\n\n1 - cor(x = cbind(usuario_a, usuario_b, usuario_c), method = \"pearson\")\n\n          usuario_a usuario_b usuario_c\nusuario_a 0.0000000 0.0000000 0.9466303\nusuario_b 0.0000000 0.0000000 0.9466303\nusuario_c 0.9466303 0.9466303 0.0000000\n\n\nDe acuerdo a la distancia euclideana, los usuarios a y c son los más similares, mientras que acorde a la correlación de Pearson, a y b son los más parecidos. Este ejemplo pone de manifiesto que no existe una única medida de distancia que sea mejor que las demás, sino que, dependiendo del contexto, una será más adecuada que otra."
  },
  {
    "objectID": "EM.html#distancia-de-mahalanobis",
    "href": "EM.html#distancia-de-mahalanobis",
    "title": "2  Análisis Multivariante",
    "section": "2.10 Distancia de Mahalanobis",
    "text": "2.10 Distancia de Mahalanobis\n\\[d^2_M \\left( \\underline{X}_i,\\underline{X}_j \\right)= \\left(\\underline{X}_i-\\underline{X}_j \\right)^t {\\bf S}^{-1}\n\\left(\\underline{X}_i-\\underline{X}_j\\right),\\]\ndonde \\({\\bf S}\\) es la matriz de covarianzas muestrales de la matriz de datos \\({\\bf X}\\).\nEs adecuada como medida de discrepancia entre datos, porque:\n\nEs invariante frente a transformaciones lineales invertibles de las variables.\n\\(d_E=d_M\\) cuando \\({\\bf S}={\\bf I}\\) y \\(d_K = d_M\\) cuando \\({\\bf S}=diag(s^2_1,\\ldots,s^2_p),\\)\nEsta distancia tiene en cuenta las correlaciones entre las variables.\nNo aumenta por el simple hecho de aumentar el número de variables registradas, sino que solamente aumentará cuando las nuevas variables no sean redundantes con respecto de la información aportada por las anteriores.\nEsta es la distancia más utilizada cuando todas las variables son cuantitativas."
  },
  {
    "objectID": "EM.html#distancias-para-variables-binarias",
    "href": "EM.html#distancias-para-variables-binarias",
    "title": "2  Análisis Multivariante",
    "section": "2.11 Distancias para variables binarias:",
    "text": "2.11 Distancias para variables binarias:\nSi \\(X_1,\\ldots,X_p\\) son variables que toman valores \\(\\{0,1\\}\\), existen muchos coeficientes de similaridad \\(s_{ij}\\) entre dos observaciones \\(i,j\\), calculados a partir de las frecuencias tales que \\(a+b+c+d=p\\), donde:\n\n\\(a\\): el número de variables con respuesta 1 en ambos individuos,\n\\(b\\): el número de variables con respuesta 0 en la observación \\(i\\) y con respuesta 1 en el individuo \\(j\\),\n\\(c\\): el número de variables con respuesta 1 en la observación \\(i\\) y con respuesta 0 en la observación \\(j\\),\n\\(d\\): el número de variables con respuesta 0 en ambos individuos.\n\nAlgunos coeficientes de similaridad son:\nSokal y Michener: \\(s_{ij}=\\frac{a+d}{p}\\), Jaccard: \\(s_{ij} =\\frac{a}{a+b+c}\\)\nAplicando uno de estos coeficientes a un conjunto de \\(n\\) objetos se obtiene una matriz de similaridades \\({\\bf S}=(s_{ij})_{n\\times n}\\)\nEjemplo: Se han medido 6 variables sobre 3 individuos:\n\n\n\nind.\n\\(X_1\\)\n\\(X_2\\)\n\\(X_3\\)\n\\(X_4\\)\n\\(X_5\\)\n\\(X_6\\)\n\n\n\n\n1\n1\n1\n0\n0\n1\n1\n\n\n2\n1\n1\n1\n0\n0\n1\n\n\n3\n1\n0\n0\n1\n0\n1\n\n\n\n\n\n\n\n\n\n\n\n\nLas matrices de similaridad son: \\[{\\bf S}_{Sokal}=\\left(\\begin{array}{ccc}\n1 & 0.6667 & 0.5\\\\\n0.667 & 1 & 0.5\\\\\n0.5 & 0.5 & 1\n\\end{array}\\right), \\;\\;\\; {\\bf S}_{Jaccard}=\\left(\\begin{array}{ccc}\n1 & 0.6 & 0.4\\\\\n0.6 & 1 & 0.4\\\\\n0.4 & 0.4 & 1\n\\end{array}\\right) \\]"
  },
  {
    "objectID": "EM.html#distancias-para-variables-categóricas",
    "href": "EM.html#distancias-para-variables-categóricas",
    "title": "2  Análisis Multivariante",
    "section": "2.12 Distancias para variables categóricas:",
    "text": "2.12 Distancias para variables categóricas:\nSe mide una variable categórica nominal con \\(k\\) niveles sobre una muestra de \\(n = n_1+\\cdots+n_g\\) observaciones provenientes de \\(g\\) poblaciones diferentes. Se desea obtener una medida de disimilaridad entre estas poblaciones.\nEn estas condiciones, el vector de frecuencias de cada población \\({\\bf n}_{\\alpha}=(n_{\\alpha1},\\ldots,n_{\\alpha k})\\), para \\(\\alpha=1,\\ldots, g\\), tiene una distribución conjunta multinomial con parámetros \\((n_{\\alpha}, {\\bf p}_{\\alpha})\\), donde \\(n_{\\alpha}=n_{\\alpha1}+\\ldots +n_{\\alpha k}\\) y \\({\\bf p}_{\\alpha}=(p_{\\alpha 1},\\ldots,p_{\\alpha k}\\) es el vector de probabilidades de los \\(k\\) niveles en la población \\(\\alpha\\) (con \\(p_{\\alpha 1}+\\ldots + p_{\\alpha k}=1\\)).\nEjemplo: La siguiente tabla contiene las proporciones génicas observadas entre 10 poblaciones. Observa que las filas suman 1.\n\n\n\nPoblación\ngrupo A\ngrupo AB\ngrupo B\ngrupo O\n\n\n\n\nfrancesa\n0.21\n0.06\n0.06\n0.67\n\n\ncheca\n0.25\n0.04\n0.14\n0.57\n\n\ngermánica\n0.22\n0.06\n0.08\n0.64\n\n\nvasca\n0.19\n0.04\n0.02\n0.75\n\n\nchina\n0.18\n0.00\n0.15\n0.67\n\n\nainu\n0.23\n0.00\n0.28\n0.49\n\n\nesquimal\n0.30\n0.00\n0.06\n0.64\n\n\nnegra USA\n0.10\n0.06\n0.13\n0.71\n\n\nespañola\n0.27\n0.04\n0.06\n0.63\n\n\negipcia\n0.21\n0.05\n0.20\n0.54\n\n\n\nDos medidas de disimilaridad para este tipo de variables son:\n\nDistancia de Bhattacharyya \\[d^2_{ij}=arccos\\left( \\displaystyle\\sum_{l=1}^k \\sqrt{p_{il} \\; p_{jl}}\\right)\\]\nDistancia de Balakrishnan-Sanghvi \\[d^2_{ij}= 2 \\displaystyle\\sum_{l=1}^k \\frac{(p_{il}-p_{jl})^2}{p_{il}+p_{jl}} \\]\n\nPara los datos del ejemplo, la matriz de distancias de Bhattacharyya es:\n\npoblacion=c(\"francesa\",\"checa\",\"germánica\",\"vasca\",\"china\",\"ainu\",\"esquimal\",\"negra USA\",\"española\",\"egipcia\")\ngrupoA=c(0.21,0.25,.22,.19,.18,.23,.3,.1,.27,.21)\ngrupoAB=c(.06,.04,.06,.04,.00,.00,.00,.06,.04,.05)\ngrupoB=c(.06,.14,.08,.02,.15,.28,.06,.13,.06,.2)\ngrupoO=c(.67,.57,.64,.75,.67,.49,.64,.71,.63,.54)\ndatos=data.frame(poblacion,grupoA,grupoAB,grupoB,grupoO)\n\n\nlibrary(philentropy)\ndistance(datos[,-1], method = \"bhattacharyya\")\n\n              v1          v2           v3          v4         v5         v6\nv1  0.0000000000 0.012324966 0.0009483429 0.007777165 0.04156128 0.08081738\nv2  0.0123249656 0.000000000 0.0066949713 0.035943482 0.02529075 0.03431210\nv3  0.0009483429 0.006694971 0.0000000000 0.013834658 0.03731750 0.06762516\nv4  0.0077771654 0.035943482 0.0138346582 0.000000000 0.05279229 0.11642553\nv5  0.0415612774 0.025290748 0.0373174959 0.052792291 0.00000000 0.01879186\nv6  0.0808173801 0.034312099 0.0676251647 0.116425530 0.01879186 0.00000000\nv7  0.0347713474 0.030975271 0.0343982089 0.034376078 0.01808704 0.04888218\nv8  0.0172057129 0.022070358 0.0157266766 0.032991841 0.03716560 0.07012655\nv9  0.0032054844 0.009334768 0.0030312958 0.011543973 0.03561201 0.06782787\nv10 0.0244823653 0.004028694 0.0160452869 0.057509566 0.03136123 0.02962590\n            v7         v8          v9         v10\nv1  0.03477135 0.01720571 0.003205484 0.024482365\nv2  0.03097527 0.02207036 0.009334768 0.004028694\nv3  0.03439821 0.01572668 0.003031296 0.016045287\nv4  0.03437608 0.03299184 0.011543973 0.057509566\nv5  0.01808704 0.03716560 0.035612005 0.031361231\nv6  0.04888218 0.07012655 0.067827866 0.029625898\nv7  0.00000000 0.06655156 0.020625956 0.052957776\nv8  0.06655156 0.00000000 0.030016960 0.020075979\nv9  0.02062596 0.03001696 0.000000000 0.024651208\nv10 0.05295778 0.02007598 0.024651208 0.000000000\n\n\nLas poblaciones más cercanas (según la distancia de Battacharyya) son la francesa y la germánica con \\(d_{1,3}=0.00095\\). Mientras que los más alejados son las poblaciones francesa y ainu con \\(d_{1,6}=0.08082\\)."
  },
  {
    "objectID": "EM.html#distancias-para-variables-mixtas",
    "href": "EM.html#distancias-para-variables-mixtas",
    "title": "2  Análisis Multivariante",
    "section": "2.13 Distancias para variables mixtas",
    "text": "2.13 Distancias para variables mixtas\nCuando disponemos de un conjunto de observaciones sobre variables tanto cuantitativas como cualitativas.\nSe define la distancia de Gower como \\(d^2_{ij}=1-s_{ij}\\), donde \\[s_{ij}=\\frac{\\sum_{k=1}^{p_1}(1-|x_{ik}-x_{jk}|/G_k)+a+\\alpha}{p_1+(p_2-d)+p_3}\\] es el coeficiente de similaridad de Gower,\n\n\\(p_1\\) es el número de variables cuantitativas continuas,\n\\(p_2\\) es el número de variables binarias,\n\\(p_3\\) es el número de variables cualitativas (no binarias),\n\\(a\\) es el número de coincidencias (1,1) en las variables binarias,\n\\(d\\) es el número de coincidencias (0,0) en las variables binarias,\n\\(\\alpha\\) es el número de coincidencias en las variables cualitativas (no binarias) y\n\\(G_h\\) es el rango de la \\(k\\)-ésima variable cuantitativa.\n\nEjemplo: Considera 7 variables registradas sobre 50 jugadores de la liga española de fútbol.\n\n\n\n\n\n\n\n\n\n\n\\(X_1\\) es el número de goles marcados, \\(X_2\\) edad (en años),\n\\(X_3\\) altura (m), \\(X_4\\) peso (kg),\n\\(X_5\\) pierna buena del jugador (1=drecha, 0=izquierda),\n\\(X_6\\) nacionalidad: (1=Argentina, 2=Brasil, 3=Camerun, 4=Italia, 5=España, 6=Francia, 7=Uruguay, 8=Portugal, 9=Inglaterra).\n\\(X_7\\) tipo de estudios (1=sin estudios, 2=básicos, 3=medios, 4=superiores)\n\n\nfutbol=read.table(\"datos/jugadores.txt\")\nlibrary(StatMatch)\ndistancia=gower.dist(futbol)\nhead(distancia,2)\n\n          [,1]      [,2]      [,3]      [,4]      [,5]      [,6]      [,7]\n[1,] 0.0000000 0.2422439 0.4123377 0.3910534 0.3887085 0.2007576 0.3412698\n[2,] 0.2422439 0.0000000 0.3331530 0.3317100 0.2575758 0.3510101 0.1863276\n          [,8]      [,9]     [,10]     [,11]     [,12]     [,13]     [,14]\n[1,] 0.4585137 0.4370491 0.3645382 0.4132395 0.3329726 0.3098846 0.5128066\n[2,] 0.3555195 0.3181818 0.2334055 0.2229437 0.2736291 0.1747835 0.3419913\n         [,15]     [,16]     [,17]     [,18]     [,19]    [,20]     [,21]\n[1,] 0.3879870 0.3872655 0.3910534 0.3324315 0.3823954 0.333153 0.4880952\n[2,] 0.3088023 0.2323232 0.2321429 0.1659452 0.2869769 0.463925 0.3930375\n         [,22]     [,23]     [,24]     [,25]     [,26]     [,27]     [,28]\n[1,] 0.2436869 0.3890693 0.3311688 0.3154762 0.3551587 0.4462482 0.3059163\n[2,] 0.3715729 0.2341270 0.2718254 0.4462482 0.2521645 0.2754329 0.4366883\n         [,29]     [,30]     [,31]     [,32]     [,33]    [,34]     [,35]\n[1,] 0.3977273 0.1803752 0.2561328 0.3612915 0.3450577 0.247114 0.3481241\n[2,] 0.2427850 0.3309885 0.1369048 0.2063492 0.2500000 0.403860 0.4788961\n         [,36]     [,37]     [,38]     [,39]     [,40]     [,41]     [,42]\n[1,] 0.4202742 0.3151154 0.4197330 0.3250361 0.2247475 0.2023810 0.2676768\n[2,] 0.2783189 0.1969697 0.3405483 0.2061688 0.3786075 0.4446248 0.1484488\n         [,43]     [,44]     [,45]     [,46]     [,47]     [,48]     [,49]\n[1,] 0.1801948 0.3403680 0.3762626 0.1524170 0.4889971 0.3986291 0.2987013\n[2,] 0.4224387 0.2453102 0.2812049 0.3318903 0.3340548 0.2278139 0.1935426\n         [,50]\n[1,] 0.4224387\n[2,] 0.2516234\n\n\nCon la librería stats de R base puedes calcular la distancia: “euclidean”, “maximum”, “manhattan”, “canberra”, “binary” o “minkowski”.\nLa función distance()de la libreria philentropy es capaz de calcular 46 diferentes distancias/similaridades.\n\ngetDistMethods()\n\n [1] \"euclidean\"         \"manhattan\"         \"minkowski\"        \n [4] \"chebyshev\"         \"sorensen\"          \"gower\"            \n [7] \"soergel\"           \"kulczynski_d\"      \"canberra\"         \n[10] \"lorentzian\"        \"intersection\"      \"non-intersection\" \n[13] \"wavehedges\"        \"czekanowski\"       \"motyka\"           \n[16] \"kulczynski_s\"      \"tanimoto\"          \"ruzicka\"          \n[19] \"inner_product\"     \"harmonic_mean\"     \"cosine\"           \n[22] \"hassebrook\"        \"jaccard\"           \"dice\"             \n[25] \"fidelity\"          \"bhattacharyya\"     \"hellinger\"        \n[28] \"matusita\"          \"squared_chord\"     \"squared_euclidean\"\n[31] \"pearson\"           \"neyman\"            \"squared_chi\"      \n[34] \"prob_symm\"         \"divergence\"        \"clark\"            \n[37] \"additive_symm\"     \"kullback-leibler\"  \"jeffreys\"         \n[40] \"k_divergence\"      \"topsoe\"            \"jensen-shannon\"   \n[43] \"jensen_difference\" \"taneja\"            \"kumar-johnson\"    \n[46] \"avg\""
  },
  {
    "objectID": "EM.html#visualización-de-matrices-de-distancias",
    "href": "EM.html#visualización-de-matrices-de-distancias",
    "title": "2  Análisis Multivariante",
    "section": "2.14 Visualización de matrices de distancias",
    "text": "2.14 Visualización de matrices de distancias\nUna solución sencilla para visualizar las matrices de distancia es utilizar la función fviz_dist() de la librería factoextra. Otros métodos especializados, como la agrupación jerárquica aglomerativa o el mapa de calor, las presentaremos más adelante.\nEjemplo: Visualizamos la tabla de los jugadores de fútbol.\n\nlibrary(factoextra)\nfviz_dist(as.dist(distancia))\n\n\n\n\nSi el color es rojo, la similitud es alta (es decir, distancia cero). Si el color es azul, la similitud es baja. El nivel de color es proporcional al valor de la distancia entre las observaciones. Se han ordenado a los jugadores por similitud.\nEjemplo: Otro ejemplo que podéis consultar son los mapas de calor que utilizamos en BIOCOM (grupo de investigación de la UIB en Biología Computacional y Bioinformática) para presentar las diferencias entre kernels (“distancias”) para comparar grafos del metabolismo entre organismos Figura 3 - Exploring the expressiveness of abstract metabolic networks\nObservamos una buena separación entre animales, plantas y hongos en todos los mapas de calor. Los protistas están separados, pero en general están más cerca de los hongos que del resto de Eucariotas. Además, en todos los mapas de calor, los animales están bien separados de los demás reinos y muestran un patrón interno adicional que merece un análisis más detallado. También cabe destacar que todos los kernels ponen de manifiesto una marcada diferencia entre los animales y las plantas. Las plantas están más cerca de los hongos que de los animales."
  },
  {
    "objectID": "Inferencia.html",
    "href": "Inferencia.html",
    "title": "3  Inferencia multivariante en poblaciones normales",
    "section": "",
    "text": "Para la exploración de datos multivariantes, no empleamos modelos formales para dar respuestas a las preguntas formuladas, sin embargo, en algunas situaciones es posible ajustar modelos “formales” para probar una hipótesis sobre los parámetros de la función de densidad de probabilidad de esa población.\nLa función de densidad de probabilidad asumida casi universalmente como la base de las inferencias para los datos multivariantes es la normal multivariante.\nEn esta parte, nos apoyaremos en el libro “Nuevos Métodos de Análisis Multivariante” del profesor Carles M. Cuadras, catedrático de Estadística (1980-2009) y emérito (2009-2015) de la Universidad de Barcelona. Trabajaremos con la versión revisada en marzo de 2018, capítulos 2 y 3.\nEspecíficamente, daremos una breve descripción de la función de densidad normal multivariante y de las formas de evaluar si un conjunto de datos multivariantes se ajusta a la densidad.\nHay características importantes de la distribución normal:\n\nLas distribuciones marginal y condicional de la normal multivariante también se distribuyen normalmente, al igual que las combinaciones lineales.\nAsí como la distribución normal domina las técnicas univariantes, la distribución normal multivariante desempeña un papel importante en algunos procedimientos multivariantes. Estudiamos la estimación de medias y covarianzas en poblaciones normales y contrastes de hipótesis multivariantes."
  },
  {
    "objectID": "ACP.html#introducción",
    "href": "ACP.html#introducción",
    "title": "4  Análisis de Componentes Principales",
    "section": "4.1 Introducción",
    "text": "4.1 Introducción\nCuando nos enfrentamos a problemas que se desarrollan en espacios muestrales de altas dimensiones, puede interesar considerar el estudio de dichos problemas en espacios de menor dimensión.\nEl análisis de componentes principales, PCA por sus siglas en inglés (Principal Component Analysis), es uno de los métodos más populares del análisis multivariado.\nEl PCA se utiliza cuando deseamos obtener una representación en menor dimensión para un conjunto de variables cuantitativas correlacionadas y queremos expresar la información importante como un conjunto de pocas variables nuevas llamadas componentes principales. Estas componentes se corresponden con una combinación lineal de las variables originales.\nDado que la información de un conjunto de datos se corresponde a la variación total que contiene, el objetivo del PCA es identificar direcciones (o componentes principales) a lo largo de las cuales la variación en los datos es máxima.\nLa reducción de la dimensionalidad consiste en describir con cierta precisión los valores de las \\(p\\) variables por un pequeño subconjunto \\(r&lt;p\\) de ellas con una pérdida mínima de información. Por lo tanto, proyectaremos la muestra original en el nuevo subespacio pero conservando algunas características. En particular, vamos a realizar un ajuste ortogonal por mínimos cuadrados.\nAlgunos ámbitos importantes en los que es frecuente la utilización de la Reducción de la dimensionalidad podrían ser:\n\nReconocimiento facial: Al partir de una proyección facial de tamaño \\(M \\times N\\) píxeles puede resultar complicado el estudio completo de dicha imagen, puesto que da lugar a un vector con dimensión excesivamente alta. De modo que debemos reducir la dimensionalidad pero de forma que nos permita generar un sistema de clasificación facial de un individuo entre el resto de la población.\n\n\n\nModelización de secuencias genómicas: Considera una proteína, formada por una secuencia de aminoácidos donde es posible encontrar hasta 20 tipos diferentes, su longitud puede llegar a ser de decenas hasta cientos de decenas de aminoácidos. De modo que, las proteínas con igual estructura pueden ser agrupadas en familias y el conjunto de familias proteicas distintas tendrá una dimensión inferior al conjunto de todas las proteínas. Pudiendo así encontrar de forma menos compleja propiedades particulares o incluso identificar nuevos miembros de una familia.\n\n\n4.1.1 Matriz (tabla) de datos\nSupongamos que disponemos de una tabla de datos con los valores de \\(p\\)-variables en \\(n\\) elementos de una población arreglados en una matriz \\(\\mathbf{X}\\) de la siguiente forma:\n\n\n\n\n\n\n\n\n\n\n\n\nID\n\\(\\bf{x}_1\\)\n\\(\\bf{x}_2\\)\n\\(\\ldots\\)\n\\(\\bf{x}_p\\)\n\\(\\bf{v}_1\\)\n\\(\\bf{v}_2\\)\n\n\n\n\n\\(1\\)\n\\(x_{11}\\)\n\\(x_{12}\\)\n\\(\\ldots\\)\n\\(x_{1p}\\)\n\\(v_{11}\\)\n\\(v_{12}\\)\n\n\n\\(2\\)\n\\(x_{21}\\)\n\\(x_{22}\\)\n\\(\\ldots\\)\n\\(x_{2p}\\)\n\\(v_{21}\\)\n\\(v_{22}\\)\n\n\n\\(3\\)\n\\(x_{31}\\)\n\\(x_{32}\\)\n\\(\\ldots\\)\n\\(x_{3p}\\)\n\\(v_{31}\\)\n\\(v_{32}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\ddots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(n\\)\n\\(x_{n1}\\)\n\\(x_{n2}\\)\n\\(\\ldots\\)\n\\(x_{np}\\)\n\\(v_{n1}\\)\n\\(v_{n2}\\)\n\n\n\n\nDonde las variables \\(x_1,\\ldots, x_n\\) describen a los \\(n\\) individuos observados.\nLas variables \\(v_1\\), \\(v_2\\) son de perfil (o explicativas) y ayudan a interpretar la variabilidad de los datos.\n\nEl objetivo del análisis es la reducción de la dimensionalidad. Buscamos un espacio de variables más reducido y fácil de interpretar.\nEl problema es que si reducimos el número de variables es posible que “perdamos parte toda la variabilidad de los datos originales”.\nAsí la idea básica es consentir una pérdida de información para lograr una ganancia en la significación.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n4.1.2 Enfoque geométrico\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSupongamos que \\(p=2\\) y que la nube de puntos de nuestra matriz de datos es la de la siguiente figura:\n\n\n\n\n\n\n\n\n\nLa siguiente figura muestra las dos componentes principales, es decir, las direcciones de las proyecciones que tienen máxima variabilidad.\n\n\n\n\n\n\n\n\n\nSi proyectamos en la dirección de la primera componente obtendremos las proyecciones siguientes (en color azul):\n\n\n\n\n\n\n\n\n\n\nLo que significa que la varianza de los puntos azules es máxima; en el sentido de que cualquier otra dirección o recta, las proyecciones sobre ésta tendrán a lo más igual varianza.\nLos puntos azules representan las coordenadas que tienen los puntos de nuestra tabla de datos (centrada) tomando como eje de abcisas la primera componente \\(CP_1\\).\nSi proyectamos en la dirección de la “segunda componente”, obtendremos las proyecciones siguientes (en color verde):"
  },
  {
    "objectID": "ACP.html#cálculo-de-las-componentes",
    "href": "ACP.html#cálculo-de-las-componentes",
    "title": "4  Análisis de Componentes Principales",
    "section": "4.2 Cálculo de las componentes",
    "text": "4.2 Cálculo de las componentes\n\n4.2.1 Cálculo de la primera componente\nLa primera componente principal se define como la combinación lineal de las variables originales que tiene varianza máxima. Los valores en esta primera componente de los \\(n\\) individuos se representarán por un vector \\(\\mathbf{z_1}\\) dado por \\[\\mathbf{z_1}=\\mathbf{Xa_1}.\\] Si las variables originales tienen media cero, \\(\\mathbf{z_1}\\) también tendrá media nula. Su varianza será\n\\[\\begin{equation} \\tag{1}\nVar(\\mathbf{z_1})=\\frac{1}{n}\\mathbf{z_1^t}\\mathbf{z_1}=\\frac{1}{n}\\mathbf{a_1^t X^t}\\mathbf{Xa_1}=\\mathbf{a_1^t S}\\mathbf{a_1}\n\\end{equation}\\]\ndonde \\(\\mathbf{S}\\) es la matriz de varianzas-covarianzas de las observaciones.\nEs obvio que podemos maximizar la varianza tanto como queramos aumentando el módulo del vector \\(\\mathbf{a_1}\\). Para que maximizar (1) tenga solución debemos imponer una restricción al módulo del vector \\(\\mathbf{a_1}\\), y, sin pérdida de generalidad, impondremos que \\(\\mathbf{a_1^t}\\mathbf{a_1}=1\\). Introducimos esta restricción mediante el multiplicador de Lagrange: \\[M=\\mathbf{a_1^t}S\\mathbf{a_1}-\\lambda(\\mathbf{a_1^t}\\mathbf{a_1}-1)\\] y maximizamos esta expresión derivando respecto a las componentes de \\(\\mathbf{a_1}\\) e igualando a cero. Entonces \\[\\frac{\\partial M}{\\partial \\mathbf{a_1}}=2\\mathbf{Sa_1}-2 \\lambda \\mathbf{a_1}=0,\\]\ncuya solución es:\n\\[\\begin{equation} \\tag{2}\n\\mathbf{Sa_1}= \\lambda \\mathbf{a_1},\n\\end{equation}\\]\nque implica que \\(\\mathbf{a_1}\\) es un vector propio de la matriz \\(\\mathbf{S}\\), y \\(\\lambda\\) su correspondiente valor propio.\nPara determinar qué valor propio de \\(\\mathbf{S}\\) es la solución de (2), multiplicamos por la izquierda por \\(\\mathbf{a_1^t}\\) a esta ecuación,\n\\[\\begin{equation}\n\\mathbf{a_1^t S a_1}= \\lambda \\mathbf{a_1^t a_1}= \\lambda\n\\end{equation}\\]\ny concluimos, por (1) que \\(\\lambda\\) es la varianza de \\(\\mathbf{z_1}\\). Como esta es la cantidad que queremos maximizar, \\(\\lambda\\) será el mayor valor propio de la matriz \\(\\mathbf{S}\\). Su vector asociado, \\(\\mathbf{a_1}\\), define los coeficientes de cada variable en la primera componente principal."
  },
  {
    "objectID": "ACP.html#ejemplo",
    "href": "ACP.html#ejemplo",
    "title": "4  Análisis de Componentes Principales",
    "section": "4.3 Ejemplo:",
    "text": "4.3 Ejemplo:\nCalculad la primera componente principal con los logaritmos del fichero acciones.txt que podéis descargad en Aula Digital. Las observaciones corresponden a distintas acciones que cotizan en el mercado español y las variables son:\n\nV1 la rentabilidad efectiva por dividendos,\nV2 la proporción de beneficios que va a dividendos\nV3 el ratio entre precio por acción y beneficios.\n\n¿Cuál de las variables está mejor representada por la componente principal 1?\nSolución\nCargamos los datos\n\ndatos&lt;-read.table(\"datos/acciones.txt\")\nn=dim(datos)[1]\nn\n\n[1] 34\n\n\nLa tabla presenta tres medidas de rentabilidad de 34 acciones en bolsa. Vamos a reescribirlas para ganar interpretabilidad. Llamamos \\(d\\) a los dividendos por acción, \\(p\\) al precio de la acción, \\(B\\) al beneficio y \\(N\\) al número de acciones. Entonces:\n\nV1 es la rentabilidad efectiva por dividendos, es decir, dividendos repartidos por acción divididos por precio de la acción. \\(V1=\\frac{d}{p}\\).\nV2 es la proporción de beneficios que va a dividendos. \\(V2=\\frac{dN}{B}\\).\nV3 es el cociente entre precio y beneficio por acción. \\(V3=\\frac{p}{B/N}=\\frac{pN}{B}\\)\n\nExploramos los datos\n\n\n\n\n\nLas densidades indican un alejamiento de la distribución normal para las tres variables. Las dos primeras sugieren la presencia de dos grupos de datos distintos (acciones con comportamientos distintos), y la tercera tiene una densidad muy asimétrica, con al menos un valor atípico.\nPor la forma de cálculo de las variables, es lógico esperar alta correlación positiva entre V1 y v2. La correlación negativa baja entre V1 y v3, así como alta negativa entre V2 yv3.\nAhora, estimamos la matriz de varianzas-covarianzas de las variables originales V1, v2 y V3, con el estimador sesgado:\n\nS0=round(((n-1)/n)*cov(datos),2)\nS0\n\n       V1     V2     V3\nV1  28.24  97.49 -15.24\nV2  97.49 559.28 -18.00\nV3 -15.24 -18.00  21.90\n\n\nRecordad que las densidades de las tres variables han mostrado una clara falta de normalidad y entre ellas hay relaciones no lineales. En estas condiciones, la matriz de varianzas-covarianzas no es un buen resumen de las relaciones de dependencia existentes.\nPara tratar de resolver el problema anterior, podemos usar el logaritmo que es una de las transformaciones más utilizadas para datos positivos en los siguientes casos:\n\nLos datos describen el tamaño de las cosas (renta de países o familias habitantes en las principales ciudades del mundo, tamaño de empresas, consumo de energía en hogares, etc), son generalmente muy asimétricas, pero se convierten en aproximadamente simétricas al expresar la variable en logaritmos.\nCuando las diferencias relativas entre los valores de la variable son importantes, conviene expresar las variables en logaritmos, ya que las diferencias entre logaritmos equivalen a diferencias relativas en la escala original.\nLa variabilidad de las variables transformadas es independiente de las unidades de medida. Para mostrar esta propiedad, supongamos que tenemos una sola variable aleatoria \\(X\\) que transformamos con \\(Y = \\log X\\), la variable transformada tiene media \\(\\mu_Y\\) y varianza \\(\\sigma^2_Y\\). Si cambiamos las unidades de medida de \\(X\\) multiplicando por una constante, \\(Z = kX\\), entonces la variable \\(\\log Z\\) tiene media \\(Y + \\log k\\) y la misma varianza que la variable \\(\\log X\\). Por tanto, al tomar logaritmos en las variables, las varianzas pueden compararse aunque los datos tengan distintas dimensiones.\n\nDe acuerdo a los anterior, aplicamos una transformación logarítmica a nuestros datos, con lo cual, la matriz de varianzas-covarianzas de las variables transformadas, sería:\n\ndatos_l=log(datos)\nS=round(((n-1)/n)*cov(datos_l),2)\nS\n\n      V1    V2    V3\nV1  0.35  0.15 -0.19\nV2  0.15  0.13 -0.03\nV3 -0.19 -0.03  0.16\n\n\nObservad que los logaritmos modifican mucho los resultados. Los datos ahora son más homogéneos y la variable de mayor varianza pasa a ser la primera, el logaritmo de la rentabilidad efectiva, mientras que la menor es la segunda, el logaritmo de la proporción de beneficios que va a dividendos. La relación entre el logaritmo del ratio precio/beneficios y la rentabilidad efectiva es negativa. Las otras relaciones son débiles.\nCalculamos los valores propios de la matriz de varianzas covarianzas de los datos transformados que son las raíces de la ecuación\n\\[\n\\begin{equation}\n\\begin{split}\n|S-\\lambda I| & =  \\left| \\begin{pmatrix}0.35 & 0.15 & -0.19\\\\ 0.15 & 0.13 & -0.03 \\\\ -0.19 & -0.03 & 0.16\\end{pmatrix} -\\begin{pmatrix} \\lambda & 0 & 0\\\\ 0 & \\lambda & 0 \\\\ 0 & 0 & \\lambda\\end{pmatrix}  \\right| \\\\ \\\\\n& = 0.000382-0.0628\\lambda+0.64 \\lambda^2 -\\lambda^3 =0\n\\end{split}\n\\end{equation}\n\\]\nBuscamos las raíces de este polinomio son\n\nlibrary(polynom)\np=polynomial(coef=c(0.00038,-0.0628,0.64,-1))\nraices=round(solve(p),3)\n\nLas raíces son \\(\\lambda_1\\)=0.521, \\(\\lambda_2\\)=0.113 y \\(\\lambda_3\\)=0.006.\nEl vector propio asociado a \\(lambda_1\\) nos da los pesos de la primera componente principal. Para calcularlo manualmente, debemos resolver el sistema \\[S \\mathbf{a_1}= \\lambda_1 \\mathbf{a_1}\\] que conduce a\n\\[\n\\begin{equation}\n\\begin{split}\n\\begin{pmatrix}0.35 & 0.15 & -0.19\\\\ 0.15 & 0.13 & -0.03 \\\\ -0.19 & -0.03 & 0.16\\end{pmatrix} \\begin{pmatrix} a_{11} \\\\a_{12} \\\\ a_{131} \\end{pmatrix} &=0.521 \\cdot \\begin{pmatrix} a_{11} \\\\a_{12} \\\\ a_{131} \\end{pmatrix}\n\\end{split}\n\\end{equation}\n\\] \\[\n\\begin{equation}\n\\begin{split}\n\\begin{pmatrix}-0.171 a_{11}+0.15 a_{12} -0.19 a_{13} \\\\ 0.15 a_{11}-0.391 a_{12} -0.03 a_{13}\\\\ -0.19 a_{11}-0.03 a_{12} -0.361 a_{13}\\end{pmatrix}  &= \\begin{pmatrix} 0 \\\\0 \\\\ 0 \\end{pmatrix}\n\\end{split}\n\\end{equation}\n\\] Este sistema es compatible indeterminado. Para encontrar una de las infinitas soluciones tomemos la primera variable como parámetro, \\(x\\), y resolvemos el sistema en función de \\(x\\). La solución es,\n\\[\\{a_{11}=x,\\; a_{12}=0.427x,\\; a_{13}=-0.562x\\]\nEl valor de \\(x\\) lo obtenemos imponiendo que el vector tenga norma uno, con lo que resulta:\n\\[\\mathbf{a_1}=\\begin{pmatrix} -0.817 \\\\-0.349 \\\\ 0.459 \\end{pmatrix}\\]\nPor lo tanto, la primera componente es \\[Z_1=-0.817 \\log(d/p)-0.349 \\log(dN/p)+0.459 \\log(pN/B)\\]\nque indica que este primer componente depende básicamente de la rentabilidad por dividendos. Esta variable es la que mejor explica la variabilidad conjunta de las acciones."
  },
  {
    "objectID": "ACP.html#cálculo-de-la-segunda-componente",
    "href": "ACP.html#cálculo-de-la-segunda-componente",
    "title": "4  Análisis de Componentes Principales",
    "section": "4.4 Cálculo de la segunda componente",
    "text": "4.4 Cálculo de la segunda componente\nVamos a obtener el mejor plano de proyección de l matriz \\(\\mathbf{X}\\). Lo calcularemos estableciendo como función objetivo que la suma de las varianzas de \\(\\mathbf{z_1}=\\mathbf{Xa_1}\\) y \\(\\mathbf{z_2}=\\mathbf{Xa_2}\\) sean máximas, donde \\(\\mathbf{a_1}\\) y \\(\\mathbf{a_2}\\) son los vectores que definen el plano. La función objetivo será:\n\\[\\begin{equation} \\tag{3}\n\\phi=\\mathbf{a_1^t S}\\mathbf{a_1} + \\mathbf{a_2^t S}\\mathbf{a_2} - \\lambda_1 (\\mathbf{a_1^t}\\mathbf{a_1}-1) - \\lambda_2 (\\mathbf{a_2^t}\\mathbf{a_2}-1)\n\\end{equation}\\]\nque incorpora las restricciones de que las direcciones deben de tener módulo unitario. Derivando e igualando a cero: \\[\\frac{\\partial \\phi}{\\partial \\mathbf{a_1}}=2\\mathbf{Sa_1}-2 \\lambda_1 \\mathbf{a_1}=0\\]\n\\[\\frac{\\partial \\phi}{\\partial \\mathbf{a_2}}=2\\mathbf{Sa_2}-2 \\lambda_2 \\mathbf{a_1}=0\\]\nLa solución del sistema es: \\[\\begin{equation} \\tag{4}\n\\mathbf{Sa_1}= \\lambda \\mathbf{a_1},\n\\end{equation}\\] \\[\\begin{equation} \\tag{5}\n\\mathbf{Sa_2}= \\lambda \\mathbf{a_2},\n\\end{equation}\\]\nque indica que \\(\\mathbf{a_1}\\) y \\(\\mathbf{a_2}\\) deben ser vectores propios de \\(\\mathbf{S}\\).\nTomando los vectores propios de norma uno y sustituyendo en (3), se obtiene que, en el máximo, la función objetivo es \\[\\begin{equation} \\tag{6}\n\\phi=\\lambda_1+\\lambda_2\n\\end{equation}\\]\nes claro que \\(\\lambda_1\\) y \\(\\lambda_2\\) deben ser los dos valores propios mayores de la matriz \\(\\mathbf{S}\\) y \\(\\mathbf{a_1}\\) y \\(\\mathbf{a_2}\\) sus correspondientes vectores propios.\nObservad que la covarianza entre \\(\\mathbf{z_1}\\) y \\(\\mathbf{z_2}\\), dada por \\(\\mathbf{a_1^t S a_2}\\) es cero ya que =0, y las variables\\(\\mathbf{z_1}\\) y \\(\\mathbf{z_2}\\) estarán incorreladas.\nSe puede demostrarse que si en lugar de maximizar la suma de varianzas, que es la traza de la matriz de covarianzas de la proyección, se maximiza la varianza generalizada (el determinante de la matriz de covarianzas) se obtiene el mismo resultado.\n\n4.4.1 Generalización\nAnálogamente, el espacio de dimensión \\(r\\) que mejor representa a los puntos viene definido por los vectores propios asociados a los \\(r\\) mayores valores propios de \\(\\mathbf{S}\\). Estas direcciones se denominan direcciones principales de los datos y a las nuevas variables por ellas definidas componentes principales. En general, la matriz \\(\\mathbf{X}\\) (y por tanto la \\(\\mathbf{S}\\)) tiene rango \\(p\\), existiendo entonces tantas componentes principales como variables que se obtendrán calculando los valores propios o raíces características, \\(\\lambda_1, \\ldots, \\lambda_p\\), de la matriz de varianzas y covarianzas de las variables, \\(\\mathbf{S}\\) , mediante:\n\\[\\begin{equation} \\tag{7}\n|\\mathbf{S}-\\lambda\\mathbf{I}|=0\n\\end{equation}\\]\ny sus vectores asociados son: \\[\\begin{equation} \\tag{8}\n(\\mathbf{S}-\\lambda_i\\mathbf{I})\\mathbf{a_i}=0.\n\\end{equation}\\]\nLos términos \\(\\lambda_i\\) son reales, al ser la matriz \\(\\mathbf{S}\\) simétrica, y positivos, ya que \\(\\mathbf{S}\\) es definida positiva.\nPor ser \\(\\mathbf{S}\\) simétrica si \\(\\lambda_j\\) y \\(\\lambda_h\\) son dos raíces distintas sus vectores asociados son ortogonales.\nSi \\(\\mathbf{S}\\) fuese semidefinida positiva de rango \\(r &lt; p\\), lo que ocurriría si \\(p−r\\) variables fuesen combinación lineal de las demás, habría solamente \\(r\\) raíces características positivas y el resto serían ceros.\nLlamando \\(\\mathbf{Z}\\) a la matriz cuyas columnas son los valores de las \\(p\\) componentes en los \\(n\\) individuos, estas nuevas variables están relacionadas con las originales mediante:\n\\[\\begin{equation} \\tag{9}\n\\mathbf{Z}=\\mathbf{X}\\mathbf{A}\n\\end{equation}\\]\ndonde \\(\\mathbf{A^t A}=\\mathbf{I}\\). Calcular las componentes principales equivale a aplicar una transformación ortogonal \\(\\mathbf{A}\\) a las variables \\(\\mathbf{X}\\) (ejes originales) para obtener unas nuevas variables \\(\\mathbf{Z}\\) incorreladas entre sí. Esta operación puede interpretarse como elegir unos nuevos ejes coordenados, que coincidan con los “ejes naturales” de los datos.\n\n\n4.4.2 Ejemplo:\nVamos a realizar el análisis de componentes principales (PCA) sobre el conjunto de datos de las acciones del mercado español, esta vez utilizando a R para calcular los valores y vectores propios.\nRecordad que ya hemos calculado la matriz de varianzas-covarianzas muestral sesgada de los datos transformados, \\(S\\)=0.35, 0.15, -0.19, 0.15, 0.13, -0.03, -0.19, -0.03, 0.16.\n\nsol=eigen(S)\n\nLos valores propios son:\n\\[\\lambda_1=0.521,\\quad \\lambda_2=0.113,\\quad \\lambda_3 = 0.007.\\]\n\nLos vectores propios ortonormales correspondientes a los valores propios, son los que aparecen con el nombre: $vectors\n\nLas expresiones de las variables nuevas \\(CP_i\\) en función de los logaritmos de las originales son:\n\\[\\begin{array}{rl}\nZ_1 = & 0.817 \\cdot \\log V_1 + 0.349\\cdot \\log V_2 - 0.459 \\cdot \\log V_3, \\\\\nZ_2 = & 0.043 \\cdot \\log V_1 + 0.758\\cdot \\log V_2 + 0.651 \\cdot \\log V_3, \\\\\nZ_3 = & 0.575 \\cdot \\log V_1 - 0.552 \\cdot \\log V_2 + 0.604 \\cdot \\log V_3, \\\\\n\\end{array}\\]\nLa nueva matriz de datos respecto de las componentes principales será:\n\nX=matrix(c(datos_l$V1,datos_l$V2,datos_l$V3),nrow=34)\nZ=X %*% sol$vectors # Z=XA  con A la matriz de vectores propios\nhead(Z)\n\n         [,1]     [,2]       [,3]\n[1,] 1.003343 5.678343 0.28103506\n[2,] 1.680545 4.608193 0.10346897\n[3,] 1.487526 4.652723 0.15673341\n[4,] 1.257073 4.543832 0.05520223\n[5,] 1.866053 4.187286 0.12455276\n[6,] 1.637393 3.919281 0.21592221\n\n\nSi representamos gráficamente las dos primeras componentes, podemos observar que se puede distinguir entre los dos grupos de acciones.\n\nplot(Z[,1],Z[,2])\n\n\n\n\n¿Qué hubiese pasado si centramos los datos?\nAunque no es obligatorio centrar la matriz de datos para PCA, es una práctica común y recomendada, ya que ayuda a garantizar que las componentes principales reflejen de manera más precisa la estructura de variabilidad en los datos.\nSi no se centran los datos, la primera componente principal estaría influenciada por la ubicación de los datos en el espacio original, es decir, por la media de los datos. En nuestro ejemplo, esto está minimizado por el efecto de la transformación logarítmica.\nVeamos los resultados centrando los datos\n\ncolMeans(X)\n\n[1] 2.071865 4.178913 2.117252\n\nHn=diag(n)-1/n # matriz de centrado\ncX=Hn%*%X # matriz centrada\nround(cX,3)\n\n        [,1]   [,2]   [,3]\n [1,] -0.848  0.318  1.291\n [2,] -0.443 -0.159  0.175\n [3,] -0.568 -0.222  0.325\n [4,] -0.819 -0.329  0.299\n [5,] -0.297 -0.425 -0.171\n [6,] -0.443 -0.758 -0.186\n [7,] -0.546 -0.014  0.351\n [8,] -0.462 -0.247  0.145\n [9,] -0.909 -0.183  0.571\n[10,] -0.848 -0.357  0.463\n[11,] -0.200 -0.493 -0.469\n[12,] -0.590 -0.483  0.500\n[13,] -0.443 -0.220  0.281\n[14,] -0.314 -0.397 -0.038\n[15,] -0.546 -0.212  0.550\n[16,] -0.098  0.008 -0.063\n[17,] -0.098 -0.117 -0.076\n[18,] -0.590 -0.110  0.376\n[19,] -0.018  0.255  0.281\n[20,]  0.701  0.391 -0.325\n[21,]  0.744  0.426 -0.200\n[22,]  0.649  0.346 -0.469\n[23,]  0.790  0.425 -0.200\n[24,]  0.713  0.359 -0.309\n[25,]  0.616  0.426 -0.230\n[26,]  0.656  0.425 -0.342\n[27,]  0.688  0.426 -0.186\n[28,]  0.835  0.389 -0.377\n[29,]  0.694  0.426 -0.309\n[30,]  0.707  0.348 -0.309\n[31,]  0.200  0.294 -0.076\n[32,] -0.140 -0.197 -0.230\n[33,]  0.595  0.296 -0.469\n[34,]  0.629 -0.638 -0.572\n\n\n\nSc&lt;-(1/n)*t(cX)%*%Hn%*%cX # estimador sesgado de la matriz de covarianza\nround(Sc,3) # daría igual cov(cX)*(n-1)/n\n\n       [,1]   [,2]   [,3]\n[1,]  0.352  0.147 -0.188\n[2,]  0.147  0.131 -0.031\n[3,] -0.188 -0.031  0.158\n\nsolc&lt;-eigen(Sc)\n\n\nZc=cX %*% solc$vectors \nplot(Zc[,1],Zc[,2])"
  },
  {
    "objectID": "ACP.html#propiedades-pca-con-la-matriz-de-covarianzas.",
    "href": "ACP.html#propiedades-pca-con-la-matriz-de-covarianzas.",
    "title": "4  Análisis de Componentes Principales",
    "section": "4.5 Propiedades PCA con la matriz de covarianzas.",
    "text": "4.5 Propiedades PCA con la matriz de covarianzas.\nLos componentes principales como nuevas variables tienen las propiedades siguientes:\n\nLas componentes principales reproducen la varianza total. \\[\\sum_{i=1}^p Var(\\mathbf{Z}_i)=\\sum_{i=1}^p \\lambda_i=tr(\\mathbf{S})=\\sum_{i=1}^p s_i^2\\].\nLos componentes principales tienen correlación cero entre sí (son incorrelados) por lo tanto su matriz de covarianzas es\n\n\\[\\mathbf{S}_{Z}=\\left(\\begin{array}{cccc}\n\\lambda_1& 0 &\\ldots &0\\\\\n0& \\lambda_{2}&\\ldots & 0\\\\\n\\vdots & \\vdots & & \\vdots\\\\\n0 & 0&\\ldots &\\lambda_{p}\n\\end{array}\n\\right)\\]\n\\(\\det(\\mathbf{S}_{Z})=\\prod_{i=1}^p \\lambda_i =\\det(\\mathbf{S})\\). Luego los componentes principales conservan la varianza generalizada.\n\nLa proporción de varianza explicada por la componente \\(j\\)-ésima es \\[\\frac{\\lambda_j}{\\sum_{i=1}^p \\lambda_i}.\\]\n\nAdemás al ser incorrelados la proporción de varianza explicada por los \\(k\\) primeros componentes es \\[\\frac{\\sum_{i=1}^k \\lambda_i}{\\sum_{i=1}^p \\lambda_i}.\\] \n\\(\\mbox{Cov}(\\tilde{\\mathbf{X}}_i, \\mathbf{Z}_j)=\\frac{\\sqrt{\\lambda_j} a_{j i}}{s_i}\\), donde \\(a_{j i}\\) es la \\(i\\)-ésima componente del vector propio \\(\\mathbf{a}_j\\)."
  },
  {
    "objectID": "ACP.html#pca-normado-o-con-matriz-de-correlaciones",
    "href": "ACP.html#pca-normado-o-con-matriz-de-correlaciones",
    "title": "4  Análisis de Componentes Principales",
    "section": "4.6 PCA normado o con matriz de correlaciones",
    "text": "4.6 PCA normado o con matriz de correlaciones\nLas componentes principales se obtienen maximizando la varianza de la proyección. En términos de las variables originales esto supone maximizar:\n\\[M=\\displaystyle \\sum_{i=1}^p a_i^2s_i^2+ 2 \\displaystyle \\sum_{i=1}^p  \\displaystyle \\sum_{j=i+1}^p a_i a_j s_{ij}\\] con la restricción \\(\\mathbf{a^ta}=1\\). Si alguna de las variables, por ejemplo la primera, tiene una varianza \\(s^2_1\\), mayor que las demás, la manera de aumentar \\(M\\) es hacer tan grande como podamos la coordenada \\(a_1\\) asociada a esta variable. En el límite si una variable tiene una varianza mucho mayor que las demás el primer componente principal coincidirá muy aproximadamente con esta variable.\nCuando las variables tienen unidades distintas esta propiedad no es conveniente: si disminuimos la escala de medida de una variable cualquiera, de manera que aumenten en magnitud sus valores numéricos (pasamos por ejemplo de medir en km. a medir en metros), el peso de esa variable en el análisis aumentará, ya que en la ecuación anterior:\n\nsu varianza será mayor y aumentará su coeficiente en el componente, \\(a_1\\), ya que contribuye más a aumentar M;\nsus covarianzas con todas las variables aumentarán, con el consiguiente efecto de incrementar \\(a_i\\).\n\nEn resumen, cuando las escalas de medida de las variables son muy distintas, la maximización de \\(M\\) dependerá decisivamente de estas escalas de medida y las variables con valores más grandes tendrán más peso en el análisis.\nSi queremos evitar este problema, conviene estandarizar las variables antes de calcular los componentes, de manera que las magnitudes de los valores numéricos de las variables \\(X\\) sean similares.\nLa estandarización resuelve otro posible problema. Si las variabilidades de la \\(X\\) son muy distintas, las variables con mayor varianza van a influir más en la determinación de la primera componente. Este problema se evita al estandarizar las variables, ya que entonces las varianzas son la unidad, y las covarianzas son los coeficientes de correlación. La ecuación a maximizar se transforma en:\n\\[M'=1 + 2 \\displaystyle \\sum_{i=1}^p  \\displaystyle \\sum_{j=i+1}^p a_i a_j r_{ij}\\] siendo \\(r_{ij}\\) el coeficiente de correlación lineal entre las variables \\(i\\), \\(j\\). En consecuencia la solución depende de la correlaciones y no de las varianzas.\nLas componentes principales normados se obtiene calculando los vectores y valores propios de la matriz R, de coeficientes de correlación. Llamando \\(\\lambda_p^R\\) a las raíces características de esa matriz, que suponemos no singular, se verifica que: \\[{\\sum_{i=1}^p \\lambda_i^R}=traza(R)=p\\] Las propiedades de las componentes extraídos de \\(R\\) son:\n\nLa proporción de variación explicada por \\(\\lambda_p^R\\) será \\(\\frac{\\lambda_p^R}{p}\\).\nLas correlaciones entre cada componente \\(z_j\\) y las variables \\(X\\) originales vienen dadas directamente por \\(a^t_j \\sqrt{\\lambda_j}\\), siendo \\(\\mathbf{z_j}=\\mathbf{Xa_j}\\).\n\nEstas propiedades son consecuencia inmediata de los resultados de la sección anterior.\n\nCuando las variables X originales están en distintas unidades conviene aplicar el análisis de la matriz de correlaciones o análisis normado.\nCuando las variables tienen las mismas unidades, ambas alternativas son posibles.\nSi las diferencias entre las varianzas de las variables son informativas y queremos tenerlas en cuenta en el análisis no debemos estandarizar las variables: por ejemplo, supongamos dos índices con la misma base pero uno fluctua mucho y el otro es casi constante. Este hecho es informativo, y para tenerlo en cuenta en el análisis, no se deben estandarizar las variables, de manera que el índice de mayor variabilidad tenga más peso. Por el contrario, si las diferencias de variabilidad no son relevantes podemos eliminarlas con el análisis normado. En caso de duda, conviene realizar ambos análisis, y seleccionar aquel que conduzca a conclusiones más informativas."
  },
  {
    "objectID": "ACP.html#análisis-de-componentes-principales-en-r",
    "href": "ACP.html#análisis-de-componentes-principales-en-r",
    "title": "4  Análisis de Componentes Principales",
    "section": "4.7 Análisis de Componentes Principales en R",
    "text": "4.7 Análisis de Componentes Principales en R\nVamos a explicar las funciones de R para hacer el PCA, para ello utilizaremos los famosos datos de las flores iris.\n\n\n\nhead(iris,2)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n\ntail(iris,2)\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n149          6.2         3.4          5.4         2.3 virginica\n150          5.9         3.0          5.1         1.8 virginica\n\n\nLo primero que hacemos es revisar si las variables están correlacionadas, requisito necesario para obtener una representación más simple de éstas.\n\ncov(iris[,1:4])\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length    0.6856935  -0.0424340    1.2743154   0.5162707\nSepal.Width    -0.0424340   0.1899794   -0.3296564  -0.1216394\nPetal.Length    1.2743154  -0.3296564    3.1162779   1.2956094\nPetal.Width     0.5162707  -0.1216394    1.2956094   0.5810063\n\ncor(iris[,1:4])\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length    1.0000000  -0.1175698    0.8717538   0.8179411\nSepal.Width    -0.1175698   1.0000000   -0.4284401  -0.3661259\nPetal.Length    0.8717538  -0.4284401    1.0000000   0.9628654\nPetal.Width     0.8179411  -0.3661259    0.9628654   1.0000000\n\n\nComparamos las funciones de R para hacer el PCA\n\n\n\n\n\n\n\n\n\nCalculamos las componentes con los datos escalados con la librería factoextra\n\nlibrary(ggplot2)\nlibrary(\"factoextra\")\niris.acp=prcomp(iris[,1:4], scale = TRUE)\n\nLos valores propios muestran el porcentaje de varianza explicada por cada componente principal.\n\nlambdas=get_eigenvalue(iris.acp)\nlambdas\n\n      eigenvalue variance.percent cumulative.variance.percent\nDim.1 2.91849782       72.9624454                    72.96245\nDim.2 0.91403047       22.8507618                    95.81321\nDim.3 0.14675688        3.6689219                    99.48213\nDim.4 0.02071484        0.5178709                   100.00000\n\n\nObservamos que las dos primeras componentes principales explican aproximadamente el 96% de la variación total. Puede ser razonable, trabajar con esas dos componentes para el análisis posterior de estos datos.\nUn método alternativo para determinar el número de componentes principales es observar el diagrama de valores propios ordenados de mayor a menor. El número de componentes se determina en el punto, más allá del cual los valores propios restantes son todos relativamente pequeños y de tamaño comparable.\n\nfviz_eig(iris.acp, addlabels = TRUE, ylim=c(0,100))\n\n\n\n\nLa representación de variables difiere de la gráfica de las observaciones: Las observaciones están representadas por sus proyecciones, pero las variables están representados por sus correlaciones.\n\nfviz_pca_var(iris.acp, col.var = \"contrib\",\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             repel = TRUE) \n\n\n\n\nEl gráfico anterior también se conoce como círculo de correlación variable. Muestra las relaciones entre todas las variables. Se puede interpretar de la siguiente manera:\n\nLas variables correlacionadas positivamente se agrupan.\nLas variables correlacionadas negativamente se colocan en lados opuestos del origen de la trama (cuadrantes opuestos).\nLa distancia entre variables y el origen mide la calidad de la representación de las variables, las que están alejadas del origen están bien representadas.\n\nLa calidad de representación de las variables se llama cos2 (coseno cuadrado, coordenadas cuadradas). Es posible crear un diagrama de barras de las variables cos2:\n\nvar &lt;- get_pca_var(iris.acp)\nfviz_cos2(iris.acp, choice = \"var\", axes = 1:2)\n\n\n\n\n\nUn cos2 alto indica una buena representación de la variable en el componente principal. En este caso, la variable se coloca cerca de la circunferencia del círculo de correlación.\nUn cos2 bajo indica que la variable no está perfectamente representada por los PC. En este caso, la variable está cerca del centro del círculo.\n\nPara ver como se relacionan las componentes principales con los datos originales, veamos los autovectores.\n\niris.acp$rotation\n\n                    PC1         PC2        PC3        PC4\nSepal.Length  0.5210659 -0.37741762  0.7195664  0.2612863\nSepal.Width  -0.2693474 -0.92329566 -0.2443818 -0.1235096\nPetal.Length  0.5804131 -0.02449161 -0.1421264 -0.8014492\nPetal.Width   0.5648565 -0.06694199 -0.6342727  0.5235971\n\n\nLa primera componente principal da aproximadamente el mismo peso a la longitud del sépalo, la longitud del pétalo y el ancho del pétalo, pero da peso de signo contrario al ancho del sépalo.\nLa segunda componente principal se refiere principalmente al sépalo.\nEl biplot es un gráfico que permite representar las variables originales y las observaciones transformadas en los ejes de componentes principales.\n\nCada flecha corresponde a una variable.\nNos fijamos primeramente en las direcciones de las flechas y su sentido.\nDos flechas que apunten al mismo lugar indica correlación alta.\nDos flechas con sentidos diferentes pero en la misma dirección indican una correlación negativa.\nCuando dos variables no están correladas en absoluto, se observan dos flechas apuntando en direcciones totalmente perpendiculares.\nEn cuanto a la diferencia en la longitud de las flechas, una menos larga informa que su variable está peor representada que una de largo mayor. Es una forma de medir la calidad de representación.\n\n\nfviz_pca_biplot(iris.acp, repel = TRUE,\n                col.var = \"#2E9FDF\", # color para las variables\n                col.ind = \"#696969\"  # color para las observaciones\n                )\n\n\n\n\nEn el gráfico anterior podemos observar lo siguiente:\n\nTodas las variables originales tienen influencia en las componentes principales (lo cual se evidencia en el tamaño de las flechas).\nLa flecha más larga corresponde al ancho del sépalo, ya que tiene una fuerte influencia (loading) sobre la segunda componente.\nLa dirección de esta última flecha indica que el “loading” del ancho del sépalo para la primera componente es negativo.\nLos “loadings” de las variables longitud del pétalo y ancho del pétalo con respecto a la segunda componente son muy bajos (las flechas son prácticamente horizontales).\nLa variable longitud del sépalo tiene loadings relativamente altos en las dos componentes principales.\n\nAcceso a los resultados del ACP\n\n# Resultados por Variables\nres.var=get_pca_var(iris.acp)\nres.var$contrib        # Contribuciones a las CP \n\n                 Dim.1       Dim.2     Dim.3     Dim.4\nSepal.Length 27.150969 14.24440565 51.777574  6.827052\nSepal.Width   7.254804 85.24748749  5.972245  1.525463\nPetal.Length 33.687936  0.05998389  2.019990 64.232089\nPetal.Width  31.906291  0.44812296 40.230191 27.415396\n\nres.var$cos2           # Calidad de la representación\n\n                 Dim.1       Dim.2       Dim.3        Dim.4\nSepal.Length 0.7924004 0.130198208 0.075987149 0.0014142127\nSepal.Width  0.2117313 0.779188012 0.008764681 0.0003159971\nPetal.Length 0.9831817 0.000548271 0.002964475 0.0133055723\nPetal.Width  0.9311844 0.004095980 0.059040571 0.0056790544\n\n\n\n# Resultados por observaciones\nres.obs=get_pca_ind(iris.acp)\nhead(res.obs$coord,3)  #Coordenadas\n\n      Dim.1      Dim.2      Dim.3      Dim.4\n1 -2.257141 -0.4784238  0.1272796 0.02408751\n2 -2.074013  0.6718827  0.2338255 0.10266284\n3 -2.356335  0.3407664 -0.0440539 0.02828231\n\nhead(res.obs$contrib,3)  #Contribuciones a las CP\n\n     Dim.1      Dim.2       Dim.3      Dim.4\n1 1.163769 0.16694510 0.073591567 0.01867287\n2 0.982590 0.32925696 0.248367113 0.33919842\n3 1.268304 0.08469576 0.008816151 0.02574286\n\nhead(res.obs$cos2,3)  # Calidad de la representación\n\n      Dim.1      Dim.2        Dim.3        Dim.4\n1 0.9539975 0.04286032 0.0030335249 0.0001086460\n2 0.8927725 0.09369248 0.0113475382 0.0021874817\n3 0.9790410 0.02047578 0.0003422122 0.0001410446\n\n\n¿Qué tan bien lo hace el ACP?\n\nlibrary(\"ggfortify\")\nautoplot(iris.acp, data = iris, colour = 'Species',\n         loadings = TRUE, loadings.colour = 'blue',\n         loadings.label = TRUE, loadings.label.size = 3)"
  },
  {
    "objectID": "ACP.html#ejercicio",
    "href": "ACP.html#ejercicio",
    "title": "4  Análisis de Componentes Principales",
    "section": "4.8 Ejercicio:",
    "text": "4.8 Ejercicio:\nConsidera los datos europa.dat que están disponibles en Aula Digital. Los datos corresponden a los porcentajes de población empleados en diferentes actividades económicas en Europa para el año 1979. Las variables consideradas son: Agricultura, Minas, Fábricas, Suministro Eléctrico, Construcción, Industrias de Servicio, Finanzas, Servicios Sociales y Personales y, Transporte y Comunicaciones. Utiliza el método de componentes principales para reducir el número de variables, y tratar de determinar grupos de países con comportamientos semejantes en la distribución de su fuerza de trabajo. En este caso, usa la matriz de covarianza para el cálculo de las componentes principales, ya que todos los datos están medidos en la misma escala (porcentaje de la población) y por las caractersticas de los datos, no parece una buena idea considerarlos todos de igual manera."
  },
  {
    "objectID": "MDS.html#introducción",
    "href": "MDS.html#introducción",
    "title": "5  Escalamiento multidimensional (MDS)",
    "section": "5.1 Introducción",
    "text": "5.1 Introducción\nLas técnicas de escalado multidimensional son una generalización de la idea de componentes principales cuando en lugar de disponer de una matriz de observaciones por variables, como en componentes principales, se dispone de una matriz, \\(\\mathbf{D}\\), cuadrada \\(n \\times n\\) de distancias o disimilaridades entre los \\(n\\) elementos de un conjunto.\nEstas distancias pueden haberse obtenido a partir de ciertas variables, o pueden ser el resultado de una estimación directa, por ejemplo preguntando a un grupo de evaluadores por sus opiniones sobre las similaridades entre los elementos considerados.\nEl objetivo del \\(MDS\\) es representar las distancias observadas mediante unas variables \\(y_1,\\dots, y_k\\), donde \\(k &lt; n\\), tales que las distancias Euclídeas entre las coordenadas de los elementos respecto a estas variables sean iguales (o lo más próximas posibles) a las distancias o disimilaridades de la matriz partida. De esta manera, la representación gráfica en \\(k\\) dimensiones será una reproducción fiel de la estructura observada.\nEl escalado multidimensional comparte con componentes principales el objetivo de describir e interpretar los datos. Si existen muchos elementos, la matriz de similaridades será muy grande y la representación por unas pocas variables de los elementos nos permitirá entender su estructura: qué elementos tienen propiedades similares, si aparecen grupos entre los elementos, si hay elementos atípicos, etc.\nEl escalado multidimensional representa un enfoque complementario a componentes principales en el sentido siguiente.\n\nComponentes principales considera la matriz \\(p \\times p\\) de correlaciones (o covarianzas) entre variables, e investiga su estructura.\nEl escalado multidimensional considera la matriz \\(n \\times n\\) de correlaciones (o covarianzas) entre individuos, e investiga su estructura. Los métodos existentes se dividen en métricos, cuando la matriz inicial es propiamente de distancias, y no métricos, cuando la matriz es de similaridades.\n\nLos métodos métricos, utilizan las diferencias entre similitudes mientras que los no métricos parten de que si \\(A\\) es más similar a \\(B\\) que a \\(C\\), entonces \\(A\\) esta más cerca de \\(B\\) que de \\(C\\), pero las diferencias entre las similitudes \\(AB\\) y \\(AC\\) no tienen interpretación."
  },
  {
    "objectID": "MDS.html#reconstrucción-de-las-variables-a-partir-de-las-distancias-entre-puntos",
    "href": "MDS.html#reconstrucción-de-las-variables-a-partir-de-las-distancias-entre-puntos",
    "title": "5  Escalamiento multidimensional (MDS)",
    "section": "5.2 Reconstrucción de las variables a partir de las distancias entre puntos",
    "text": "5.2 Reconstrucción de las variables a partir de las distancias entre puntos\nDado un conjunto \\(p\\) de variables en \\(n\\) individuos representados en la matriz \\(\\mathbf{X}_{n \\times p}\\), podemos construir dos tipos de matrices cuadradas y semidefinidas positivas: la matriz de covarianzas muestral \\(\\mathbf{S}\\) (definida por \\(\\frac{1}{n}\\mathbf{X}^t \\mathbf{X}\\)), si las variables tienen media cero, y la matriz de productos cruzados \\(\\mathbf{Q} = \\mathbf{XX}^t\\).\nEl \\(MDS\\) puede verse como un análisis de la matriz \\(\\mathbf{Q}\\) y esta puede interpretarse como una matriz de similitudes entre las observaciones ya que sus términos, \\(q_{ij}\\), contienen el producto escalar de las observaciones de dos elementos dados: \\[q_{ij} = \\sum_{p=1}^k x_{ip}x_{jp} = \\mathbf{x}_i^t \\mathbf{x}_j \\;\\; \\;\\;  \\; (1)\\] En efecto, como \\(q_{ij}=|\\mathbf{x}_i||\\mathbf{x}_j|\\cos \\theta_{ij}\\), si los dos elementos tienen coordenadas similares, \\(\\cos \\theta_{ij}\\approx 1\\) y \\(q_{ij}\\) será grande. Por el contrario, si los dos elementos tienen valores distintos \\(\\cos \\theta_{ij}\\approx 0\\) y \\(q_{ij}\\) será pequeño.\nLa distancia Euclídea al cuadrado entre dos elementos se define por:\n\\[d^2_{ij}=\\sum_{p=1}^k (x_{ip}-x_{jp})^2=\\sum_{p}x_{ip}^2+\\sum_{p}x_{jp}^2-2\\sum_{p}x_{ip}x_{jp}\\] y la podemos escribir en función de los términos de la matriz \\(\\mathbf{Q}\\),\n\\[d^2_{ij}=q_{ii}+q_{jj}-2q_{ij} \\;\\; \\;\\;  \\; (2)\\]\nPor tanto, dada la matriz \\(\\mathbf{X}\\) podemos construir la matriz \\(\\mathbf{Q}\\) y a partir de esta matriz es fácil obtener la matriz de distancias al cuadrado con ayuda de las expresiones (1) y (2).\nEl problema que se aborda en MDS es el inverso: dada una matriz de distancias al cuadrado, \\(\\mathbf{D}\\), con elementos \\(d^2_{ij}\\) se trata de reconstruir la matrix \\(\\mathbf{X}\\).\nLo primero que se plantea es obtener la matriz \\(\\mathbf{Q}\\) dada la matriz \\(\\mathbf{D}\\). Para ello, observemos, que sin pérdida de generalidad, siempre podemos suponer que las variables \\(X\\) tienen media cero. En efecto, las distancias entre los puntos, \\(d^2_{ij}\\) no varían si expresamos las variables en desviaciones a la media, ya que:\n\\[d^2_{ij}=\\sum_{p=1}^k (x_{ip}-x_{jp})^2=\\sum_{p}\\left[ (x_{ip}-\\bar{x}_p)-(x_{jp}-\\bar{x}_p) \\right]^2 \\;\\; \\;\\;  \\; (3)\\] y, por tanto, podemos suponer siempre que las variables que buscamos tienen media cero.\nPor ello, como resulta que \\(\\mathbf{X}^t \\mathbf{1} = 0\\) se debe verificar que \\(\\mathbf{Q}\\mathbf{1} = 0\\), es decir, la suma de todos los elementos de una fila de la matriz \\(\\mathbf{Q}\\) (y de una columna ya que la matriz es simétrica) debe de ser cero.\nLuego, sumamos en (2) por filas: \\[\\sum_{i=1}^n d^2_{ij}=\\sum_{i=1}^n q_{ii}+nq_{jj}=t+nq_{jj} \\;\\; \\;\\;  \\; (4)\\] donde \\(t=\\sum_{i=1}^n q_{ii}=traza(\\mathbf{Q})\\), y sabiendo que \\(\\sum_{i=1}^n q_{ij}=0\\).\nSumando (2) por columnas se tiene: \\[\\sum_{j=1}^n d^2_{ij}=n q_{ii}+t \\;\\; \\;\\;  \\; (5)\\] y sumando ahora (4) por filas de nuevo se tiene:\n\\[\\sum_{i=1}^n \\sum_{j=1}^n d^2_{ij}=2nt\\]\nSustituyendo (4) y (5) en (2), tenemos que\n\\[d^2_{ij}=\\frac{1}{n}\\sum_{p=1}^k d^2_{ij} - \\frac{t}{n} + \\frac{1}{n}\\sum_{p=1}^k d^2_{ij} - \\frac{t}{n} -2 q_{ij},  \\] y llamando \\(d^2_{i.}\\) y \\(d^2_{.j}\\) a las medias por filas y por columnas y utilizando (5), tenemos que\n\\[ d^2_{ij}= d^2_{i.}+ d^2_{.j}-d^2_{..}-2 q_{ij} \\;\\; \\;\\;  \\; (6) \\] donde \\(d^2_{..}=\\frac{1}{n^2}\\sum_{i=1}^n \\sum_{j=1}^n d^2_{ij}\\)\nFinalmente, despejando de (6) resulta que \\[q_{ij}=-\\frac{1}{2}\\left( d^2_{ij}-d^2_{i.}-d^2_{.j}+d^2_{..}\\right)\\] expresión que indica cómo construir la matriz \\(\\mathbf{Q}\\) a partir de la matriz de distancias \\(\\mathbf{D}\\).\nDe una manera equivalente, se puede obtener la matriz \\(\\mathbf{X}\\) a partir de la matriz \\(\\mathbf{Q}\\). Si ésta es definida positiva de rango \\(k\\) entonces puede expresarse por \\[\\mathbf{Q}=\\mathbf{V} \\mathbf{\\Lambda}\\mathbf{V}^t\\] donde \\(\\mathbf{V}\\) es una matriz \\(n \\times k\\) y \\(\\mathbf{\\Lambda}\\) es una matriz diagonal \\(k \\times k\\) y contiene los vectores propios no nulos de \\(\\mathbf{Q}\\). Entonces, podemos escribir \\[\\mathbf{Q}=\\left(\\mathbf{V} \\mathbf{\\Lambda}^{1/2} \\right) \\left( \\mathbf{\\Lambda}^{1/2} \\mathbf{V}^t \\right)\\] y tomando\n\\[\\mathbf{Y}=\\mathbf{V}\\mathbf{\\Lambda}^{1/2}\\] se obtienen \\(k\\) variables incorreladas que reproducen la métrica.\nObserva que las variables \\(\\mathbf{Y}\\) no son las variables originales \\(\\mathbf{X}\\), sino sus componentes principales."
  },
  {
    "objectID": "MDS.html#construcción-de-las-coordenadas-principales",
    "href": "MDS.html#construcción-de-las-coordenadas-principales",
    "title": "5  Escalamiento multidimensional (MDS)",
    "section": "5.3 Construcción de las coordenadas principales",
    "text": "5.3 Construcción de las coordenadas principales\nEn general, la matriz de disimilaridades o distancias no será compatible con una métrica Euclídea, pero es frecuente que la matriz \\(\\mathbf{Q}\\) tenga \\(k\\) valores propios positivos y más grandes que el resto.\nSi los restantes \\(n-k\\) valores propios no nulos son mucho menores que los demás, podemos obtener una representación aproximada de los puntos utilizando los \\(k\\) vectores propios asociados a valores propios positivos. En este caso, las representaciones gráficas conservarán aproximadamente la distancia entre los puntos.\nEl procedimiento para obtener las coordenadas principales es el siguiente:\n1.- Formar la matriz de distancias al cuadrado, \\(\\mathbf{D}\\), cuyos elementos son los cuadrados de las distancias.\n2.- Construir la matriz \\(\\mathbf{Q}\\) de productos cruzados.\n3.- Obtener los valores y vectores propios de \\(\\mathbf{Q}\\). Tomar los \\(k\\) mayores si podemos suponer que los restantes \\(n \\times k\\) son próximos a cero.\n4.- Obtener las coordenadas de los puntos en las variables mediante \\(\\sqrt{{\\lambda_i}} \\mathbf{v}_i\\), donde \\(\\mathbf{\\lambda}_i\\) es el valor propio y \\(\\mathbf{v}_i\\) el vector propio.\nEl método puede aplicarse también cuando la matriz de partida \\(\\mathbf{Q}\\) es una matriz de similaridades cualquiera. Entonces \\(q_{ii}=1\\), \\(q_{ij}=q_{ji}\\) y \\(0 \\leq q_{ij} \\leq 1\\). De acuerdo a (2), la matriz de distancias asociadas será: \\[d^2_{ij}= q_{ii}+ q_{jj}-2 q_{ij}=2(1-q_{ij})\\] y puede comprobarse que \\(\\sqrt{2(1-q_{ij})}\\) es una distancia y verifica la desigualdad triangular al corresponder a la distancia euclídea para cierta configuración de puntos.\nSe ha propuesto como medida de la precisión en la aproximación mediante los \\(k\\) valores propios positivos, los coeficientes:\n\\[m_{1,K}=\\frac{\\sum_{i=1}^K |\\lambda_i|}{\\sum_{i=1}^p |\\lambda_i|} \\cdot 100\\] ó\n\\[m_{2,K}=\\frac{\\sum_{i=1}^K \\lambda_i^2}{\\sum_{i=1}^p \\lambda_i^2} \\cdot 100\\]"
  },
  {
    "objectID": "MDS.html#ejemplo-a-mano",
    "href": "MDS.html#ejemplo-a-mano",
    "title": "5  Escalamiento multidimensional (MDS)",
    "section": "5.4 Ejemplo a mano",
    "text": "5.4 Ejemplo a mano\nSobre el siguiente conjunto de animales $={león, jirafa, vaca, oveja, gato, hombre } $ se han medido las siguientes variables binarias:\n\n\\(X_1\\): ¿tiene cola?\n\\(X_2\\): ¿es salvaje?\n\\(X_3\\): ¿tiene el cuello largo?\n\\(X_4\\): ¿es animal de granja?\n\\(X_5\\): ¿es carnívoro?\n\\(X_6\\): ¿camina sobre cuatro patas?\n\nLa matriz de datos es :\n\\[\\mathbf{X}=\\begin{pmatrix}\n1 & 1 & 0 & 0 & 1 & 1\\\\\n1 & 1 & 1 & 0 & 0 & 1\\\\\n1 & 0 & 0 & 1 & 0 & 1\\\\\n1 & 0 & 0 & 1 & 0 & 1\\\\\n1 & 0 & 0 & 0 & 1 & 1\\\\\n0 & 0 & 0 & 0 & 1 & 0\\\\\n\\end{pmatrix}= \\begin{matrix}\nleón\\\\\njirafa \\\\\nvaca \\\\\noveja \\\\\ngato \\\\\nhombre\n\\end{matrix}\\]\nA partir de esta matriz, construye las coordenadas principales y realiza una representación en 2 dimensiones. Calcula una medida de precisión de la representación que has realizado.\nEn este caso tenemos \\(\\mathbf{X}\\), si no fuese el caso habría que usar una medida de similiaridad como por ejemplo la de Sokal y Mikener (Ejercicio)\n\nX=matrix(c(1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,1, 0, 0, 1, 0, 1,1, 0, 0, 1, 0, 1,1, 0, 0, 0, 1, 1,0, 0, 0, 0, 1, 0),\nnrow=6, byrow=TRUE)\nX\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]    1    1    0    0    1    1\n[2,]    1    1    1    0    0    1\n[3,]    1    0    0    1    0    1\n[4,]    1    0    0    1    0    1\n[5,]    1    0    0    0    1    1\n[6,]    0    0    0    0    1    0\n\nQ=X %*% t(X)\nvalores=eigen(Q)$values\nvectores=eigen(Q)$vectors\ny1=sqrt(valores[1])*vectores[,1]\ny2=sqrt(valores[2])*vectores[,2]\n\nCalculamos una medida de precisión (variabilidad explicada) con 2 componentes y miramos la representación\n\n(m_12=(sum(abs(valores[1:2]))/sum(abs(valores)))*100)\n\n[1] 86.20502\n\n(m_22=(sum(valores[1:2]^2)/sum(valores^2))*100)\n\n[1] 97.63187"
  },
  {
    "objectID": "MDS.html#mds-no-métrico",
    "href": "MDS.html#mds-no-métrico",
    "title": "5  Escalamiento multidimensional (MDS)",
    "section": "5.5 MDS no métrico",
    "text": "5.5 MDS no métrico\nPartiendo de la matriz de disimilaridades \\(\\mathbf{\\Lambda}=(\\delta_{ij})\\), el Escalado no métrico consiste en encontrar unas coordenadas, cuyas distancias euclídeas al cuadrado mantengan el órden de las disimilaridades. Es decir, el Escalado no métrico solo tiene en cuenta la información referente al órden entre las disimilaridades, y no su magnitud. El procedimiento es el siguiente:\n\nSe calculan unas coordenadas iniciales \\(Z^{(0)}\\), por ejemplo aplicando MDS métrico a \\(\\mathbf{\\Lambda}\\). Esto es, calcular \\(\\mathbf{Q}\\), realizar la descomposición \\(\\mathbf{Q}=\\mathbf{V} \\mathbf{\\Lambda}\\mathbf{V}^t\\) y tomar \\(Z^{(0)} =\\mathbf{V}_K \\mathbf{\\Lambda}_K^{1/2}\\), donde \\(\\mathbf{\\Lambda}_K\\) contiene los \\(k\\) valores propios mayores y sus \\(\\mathbf{V}_K\\) vectores propios asociados en columnas. Así, obtenemos \\(D^{(0)}=(d^{(0)}_{ij})\\) a partir de las coordenadas \\(Z^{(0)}\\).\nSe calculan disparidades \\(\\hat{d}_{ij}\\) que son una transformación de las distancias \\(d_{ij}\\) que mantienen la misma ordenación que las disimilaridades \\(\\delta_{ij}\\), es decir \\[\\hat{d}_{ij}=f(d_{ij})\\] donde \\(f\\) es una función monótona que verifica: Si \\(\\delta_{ij} \\leq \\delta_{kl}\\), entonces \\(\\hat{d}_{ij} \\leq \\hat{d}_{kl}\\).\n\nEjemplo:: Consideremos la matriz de disimilaridades \\[\\mathbf{\\Lambda}=(\\delta_{ij})=\\begin{pmatrix}\n0 & 2.1 & 3 & 2.4\\\\\n  & 0  &  1.7 & 3.9 \\\\\n  &    &  0  & 3.2 \\\\\n  &    &     & 0\n\\end{pmatrix} \\]\nSupongamos que hemos obtenido una matriz de coordenadas inicial \\(Z^{(0)}\\), cuya matriz de distancias es \\[D^{(0)}=(d^{(0)}_{ij})=\\begin{pmatrix}\n0 & 1.6 & 4.5 & 5.7\\\\\n  & 0  &  3.3 & 4.3 \\\\\n  &    &  0  & 1.3 \\\\\n  &    &     & 0\n\\end{pmatrix}\\]\nObtener las disparidades:\nEscribimos las disimilaridades en órden creciente \\[\\delta_{23}=1.7, \\delta_{12}=2.1, \\delta_{14}=2.4, \\delta_{13}=3, \\delta_{34}=3.2, \\delta_{24}=3.9\\]\nAhora escribimos las distancias correspondientes \\[d_{23}= \\ldots, d_{12}= \\ldots, d_{14}= \\ldots, d_{13}= \\ldots, d_{34}= \\ldots, d_{24}= \\ldots\\]\nSi mantuviesen el mismo órden que las disimilaridades, estarían ordenadas de menor a mayor, y en ese caso, las disparidades serían iguales a las distancias. Entonces \\(Z^{(0)}\\) sería una solución válida.\nUna transformación monótona de estas distancias que preserva el órden de las disimilaridades se calcula de la siguiente forma: cuando existe una secuencia de distancias que están ordenadas al contrario de lo deseado, se reemplazan todas estas distancias por la media de las distancias de dicha secuencia. Así, las disparidades son:\n\\[\\begin{align*}\n\\hat{d}_{23} &= \\hat{d}_{12}=\\frac{1}{2} (d_{23}+d_{12}), \\\\\n\\hat{d}_{14} &= \\hat{d}_{13}=\\hat{d}_{34}=\\frac{1}{3} (d_{14}+d_{13}+d_{34}), \\\\\n\\hat{d}_{24} &= d_{24}\n\\end{align*}\\]\nPara medir la bondad de ajuste de la solución obtenida:\n\nSTRESS: \\(S= \\left[\\frac{\\sum_{i&lt;j} (d_{ij}-\\hat{d}_{ij})^2}{\\sum_{i&lt;j}d^2_{ij}} \\right]^{1/2}\\)\n\n\\(S \\in (0, 0.01] \\implies\\) Solución muy buena;\n\\(S \\in (0.01, 0.05] \\implies\\) Solución buena;\n\\(S \\in (0.05, 0.1] \\implies\\) Solución aceptable.\n\nS-STRESS: \\(S= \\left[\\frac{\\sum_{i&lt;j} (d_{ij}-\\hat{d}_{ij})^2}{\\sum_{i&lt;j}d^4_{ij}} \\right]^{1/2}\\)\n\nEsta medida está entre 0 y 1, siendo valores cercanos a cero indicadores de un buen ajuste, y valores cercanos a 1 indicadores de un mal ajuste.\n\nRSQ: Es el coeficiente de correlación al cuadrado entre las distancias y las disparidades. El ajuste es aceptable para RSQ \\(\\geq 0.6\\)."
  },
  {
    "objectID": "MDS.html#section",
    "href": "MDS.html#section",
    "title": "5  Escalamiento multidimensional (MDS)",
    "section": "5.6 ",
    "text": "5.6 \nSi para las coordenadas actuales, estas medidas no son satisfactorias, entonces pasamos a la búsqueda de una nueva solución. Esta solución se busca minimizando una de las medidas de ajuste respecto a las coordenadas, generalmente se utiliza el STRESS o el S-STRESS.\nMinimización del STRESS\nSea \\(\\mathbf{z}=(\\mathbf{z}^t_1, \\ldots, \\mathbf{z}^t_n)\\) el vector formado por las \\(n\\) filas de \\(\\mathbf(Z)\\) (nuestras incógnitas). El problema es encontrar \\(\\mathbf{z}\\) que minimice \\[S= \\left[\\frac{\\sum_{i&lt;j} (d_{ij}(\\mathbf{z})-\\hat{d}_{ij})^2}{\\sum_{i&lt;j}d^2_{ij}(\\mathbf{z})} \\right]^{1/2}\\] donde \\[d^2_{ij}(\\mathbf{z})=d^2_e(\\mathbf{z}_i,\\mathbf{z}_j)=\\sum_{k=1}^p (z_{ik}-z_{jk})^2.\\] Este problema es no lineal, con lo cual es necesario recurrir a métodos de resolución numéricos."
  },
  {
    "objectID": "MDS.html#ejemplos-en-r",
    "href": "MDS.html#ejemplos-en-r",
    "title": "5  Escalamiento multidimensional (MDS)",
    "section": "5.7 Ejemplos en R",
    "text": "5.7 Ejemplos en R\n\nCon R base:\nhttps://www.statmethods.net/advstats/mds.html\nCon tidyverse\nhttp://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/122-multidimensional-scaling-essentials-algorithms-and-r-code/"
  }
]