[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Análisis de datos",
    "section": "",
    "text": "Presentación\nEsto es una edición en línea de los apuntes de Análisis de Datos.\nEl enfoque es teórico-práctico para el grado de Matemática de la UIB y se puede emplear como un curso previo en una asignatura de Análisis de Datos en grados de informática.\nLa estructura de algunos capítulos será similar a la del libro An Introduction to Applied Multivariate Analysis with R de Everitt P. Horton (véase Horton 2011)\n\n\n\n\nHorton, Everitt P. 2011. An Introduction to Applied Multivariate Analysis with R.",
    "crumbs": [
      "Presentación"
    ]
  },
  {
    "objectID": "t1_intro.html",
    "href": "t1_intro.html",
    "title": "1  Introducción al Análisis de Datos",
    "section": "",
    "text": "1.1 La estadística y el método científico\nEn resumen, la estadística es una herramienta esencial que ayuda a garantizar que la investigación científica sea rigurosa, confiable y basada en evidencia sólida.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción al Análisis de Datos</span>"
    ]
  },
  {
    "objectID": "t1_intro.html#la-estadística-y-el-método-científico",
    "href": "t1_intro.html#la-estadística-y-el-método-científico",
    "title": "1  Introducción al Análisis de Datos",
    "section": "",
    "text": "La ciencia avanza definiendo teorías que intentan explicar el mundo.\nLa comunidad científica elabora teorías/hipótesis que intentan explicar hechos que ocurren. Una hipótesis es científica si existe alguna manera de comprobar su veracidad.\nPodemos diseñar experimentos para comprobar si se cumplen las afirmaciones de la teoría.\nComo la naturaleza tiene un comportamiento con “incertidumbre”, es decir, que si repetimos el experimento se obtienen resultados similares pero no idénticos, la estadística permite analizar estos resultados y ver si las desviaciones de la teoría son razonables o no.\nSe ha definido estadística de muchas maneras. La que más nos gusta, y que está relaciona con la situación que acabamos de explicar, es que:\n\n\nLa estadística es la ciencia que permite adquirir conocimiento generalizable a partir de datos.\n\n\nLa estadística ayuda en todas las fases del método científico:\n\nPlanteamiento del problema: Diseño de experimentos y encuestas, determinación del tamaño de la muestra y métodos de muestreo adecuados para garantizar que los datos recopilados sean representativos de la población objetivo.\nRecopilación de datos: Proporciona herramientas para recopilar y organizar datos relevantes sobre el problema.\nAnálisis de datos: Aplicación de técnicas descriptivas (Análisis explorartorio de datos), así como técnicas inferenciales (contrastes de hipótesis, ajustes de modelos,etc) para sacar conclusiones sobre la población en función de la muestra recopilada.\nInterpretación de resultados: Ayuda a los científicos a determinar si los resultados son estadísticamente significativos y si las conclusiones se pueden generalizar a la población más amplia.\nComunicación de hallazgos: La estadística se usa para comunicar los resultados de manera efectiva a través de gráficos, tablas y tests estadísticos. Esto es esencial para que otros investigadores puedan comprender y evaluar los resultados.\nReproducibilidad: Proporciona métodos estadísticos claros y transparentes, se permite que otros repitan los experimentos y análisis para verificar la validez de los hallazgos.\nToma de decisiones: En muchos campos científicos, los resultados estadísticos se utilizan para tomar decisiones importantes. Por ejemplo, en la medicina, la estadística se usa para evaluar la eficacia de tratamientos y tomar decisiones sobre su uso en la práctica clínica.\n\nCuando alguien realiza un nuevo descubrimiento lo envía a una revisión por pares de la comunidad científica. Para que estos acepten el descubrimiento y pase a formar parte del conocimiento científico debes poner a disposición:\n\nLos datos brutos (raw data) junto con el modelo de datos.\nEl código parametrizado y con las líneas más importantes comentadas.\nLa documentación (artículo/ reporte) donde se interpretan y presentan los resultados más relevantes.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción al Análisis de Datos</span>"
    ]
  },
  {
    "objectID": "t1_intro.html#gestión-básica-de-datos",
    "href": "t1_intro.html#gestión-básica-de-datos",
    "title": "1  Introducción al Análisis de Datos",
    "section": "1.2 Gestión básica de datos",
    "text": "1.2 Gestión básica de datos\n\nEn estadística, siempre se empieza obteniendo unos datos sobre un grupo (relativamente pequeño) de individuos de una población. Bueno, en realidad, no se empieza obteniendo los datos, sino planificando cuidadosamente cómo se van a obtener, pero todo forma parte de la “obtención” de los datos.\nSe generaliza la información que se ha obtenido sobre este grupo de personas al total de la población.\nY no se trata de trucos de magia adivinatoria, sino de una ciencia cuya metodología ha sido validada por medio de demostraciones matemáticas o, en el peor de los casos, mediante simulaciones numéricas (el equivalente en matemáticas de los experimentos en las otras ciencias).\n\nAsí pues, la situación de partida a la hora de aplicar técnicas estadísticas es que disponemos de un conjunto de datos que describen algunas características de un grupo de individuos. El análisis estadístico de estos datos puede ser entonces de dos tipos básicos:\n\nAnálisis exploratorio de datos, cuando nuestro objetivo sea simplemente resumir, representar y explicar los datos concretos de los que disponemos. La estadística descriptiva es el conjunto de técnicas que se usan con este fin.\nAnálisis inferencial, si nuestro objetivo es deducir (inferir), a partir de estos datos, información significativa sobre el total de la población de interés. Las técnicas que se usan en este caso forman la estadística inferencial.\n\n\n\n\n\n\nAmbos tipos de análisis están relacionados. Por un lado, porque es conveniente (obligatorio, en nuestra opinión) empezar cualquier análisis inferencial dando un vistazo a los datos que se usarán.\nPor otro, porque muchas técnicas descriptivas permiten estimar propiedades de la población de la que se ha extraído la muestra. Por citar un ejemplo, la media aritmética de las alturas de un grupo de individuos nos da un valor más o menos representativo de sus alturas, pero también sirve para estimar la altura media de los individuos de la población total.\nLa estadística inferencial entra en juego cuando se quiere obtener información sobre una población y no se puede acceder a todos sus integrantes. Si por ejemplo queremos conocer la altura media de los estudiantes matriculados en esta asignatura de la UIB en este curso, en principio no necesitamos para nada la estadística inferencial. Sois pocos, os mediríamos a todos y calcularíamos la media. En todo caso, usaríamos técnicas de estadística descriptiva para arropar este valor representando la distribución de vuestras alturas de manera adecuada.\nPero si quisiéramos conocer la altura media de los mallorquines entre 18 y 25 años, sería muy complicado medirlos a todos. Entonces, lo que haríamos sería tomar una muestra representativa de esta población, medirlos y a partir de sus alturas estimar dicha altura media. Naturalmente, lo más seguro es que de esta manera no obtuviéramos el valor exacto de la altura media de los mallorquines de 18 años, nos tendríamos que conformar con obtener una aproximación dentro de un cierto margen de error y determinar la probabilidad de acertar con nuestra estimación y este margen de error. La estadística inferencial es la que nos permite acotar el error que podamos haber cometido y calcular la probabilidad de cometerlo, incluyendo la metodología que tendríamos que haber usado para tomar la muestra en primer lugar.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción al Análisis de Datos</span>"
    ]
  },
  {
    "objectID": "t1_intro.html#r-rstudio---posit-rmarkdowm---quarto",
    "href": "t1_intro.html#r-rstudio---posit-rmarkdowm---quarto",
    "title": "1  Introducción al Análisis de Datos",
    "section": "1.3 R/ RStudio - Posit / RMarkdowm - Quarto",
    "text": "1.3 R/ RStudio - Posit / RMarkdowm - Quarto\nTodas las técnicas que usaremos en la asignatura pueden ser implementadas y/o desarrolladas en software libre como Python y R. Ambos se consideran lenguajes de programación esenciales para la ciencia de datos. Lo ideal sería dominar ambos para tener una base de programación completa, pero:\n\nR es un lenguaje específico utilizado para el análisis de datos y la estadística.\nR es muy adecuado para un sub-campo del aprendizaje automático conocido como aprendizaje estadístico. Cualquier persona con una formación formal en estadística debería reconocer la sintaxis y la construcción de R.\nAl igual que Python, R cuenta con una sólida comunidad, estructurada alrededor de la “Comprehensive R Archive Network”, o CRAN, pero no ofrece un desarrollo de software de propósito general como Python.\nCada día salen nuevos paquetes que extienden las funcionalidades de R y cubren casi todas las necesidades computacionales y estadísticas de un científico. Para que os hagáis una idea, en el momento de revisar estas notas (septiembre de 2023) el número de paquetes en el repositorio de la CRAN acaba de superar los 19800.\nEl acceso a R se proporciona a través de RStudio, entorno que presenta una ventana de visualización, un explorador de archivos, un visor de datos y un editor. Este entorno suele ser menos intimidante que el shell de R. Además, cuenta con ayuda integrada, resaltado de sintaxis y completado contextual por tabulaciones; todas estas herramientas facilitan el trabajo.\nRStudio tiene un nuevo nombre desde julio de 2022: Posit. Posit es una palabra que significa proponer una idea para su discusión, proviene de la aspiración científica de construir niveles cada vez mayores de conocimiento y comprensión de experimentos que generan\n\n\n\n\n\n\n\nPosit tiene como misión la creación de software libre y de código abierto para la ciencia de datos, la investigación científica y la comunicación técnica. Han incluido algunas herramientas para Python a través de Quarto.\nQuarto está pensado como un cuaderno de laboratorio moderno donde predomina R pero que soporta código Python (reticulate), SQL, Julia, entre otros; pensado para experimentos que requieren multilenguaje.\nPosit Cloud permite acceder al potente conjunto de herramientas de ciencia de datos de Posit directamente desde su navegador. Esto favorecerá el trabajo en equipo. Podéis revisar la siguiente Guía para crearos una cuenta.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción al Análisis de Datos</span>"
    ]
  },
  {
    "objectID": "t1_intro.html#control-de-versiones-con-git-github",
    "href": "t1_intro.html#control-de-versiones-con-git-github",
    "title": "1  Introducción al Análisis de Datos",
    "section": "1.4 Control de versiones con Git / GitHub",
    "text": "1.4 Control de versiones con Git / GitHub\n\nEn esta asignatura la forma de llegar a un resultado de análisis de datos es tan importante como el propio resultado. Además, uno de los objetivos es exponeros al uso de herramientas de software para la ciencia de datos moderna.\nLa idea de reproducibilidad lleva implícita la colaboración. El código que se produce es parte de la documentación del proceso y es fundamental compartirlo (aunque sólo sea con uno mismo).\nLo anterior se logra mejor con un sistema de control de versiones distribuido como Git. Mantener un registro sobre los proyectos, es lo que permite rastrear y gestionar cambios en el código a lo largo del tiempo. Se puede decir que nos permite guardar el progreso de nuestro código de tal forma que, si en algún momento cometemos algún error irreversible en una versión posterior, siempre podremos recuperar una versión anterior en la que todo funcionaba correctamente y retomar el proyecto desde ese punto.\nGit permite la colaboración, pero carece de características sociales y herramientas específicas para la colaboración en equipo. GitHub proporciona herramientas para la revisión de código, la gestión de problemas y la colaboración en proyectos.\n\n\n\n\n\n\n\nGitHub es un servicio en la nube donde se pueden subir repositorios propios y compartir el código con otras personas de tal forma que sea accesible desde Internet.\nUn repositorio funciona como una carpeta virtual. En él se encuentran todos los archivos de un proyecto y el historial de revisiones de cada uno, permitiendo restablecer una versión del código en caso de error en su ejecución.\nPodemos ver proyectos de otras usuarios, valorarlos, proponer mejoras en el código, GitHub es una de las aplicaciones que mejora la gestión de proyectos y el acceso a recursos compartidos.\nEn octubre del 2021 se estrenó GitHub Copilot, una herramienta de inteligencia artificial en la nube desarrollada conjuntamente entre GitHub y OpenAI. Su objetivo es sugerir y autocompletar el código escrito en entornos de desarrollo integrados (IDE).\n\n\nEn esta clase, utilizaremos GitHub como sistema de gestión del aprendizaje para distribuir y recopilar las entregas como repositorios.\n\n\nCrearemos en GitHub un repositorio por estudiante/equipo para cada entrega. Utilizaremos un sencillo flujo de trabajo centralizado que sólo requiere realizar acciones simples como push, pull, add, rm, commit, status y clone.\n\n\n1.4.1 Git / GitHub con R\nVeamos cómo configurar todo. Gran parte de lo que está aquí proviene del libro Happy Git and GitHub for the useR de Jenny Bryan y del artículo de David Keyes puedes ver sus vídeos en caso de que, la breve explicación que presentamos abajo, no sea suficiente para ti.\n\nInstalar Git: El primer paso es instalar Git, en el Capítulo 6 del libro explican el proceso para los usuarios de Mac, Windows y Linux. Nosotros ya lo tenemos instalado, así que mostramos cómo verificar si tienes Git instalado y su versión usando el terminal en RStudio.\n\nEn el terminal de RStudio:\n\nwhich git # ruta donde está instalado el Git\ngit --version # version\n\n\nConfigurar Git (Editar gitconfig file):El siguiente paso es configurar Git. Esto se trata en el Capítulo 7 del libro, aunque mostramos lo que creemos es un proceso un poco más fácil. Específicamente, sugerimos usar la función edit_git_config() del paquete usethis, que abrirá su archivo gitconfig. Agrega tu nombre y correo electrónico y cierra esto.\n\nEn la consola de RStudio:\n\nlibrary(usethis)\nusethis::edit_git_config()\n# Modificar en el fichero \".gitconfig\" los apartados: \"name\" y \"email\" \n# y guardar el fichero\n\n\nInicializar un repositorio Git: Ahora que has instalado y configurado Git, puedes usarlo localmente. La función use_git() agregará un repositorio Git (a menudo denominado “repositorio”) a un proyecto RStudio existente. Aquí crearemos un nuevo proyecto y luego inicializaremos un repositorio de Git.\n\nEn RStudio: * Crear un proyecto nuevo\n\nSeleccionar “Nuevo Directorio”\nProyecto\n\nActivar: “Create a git repository”\n\n\nEn la consola de RStudio:\n\nlibrary(usethis)\nusethis::use_git()\n# Elegir siempre la opción: 1\n# Y ante la ventana, seleccionar: \"Save\"\n\nY visitar la pestaña: “Git” en RStudio.\n\nVer historial de confirmación: Ahora que tu proyecto de RStudio tiene un repositorio Git asociado, verás una pestaña adicional en la parte superior derecha: la pestaña Git. Desde aquí, puedes ver todo el historial de cambios en tu código a lo largo del tiempo (¡todavía no muchos!).\n\nEn RStudio:\nVisitar la pestaña: “Git” de RStudio Pulsar el icono del reloj para ver el historial de “Commit” realizados para ver el “Initial Commit”.\n\nHacer una confirmación (commit) y ver más historia: Git no realiza un seguimiento automático de los cambios de la manera en que lo hace una herramienta como Google Docs. En su lugar, tienes que decirle a Git: Hice cambios y quiero que mantengas un registro de ellos. Decirle a Git esto se llama hacer una confirmación (commit) y puedes hacerlo desde RStudio.\n\nCada commit tiene un mensaje de confirmación, lo que es útil porque, cuando miras tu historial de código, ves lo que hiciste en cada momento (es decir, en cada commit). RStudio tiene una herramienta integrada para ver su historial de código. Puedes hacer clic en cualquier commit para ver qué cambió, en relación con el commit anterior. Las líneas que se agregaron en verde; y las que se eliminaron en rojo.\nEn RStudio:\n\nCrear un fichero de script R: “test.R” y guardarlo.\nVisita la pestaña “Git” de RStudio y pulsa sobre el botón de “commit” para confirmar la creación del fichero: “test.R”.\nEn el panel del commit añada un texto que lo defina.\nHaz varios cambios en el fichero “test.R” y en cada uno de ellos haz de nuevo un “commit”.\nRevisa luego la historia de los cambios que se han producido en el historial (pulsar el icono del reloj).\nObserva los nuevos cambios resaltados en color verde. Frente a los valores antiguos que aparecerán en color rojo.\n\n\n\n1.4.2 Conectar RStudio y GitHub\nEl proceso hasta ahora nos ha permitido usar Git localmente. Pero, ¿qué pasa si queremos conectarnos a GitHub? ¿Cómo lo hacemos?\nLa mejor manera de conectar RStudio y GitHub es usando tu nombre de usuario y un token de acceso personal (PAT). Para generar un token de acceso personal, usa la función create_github_token() de usethis. Esto te llevará a la página correspondiente en el sitio web de GitHub, donde le darás un nombre a tu token y lo copiarás (¡no lo pierdas porque nunca volverá a aparecer!).\nEn la consola de RStudio:\n\nlibrary(usethis)\nusethis::create_github_token()\n\n\nPulsa sobre el enlace que aparece en la salida en la consola.\nSe abrirá una página web de Github en la que tendrás que pulsar el botón “Generate token”.\nCopia el token que aparece en Github (lo utilizarás en el siguiente paso).\nAhora que has creado un token de acceso personal, debes almacenarlo para que RStudio pueda acceder a él y sepa conectarse a tu cuenta de GitHub. La función gitcreds_set() del paquete gitcreds te ayudará aquí. Ingresará tu nombre de usuario de GitHub y el token de acceso personal como contraseña (NO tu contraseña de GitHub). Una vez que hayas hecho todo esto, ¡habrás conectado RStudio a GitHub!.\n\nEn la consola de RStudio:\n\nlibrary(gitcreds)\ngitcreds::gitcreds_set()\n# Ante la pregunta: \"Enter password or token\"\n# introduce el token copiado en el paso anterior\n\n\n\n1.4.3 Conectar proyectos de RStudio con repositorios de GitHub\nAhora que hemos conectado RStudio y GitHub, discutamos cómo hacer que los dos funcionen juntos. La idea básica es que configures los proyectos que creas en RStudio con repositorios GitHub asociados. Cada proyecto de RStudio vive en un solo repositorio de GitHub.\n¿Cómo conectamos un proyecto de RStudio a un repositorio de GitHub? Happy Git and GitHub for the useR propone tres estrategias. Demostraremos la forma más sencilla\ncrear un repositorio en GitHub primero. Cree el repositorio y, a continuación, cuando inicie un nuevo proyecto en RStudio, utilice la opción de control de versiones, introduzca la URL de su repositorio y listo.\nGitHub primero\nCrea el repositorio en GitHub y, a continuación, cuando inicies un nuevo proyecto en RStudio, utiliza la opción de control de versiones, introduce la URL de tu repositorio y listo.\nPara bajar un repositorio creado en Github a un proyecto local en RStudio, tendréis que realizar los siguientes pasos:\n\nCrear un nuevo repositorio en nuestra cuenta de Github (o utilizar uno ya existente): pulsar el botón “Create repository”.\nCopiar al portapapeles la primera dirección que aparece (pulsando el botón de la derecha). Coincide con la dirección url que aparece en la barra del navegador.\nEn RStudio seleccionamos crear “New project”, elegimos “Version Control” y luego seleccionamos “Git”.\nIntroducimos en el primer cuadro de texto la url copiada anteriormente. Pulsamos “Create Project”.\nA continuación podrás consultarse la pestaña “Git” y ver la información asociada al repositorio descargado.\n\n\n\n1.4.4 Flujo de trabajo general\nAhora que hemos conectado RStudio y GitHub, podemos compartir nuestro trabajo entre los dos.\nPush (Subir a Github)\n“Push” significa enviar cualquier cambio en tu código de RStudio a GitHub. Para hacer esto, primero tenemos que hacer un commit. Después de confirmar, ahora tenemos un botón (la flecha hacia arriba) en RStudio que podemos usar para enviar nuestro código a GitHub.\nEn RStudio:\n\nCreamos un nuevo fichero de script R o un fichero Rmd y lo guardamos.\nPulsamos en la pestaña “Git” sobre el botón de “commit”. Marcamos todos los ficheros sobre los checks de “Staged”, rellenamos la descripción del commit y pulsamos sobre el botón de “commit”.\nDespués de hacer el commit, pulsamos sobre el botón “Push” para subir los cambios a Github.\nA continuación puedes comprobar en la página de Github del repositorio que se han actualizado los últimos ficheros considerados en el último commit.\n\nPull (Descargar desde Github)\nLo opuesto a “empujar”Push” es bajar (“Pull”). Utilizando el botón de flecha hacia abajo, RStudio va al repositorio de GitHub, toma el código más reciente y lo lleva a su editor local.\nHacer “Push” regularmente es extremadamente importante si estás colaborando, aunque si eres el único que trabaja en un proyecto de RStudio y un repositorio GitHub asociado, sabes que tu código local coincide con lo que está en GitHub, por lo que es menos importante.\nEn la página de Github de nuestro repositorio:\n\nEditamos uno de los ficheros de nuestro repositorio pulsando sobre el icono de un lápiz (a la derecha). Realizamos alguna modificación sobre el fichero (o ficheros).\nPulsamos en la parte inferior de la página en el botón de “Commit changes” (rellenando los comentarios que creamos oportunos sobre el commit que se está realizando). Se puede navegar por la página de Github para consultar todos los commits realizados (y mucha más información).\n\nVolvemos a RStudio:\n\nEn la pestaña “Git” pulsamos sobre el botón de la flecha que apunta hacia abajo (verde) para realizar un “Pull” o descarga de los cambios en Github a nuestro proyecto local en RStudio.\nDespués de eso puedes comprobare que los ficheros locales de nuestro proyecto se han actualizado con los cambios que se han producido en el repositorio.\n\n¡Lo lograste!\n¡Ahora está todo configurado para usar Git y GitHub con RStudio!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción al Análisis de Datos</span>"
    ]
  },
  {
    "objectID": "t1_intro.html#los-datos-y-sus-tipos",
    "href": "t1_intro.html#los-datos-y-sus-tipos",
    "title": "1  Introducción al Análisis de Datos",
    "section": "1.5 Los datos y sus tipos",
    "text": "1.5 Los datos y sus tipos\nEn vuestro curso de Estadística estudiasteis algunas técnicas básicas de estadística descriptiva. Estas técnicas consisten en una serie de valores y gráficos que nos permiten resumir y explorar un conjunto de datos, con el objetivo final de entenderlos o describirlos lo mejor posible.\nLos datos de los que disponemos suelen ser multidimensionales, en el sentido de que observamos varias características (variables) de una serie de individuos. Almacenamos estos datos en tablas de datos como la que presentamos abajo, donde cada columna corresponde a una variable y cada fila son los datos de un individuo concreto. Así, en esta tabla, cada fila representa un niño y cada columna recoge una de las características que hemos anotado: su nombre, su altura (en cm), su número de hermanos, el color de sus cabellos, el número semanal de refrescos que suele tomar, y su grado de satisfacción con un juego para móvil (entre 0 y 5).\n\n\n\n\nUna pequeña tabla de datos sobre niños\n\n\n\nNombre\nAltura\nHermanos\nCabello\nRefrescos semanales\nSatisfacción App\n\n\n\n\n1\nMarta\n135\n2\nrubio\n2-3\n4\n\n\n2\nLaura\n132\n1\nnegro\n2-3\n4\n\n\n3\nXavier\n138\n0\nnegro\n0-1\n3\n\n\n4\nJoan\n141\n3\ncastaño\n4-5\n2\n\n\n5\nMaria\n134\n2\nrojo\n0-1\n3\n\n\n6\nMaria\n136\n1\ncastaño\n6 o más\n5\n\n\n\n\n\n\n\n\n\n\n\nEn este curso vamos a “sobrecargar” el término variable, en el sentido de que tendrá dos significados diferentes que esperamos que podáis distinguir según el contexto:\n\nPor un lado, llamaremos variable a una característica que puede tomar diferentes valores sobre diferentes individuos; cuando tenga este sentido, a veces le añadiremos el adjetivo poblacional. Por ejemplo, la altura de las personas (de todo el mundo, de un país, de una ciudad…) es una variable poblacional.\nPor otro lado, también llamaremos una variable a un vector formado por los valores de una variable poblacional sobre los sujetos de una muestra. Por ejemplo, las alturas de los niños recogidas en la tabla forman una variable en este sentido.\n\n\n\nLos tipos básicos de datos que consideramos en este curso son los siguientes:\n\nDatos cualitativos. Son los que expresan una cualidad del individuo, como por ejemplo el sexo cromosómico (macho, hembra), el género de una persona (hombre, mujer, lesbiana, gay, bisexual, transexual, intersexual, asexual), tipos de cáncer (de mama, de colon, de próstata…)… Si solo pueden tomar dos valores (“Sí” o “No”, “Macho” o “Hembra”…) los llamamos binarios o dicotómicos y si pueden tomar más de dos valores, politómicos o multicotómicos, dependiendo de lo que queramos complicar los adjetivos. A los posibles valores que puede tomar un tipo de datos cualitativo se los suele llamar niveles.\nLos datos cualitativos pueden ser iguales o distintos, y no admiten ningún otro tipo de comparación.\nDatos ordinales. Son datos similares a los cualitativos, en el sentido de que expresan una cualidad del individuo, pero con la diferencia de que se pueden ordenar de manera natural. Por ejemplo, los niveles de gravedad de una enfermedad (sano, leve, grave, muy grave, …) o las calificaciones en un examen (suspenso, aprobado, notable, sobresaliente) son datos ordinales. En cambio, no se pueden ordenar de manera significativa los sexos o los tipos de cáncer de los individuos: por eso son datos cualitativos y no ordinales.\nTambién se suele llamar a los posibles valores que puede tomar un tipo de datos ordinal sus niveles.\nDatos cuantitativos. Son datos que se refieren a medidas que sean números genuinos, con los que tenga sentido operar, tales como edades, longitudes, pesos, tiempos, números de individuos, etc. Distinguimos dos tipos:\n\nDiscretos: Pueden tomar solo valores que avanzan a saltos y que podemos identificar con números naturales: número de hermanos, número de ingresos en un día en un hospital…\nContinuos: Podrían tomar cualquier valor real dentro de un intervalo si se pudieran medir con precisión infinita: altura, temperatura, tiempo…\n\n\n\nEn la tabla anterior:\n  \n* La variable \"Nombre\" es cualitativa.\n* La variable \"Altura\" es cuantitativa continua.\n* La variable \"Hermanos\" es cuantitativa discreta.\n* La variable \"Cabello\" es cualitativa.\n* La variable \"Refrescos semanales\" es ordinal.\n* La variable \"Satisfacción App\" también es ordinal.\n\n\nDos puntos relevantes a tener en cuenta y que justifican algunas clasificaciones que puede que encontréis dudosas en el ejemplo anterior:\n\nNo todo número es un dato cuantitativo. Solo los consideramos cuantitativos cuando son números genuinos, “de verdad”. Por ejemplo, si pedimos a un paciente que califique su dolor con un número natural de 0 a 10, no es un dato cuantitativo, sino ordinal:\n\nNo es una medida precisa del dolor; no son números “de verdad”, sino abreviaturas de “Nada”, “Un poquito”,…, “Matadme”.\nTener dolor 6 no significa “tener el doble de dolor” que tener dolor 3 (si lo significara, ¿cuál sería el valor correspondiente “al doble de dolor” que 7?). En cambio, una persona con 6 hermanos sí que tiene el doble de hermanos que si tuviera 3.\nNo tiene sentido sumarlos u operarlos en general. Por ejemplo, si yo tengo dolor de nivel 6 y tú tienes dolor de nivel 5, entre los dos no tenemos dolor de nivel 11. En cambio, si yo tengo 6 hermanos y tú 5, entre los dos sí que tenemos 11 hermanos.\n\nEste es justamente el caso de la variable “Satisfacción App” de la tabla anterior. Pese a que sus valores son números, el único contenido real que tienen es su orden: a la María que toma muchos refrescos le ha gustado la app bastante más que a la María que apenas toma refrescos.\nLa distinción discreto-continuo es puramente teórica. En realidad, todo dato es discreto porque no podemos medir nada con precisión infinita, pero las herramientas matemáticas “continuas” (derivadas, integrales, etc.) son mucho más potentes que las discretas, por lo que siempre que tenga sentido, es conveniente considerar una variable como continua.\nObservad, por ejemplo, la diferencia entre la altura, pongamos que medida en cm y redondeada a unidades como en la tabla anterior, y el número de hermanos. Ambos se presentan como números naturales, pero los números de hermanos no admiten mayor precisión, mientras que las alturas las podríamos medir, con los aparatos adecuados, en mm, en µm, en nm…. Como además las herramientas para tratar datos continuos son mucho más potentes, vamos a considerar las alturas como datos continuos, mientras que los números de hermanos no hay más remedio que tratarlos como discretos.\nEn concreto, es conveniente considerar en la práctica como datos continuos aquellos que dan lugar a números naturales muy grandes, como por ejemplo los números de glóbulos rojos en un litro de sangre, de bases nucléicas en un genoma, o de personas de un país. La diferencia entre diez millones, diez millones uno, diez millones dos… puede considerarse como continua: de hecho, si tomamos el millón como unidad, la diferencia está en la séptima cifra decimal.\n\n\n\n\nHemos dicho que la variable “Cabello” es cualitativa. En principio, el color de los cabellos no tiene ningún orden “natural”. Pero si en un estudio definimos un orden claro para esta variable (por ejemplo, por la longitud de onda correspondiente) y este orden es relevante en nuestro estudio, habrá que considerarla una variable ordinal.\n\n\n\n\n\nLa variable “Refrescos semanales” es de un tipo de datos ordinales muy concreto que a veces se califican de cuantitativos agrupados: sus niveles se obtienen agrupando en intervalos los posibles valores de una variable cuantitativa (en este caso, la variable discreta que mide el número preciso de refrescos semanales).\n\n\n\nEl análisis, tanto descriptivo como inferencial, de un conjunto de datos es diferente según su tipo.\n\nAsí, para datos cualitativos sólo tiene interés estudiar y representar las frecuencias con que aparecen sus diferentes valores, mientras que el análisis de datos cuantitativos suele involucrar el cálculo de medidas estadísticas, como la media o la desviación típica, que expresen numéricamente sus propiedades.\nOs dejamos el material Aprender R1 para que repaséis los capítulos 10 al 14 correspondientes a la parte de Estadística descriptiva.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción al Análisis de Datos</span>"
    ]
  },
  {
    "objectID": "t1_intro.html#práctica-1",
    "href": "t1_intro.html#práctica-1",
    "title": "1  Introducción al Análisis de Datos",
    "section": "1.6 Práctica 1:",
    "text": "1.6 Práctica 1:\n\nFormad grupos de mínimo 2 y máximo 3 integrantes.\nTrabajaréis con datos del repositorio de Datos abiertos del Gobierno de España. Entrad al catálogo de datos y seleccionad un conjunto de datos que os interese. El conjunto de datos debe tener datos cuantitativos y cualtitativos (mejor si contiene datos ordinales), por ejemplo, Alumnado matriculado en Escuelas Oficiales de Idiomas 2023\n\nCread un repositorio en Github para vuestro grupo con un nombre que sea fácilmente identificable, por ejemplo,“Entrega_1_AD”.\nCread un proyecto nuevo en RStudio conectado al repositorio que habéis creado en el paso anterior. Agregad un documento de quarto donde trabajaréis.\nRealizad un análisis exploratorio de los datos, redactad un informe con los hallazgos más importantes y entregadlo en la tarea de Aula Digital disponible. Revisad la fecha en que cierra la tarea.\n\n\nMás adelante, en la asignatura, algunas técnicas que explicaremos se ejemplificarán haciendo uso del conjunto de datos: pingüinos, leed la documentación, instalad y cargad en RStudio la librería palmerpenguins , así como el conjunto de datos penguins\n\n#install.packages(\"palmerpenguins\",dep=TRUE)\nlibrary(\"palmerpenguins\")\nprint(penguins, width = 50)\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;\n 1 Adelie  Torgersen           39.1          18.7\n 2 Adelie  Torgersen           39.5          17.4\n 3 Adelie  Torgersen           40.3          18  \n 4 Adelie  Torgersen           NA            NA  \n 5 Adelie  Torgersen           36.7          19.3\n 6 Adelie  Torgersen           39.3          20.6\n 7 Adelie  Torgersen           38.9          17.8\n 8 Adelie  Torgersen           39.2          19.6\n 9 Adelie  Torgersen           34.1          18.1\n10 Adelie  Torgersen           42            20.2\n# ℹ 334 more rows\n# ℹ 4 more variables: flipper_length_mm &lt;int&gt;,\n#   body_mass_g &lt;int&gt;, sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n\nImagen de Allison Horst\n\n\n\nCon lo que sabéis de R base, realizad un análisis exploratorio de datos y redactad un reporte con los hallazgos más importantes. No olvidéis agregar en el reporte el URL de vuestro repositorio de GitHub.\nEntregad el reporte en la tarea de Aula Digital disponible. Revisad la fecha en que cierra la tarea.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción al Análisis de Datos</span>"
    ]
  },
  {
    "objectID": "t1_intro.html#gramática-limpia-y-coherente-con-tidyverse",
    "href": "t1_intro.html#gramática-limpia-y-coherente-con-tidyverse",
    "title": "1  Introducción al Análisis de Datos",
    "section": "1.7 Gramática limpia y coherente con Tidyverse",
    "text": "1.7 Gramática limpia y coherente con Tidyverse\n\n1.7.1 La librería Tidyverse\n\n\n\n\n\n\nTidyverse es una colección de paquetes/librerías de R para ciencia de datos que comparten una filosofía de diseño, gramática y estructuras similar. En la página Tidverse.org podéis encontrar una descripción detallada de cada una de las librerías, un Blog con artículos de interés para la ciencia de datos, ayuda y recursos de aprendizaje.\nTodos estos paquetes están pensados para:\n\nTener una tecnología con la que puedan convivir diferentes tipos de profesionales (como por ejemplo: informáticos, economistas, matemáticos, gestores) compartiendo el mismo flujo de datos.\nFacilitar el análisis y modelización de datos\n\n\n\n\n\n\n\n\nHadley Wickham, su creador, es el director de los científicos de datos de RStudio (actual Posit) y profesor adjunto de estadística en la Universidad de Auckland, la Universidad de Stanford y la Universidad de Rice.\nLas librerías de tidyverse han venido a sustituir R base por su eficiencia y facilidad de programación para no informáticos. Casi todas las consultas a páginas técnicas de R son o incluyen código de tidyverse.\nPaquetes del core de tidyverse:\n\nggplot2: Permite crear gráficos de forma declarativa. Le introducimos los datos, le decimos cómo asignar variables a la estética, qué tipo de gráfico utilizar, y ggplot2 nos devolverá un gráfico más “elegante” y fácil de editar que los de R base.\ndplyr: Gramática de manipulación de datos, un conjunto coherente de acciones que resuelven los retos más comunes como juntar datos y transformarlos.\ntidyr: Conjunto de funciones que ayudan a obtener datos ordenados. Los datos ordenados son datos con una forma consistente: en resumen, cada variable va en una columna, y cada fila es una unidad muestral.\nreadr: Proporciona una forma rápida y amigable de leer datos rectangulares (como csv, tsv).\npurrr: Mejora el conjunto de herramientas de programación funcional (PF) de R proporcionando un conjunto completo y coherente de herramientas para trabajar con funciones y vectores. Una vez dominados los conceptos básicos, purrr permite sustituir muchos bucles for por código más fácil de escribir y más expresivo.\ntibble: Un formato más moderno que el data frame, manteniendo lo que en el tiempo ha demostrado ser eficaz, y desechando lo que no.\nstringr: Conjunto cohesivo de funciones diseñadas para hacer el trabajo con cadenas de texto lo más fácil posible.\nforcats: Conjunto de herramientas útiles que resuelven problemas comunes con factores. R utiliza factores para manejar variables categóricas, variables que tienen un conjunto fijo y conocido de posibles valores.\npurrr: programación funcional (pipes)\n\nHay muchos otros paquetes que se integran sin problemas, por ejemplo, lubridate (para manejar datos tomados en el tiempo), stringr (texto), forcats (factores), etc.\nPara instalar y cargar tidyverse\n\n\n#install.packages(\"tidyverse\")\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter()     masks stats::filter()\n✖ dplyr::group_rows() masks kableExtra::group_rows()\n✖ dplyr::lag()        masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nSe puede ver la versión del paquete tidyverse y la de los paquetes base.\nCuidado: algunas funciones de R se sobrescriben por sus equivalentes de tidyverse. En ocasiones es preferible indicar explícitamente el nombre de la función que deseamos utilizar, por ejemplo: dplyr::group_by para distinguir de plyr::group_by (dplyr es una evolución del paquete plyr).\n\n1.7.1.1 ¿Qué significa tidy data?\n\n\n\n\n\n\nDecimos que unos datos están bien estructurados o son “tidy data” si se cumplen los siguientes principios:\n\nCada variable forma una columna.\nCada observación forma una fila.\nCada tipo de unidad de observación forma una tabla.\n\n\n\n\n\nImagen de Allison Horst\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlgunas formas de violar los principios de los datos ordenados son:\n\nLas cabeceras de las columnas son valores, no nombres de variables.\nSe almacenan múltiples variables en una columna.\nLas variables se almacenan tanto en filas como en columnas.\nSe almacenan múltiples tipos de unidades de observación en la misma tabla.\nUna misma unidad de observación se almacena en varias tablas.\n\n\nVeamos ejemplos de datos “No tidy” creados a partir del conjunto de datos con el que venimos trabajando de los pingüinos.\nEjemplo 1:\n\n\n# A tibble: 3 × 4\n  species   Biscoe Dream Torgersen\n  &lt;fct&gt;      &lt;int&gt; &lt;int&gt;     &lt;int&gt;\n1 Adelie        44    56        52\n2 Chinstrap     NA    68        NA\n3 Gentoo       124    NA        NA\n\n\nEjemplo 2:\n\n\n# A tibble: 5 × 3\n  col            island     year\n  &lt;chr&gt;          &lt;fct&gt;     &lt;int&gt;\n1 Gentoo_NA      Biscoe     2007\n2 Adelie_male    Torgersen  2007\n3 Gentoo_female  Biscoe     2008\n4 Chinstrap_male Dream      2008\n5 Adelie_male    Torgersen  2009\n\n\nEjemplo 3:\n\n\n# A tibble: 3 × 4\n  term              bill_length_mm bill_depth_mm flipper_length_mm\n  &lt;chr&gt;                      &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;\n1 bill_length_mm            NA            -0.235             0.656\n2 bill_depth_mm             -0.235        NA                -0.584\n3 flipper_length_mm          0.656        -0.584            NA    \n\n\nEjemplo 4:\n\n\n# A tibble: 6 × 6\n  species   island sex    model              mpg   cyl\n  &lt;fct&gt;     &lt;fct&gt;  &lt;fct&gt;  &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;\n1 Chinstrap Dream  female &lt;NA&gt;              NA      NA\n2 Gentoo    Biscoe female &lt;NA&gt;              NA      NA\n3 Gentoo    Biscoe male   &lt;NA&gt;              NA      NA\n4 &lt;NA&gt;      &lt;NA&gt;   &lt;NA&gt;   Merc 450SLC       15.2     8\n5 &lt;NA&gt;      &lt;NA&gt;   &lt;NA&gt;   Dodge Challenger  15.5     8\n6 &lt;NA&gt;      &lt;NA&gt;   &lt;NA&gt;   Pontiac Firebird  19.2     8\n\n\n\nSi tenemos datos provenientes de distintas fuentes, seguramente tendremos que limpiarlos y juntarlos en un único tibble.\n\n\n\n\n1.7.2 El operador de tuberías (pipe) %&gt;%\n\nLos pipes básicos pasan un valor, atributo u objeto (LHS: Left Hand Side) a la siguiente llamada de función (RHS: Right Hand Side) como primer argumento\n\n\nx %&gt;% f # equivalente a: f(x)\nx %&gt;% f(y) # equivalente a: f(x, y)\nx %&gt;% f %&gt;% g %&gt;% h # equivalente a: h(g(f(x)))\n\n\nLos pipes también se usan con marcadores de posición; en este caso, reenvian un valor u objeto (LHS) a la siguiente llamada de función (RHS) como cualquier argumento.\n\n\nx %&gt;% f(.) # equivalente a: x %&gt;% f\nx %&gt;% f(y, .) # equivalente a: f(y, x)\nx %&gt;% f(y, z = .) # equivalente a: f(y, z = x)\nx %&gt;% f(y = nrow(.),\n        z = ncol(.))  # equivalente a: f(x, y = nrow(x), z = ncol(x))\n\n\nUna secuencia de código que comienza con el marcador de posición (.) devuelve una función que puede utilizarse para aplicar posteriormente la tubería a valores concretos.\n\n\nf &lt;- . %&gt;% cos %&gt;% sin # equivalente a: f &lt;- function(.) sin(cos(.))\n\n\nf(20) # equivalente a: la tubería 20 %&gt;% cos %&gt;% sin\n\n\nPara saber más sobre %&gt;%, haced vignette(\"magrittr\") en la consola de R.\nSe puede obtener %&gt;% en Rstudio desktop utilizando el atajo de teclado: Ctrl-Shift-MCtrl-Shift-M.\nEjemplo: ¿Cuál es la masa corporal media, en gramos, de los pingüinos estudiados durante el año 2007?\n\n\n# Sin pipes\nmean(subset(penguins, year == 2007)$body_mass_g, na.rm = T)\n\n[1] 4124.541\n\n# Con pipes (tidyverse)\nresultado &lt;- penguins %&gt;% \n  subset(year == 2007) %&gt;% \n  .$body_mass_g %&gt;% \n  mean(na.rm = T)\n\nLa respuesta a la pregunta formulada sería: “La masa corporal media, en gramos, de los pingüinos estudiados durante el año 2007 es 4125 gramos”.\n\nVentajas de usar pipes:\n\nEl estilo secuencial de las tuberías mejora la lectura del código en comparación con las funciones anidadas.\nHace innecesario almacenar los resultados intermedios.\nEs muy fácil añadir o eliminar pasos (empalmes de tuberías) individuales en el “pipeline”.\n\nLas versiones recientes de R también tienen un operador de tuberías nativo (|&gt;).\n\n\nmtcars |&gt; head(2) #  es lo mismo que  head(mtcars, 2)\n\n              mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4      21   6  160 110  3.9 2.620 16.46  0  1    4    4\nMazda RX4 Wag  21   6  160 110  3.9 2.875 17.02  0  1    4    4\n\nmtcars |&gt; subset(cyl == 4) |&gt; nrow()  \n\n[1] 11\n\n\n\n\n1.7.3 Data frames avanzados tibbels\nEL paquete tibble proporciona un objeto de tipo data frame mejorado: tbl_df. Un tibble se puede crear de cuatro maneras diferentes.\n\nA partir de vectores columna:\n\n\ntibble(\n  x = c(\"a\", \"b\"),\n  y = c(1, 2),\n  z = c(T, F)\n)\n\n# A tibble: 2 × 3\n  x         y z    \n  &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt;\n1 a         1 TRUE \n2 b         2 FALSE\n\n\n\nEscribiendo en texto por columnas:\n\n\ntribble(\n  ~x, ~y, ~z,\n  \"a\", 1, T,\n  \"b\", 2, F\n)\n\n# A tibble: 2 × 3\n  x         y z    \n  &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt;\n1 a         1 TRUE \n2 b         2 FALSE\n\n\n\nCreando un tibble a partir de otro objeto de las clases matrix o data.frame:\n\n\ndata.frame(\n  x = c(\"a\", \"b\"),\n  y = c(1, 2),\n  z = c(T, F)\n) %&gt;% \nas_tibble\n\n# A tibble: 2 × 3\n  x         y z    \n  &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt;\n1 a         1 TRUE \n2 b         2 FALSE\n\n\n\nCreando un tibble a partir de vectores con nombre:\n\n\nc(x = \"a\", y = \"b\", z = 1) %&gt;%\n  enframe(name = \"x\", value = \"y\")\n\n# A tibble: 3 × 2\n  x     y    \n  &lt;chr&gt; &lt;chr&gt;\n1 x     a    \n2 y     b    \n3 z     1    \n\n\n\nDiferencias entre tibble y data.frame\n\nUn tibble nunca cambia el tipo de entrada. Ya no hay que preocuparse de que los caracteres se conviertan automáticamente en cadenas.\nUn tibble puede tener columnas que son listas.\nUn tibble puede tener nombres de variables no estándar. Pueden empezar por un número o contener espacios. Para utilizarlo se refiere a estos en un backtick, por ejemplo, peso en Kg.\nSólo recicla vectores de longitud 1.\nNo tiene como atributo nombres de filas row.names.\nPor defecto, tibble() imprime sólo las diez primeras filas, todas las columnas que caben en la pantalla, y las clases de las columnas\n\n\n\npenguins\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n# data.frame(penguins) recordad que esto imprime todo el data frame\n\n\nglimpse nos da la versión transpuesta de print()\n\n\npenguins %&gt;% glimpse\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n\nEl subconjunto de un tibble ([]) siempre devuelve otro tibble y nunca un vector (en contraste con los objetos de un data.frame).\n\n\ndata.frame(penguins) %&gt;% .[, \"species\"] %&gt;% class\n\n[1] \"factor\"\n\n\n\npenguins[, \"species\"] %&gt;% class\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\n\nEl subconjunto de un data.frame busca el nombre de la variable más parecida\n\n\nnames(data.frame(penguins))\n\n[1] \"species\"           \"island\"            \"bill_length_mm\"   \n[4] \"bill_depth_mm\"     \"flipper_length_mm\" \"body_mass_g\"      \n[7] \"sex\"               \"year\"             \n\nhead(data.frame(penguins)$spec)\n\n[1] Adelie Adelie Adelie Adelie Adelie Adelie\nLevels: Adelie Chinstrap Gentoo\n\n\n\ntibble no permite la coincidencia parcial, es decir, siempre se debe proporcionar el nombre completo de la columna.\n\n\nhead(penguins$spec)\n\nWarning: Unknown or uninitialised column: `spec`.\n\n\nNULL\n\n\n\nhead(penguins$species)\n\n[1] Adelie Adelie Adelie Adelie Adelie Adelie\nLevels: Adelie Chinstrap Gentoo\n\n\n\nLas tibbles dan mejores mensajes Warnings y Errors para solucionar problemas.\n\n\n\n1.7.4 Lectura de datos de texto rectangulares readr\nEl paquete readr proporciona funciones de lectura y escritura para múltiples formatos de archivo:\n\nread_delim(): archivos delimitados en general\nread_csv(): archivos separados por comas\nread_csv2(): archivos separados por punto y coma. En la mayoría de los países europeos, Microsoft Excel utiliza ; como delimitador común\nread_tsv(): archivos separados por tabulaciones\nread_fwf(): archivos de ancho fijo\nread_table(): archivos separados por espacios en blanco\nread_log(): archivos de registro web\nConvenientemente, las funciones write_*() funcionan de forma análoga.\nSe utiliza el paquete readxl para archivos de Excel,\nEl paquete haven para archivos de Stata, SAS y SPSS,\nEl paquete googlesheets4 para Google Sheets\nEl paquete rvest para archivos HTML. Esta es la librería de referencia en el contexto de la extracción de datos de la web con R\n\nPara ilustrar el paquete readr, hemos creado previamente un archivo csv que contiene los datos de los pingüinos, utilizando write_csv(penguins, archivo = \"datos/penguins.csv\").\n\ndata &lt;- read_csv(file = \"./datos/penguins.csv\")\n\nNew names:\nRows: 344 Columns: 9\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(3): species, island, sex dbl (6): ...1, bill_length_mm, bill_depth_mm,\nflipper_length_mm, body_mass_g...\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n\n\ndata &lt;- read_csv(file = \"./datos/penguins.csv\", col_select = c(species, island))\n\nNew names:\nRows: 344 Columns: 2\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(2): species, island\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n\n\ndata &lt;- read_csv(file = \"./datos/penguins.csv\",\n                 col_names = paste(\"Var\", 1:8, sep = \"_\"))\n\nRows: 345 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): Var_2, Var_3, Var_4, Var_5, Var_6, Var_7, Var_8, X9\ndbl (1): Var_1\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\ndata &lt;- read_csv(file = \"./datos/penguins.csv\", skip = 5)\n\nRows: 339 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Adelie, Torgersen, female\ndbl (6): 5, 36.7, 19.3, 193, 3450, 2007\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nObserva que la salida de cualquier función read_*() es un objeto tibble.\nreadr imprime las especificaciones de las columnas después de la importación.\nPor defecto, readr intenta inferir el tipo de columna (por ejemplo, int, dbl, chr, fct, date, lgl) a partir de las primeras 1.000 filas y analiza las columnas en consecuencia.\nUna buena práctica es especificar de forma explícita el formato de las columnas.\n\n\nread_csv(\n  archivo = \"./datos/penguins.csv\",\n  col_types = cols(\n    species = col_character(),\n    año = col_datetime(formato = \"%Y\"),\n    isla = col_skip())\n  )\n\n\nAnalizar sólo las primeras 1.000 filas es eficiente, pero puede llevar a conjeturas erróneas:\n\n\nread_csv(file = \"./datos/penguins.csv\", guess_max = 2000)\n\n\nEncuentra más información y funciones de readr en la hoja de trucos.\nA veces puedes tener problemas al leer datos de texto (tipo carácter): los signos especiales como ö, ä o ü pueden ser codificados de forma extraña como símbolos. En esos casos debes controlar la codificación los datos en la función read_csv (por ejemplo, UTF-8)\nSupongamos que deseamos dejar de utilizar los archivos .xlsx y .csv ya que no son capaces de almacenar de forma fiable los metadatos (por ejemplo, los tipos de datos).\nLas funciones write_rds() y read_rds() proporcionan una buena alternativa para serializar tus objetos de R (por ejemplo, tibbles, modelos) y almacenarlos como archivos .rds.\nMás info sobre archivos rds\n\n\npenguins %&gt;% \n  write_rds(file = \"./datos/penguins.rds\")\n\n\npenguins &lt;- read_rds(file = \"./datos/penguins.rds\")\n\nNota que:\n\nwrite_rds() solo puede utilizarse para guardar un objeto a la vez,\nun archivo .rds cargado debe ser almacenado en una nueva variable, es decir, darle un nuevo nombre,\nread_rds() ¡conserva los tipos de datos!\n\n\n\n1.7.5 Ordenando datos tidyr\n\nPodemos cambiar el formato de una tabla de datos con las funciones: pivot_longer() y pivot_wider() tal como se muestra en la siguiente figura.\n\n\n\n\n\n\n\nEjemplo: Tomamos la tabla de datos no tidy del Ejemplo 1 de estas notas y tratamos de estructurarla correctamente. La tabla la hemos llamado ejemplo1 y os la presentamos a continuación:\n\n\n\n# A tibble: 3 × 4\n  species   Biscoe Dream Torgersen\n  &lt;fct&gt;      &lt;int&gt; &lt;int&gt;     &lt;int&gt;\n1 Adelie        44    56        52\n2 Chinstrap     NA    68        NA\n3 Gentoo       124    NA        NA\n\n\nVamos a aplicar la función pivot_longer para hacerla tidy:\n\nnueva &lt;- ejemplo1 %&gt;% \n  pivot_longer(\n    cols = c(Biscoe, Dream, Torgersen),\n    names_to = \"Isla\", values_to = \"Frecuencia\"\n  )\n\nnueva\n\n# A tibble: 9 × 3\n  species   Isla      Frecuencia\n  &lt;fct&gt;     &lt;chr&gt;          &lt;int&gt;\n1 Adelie    Biscoe            44\n2 Adelie    Dream             56\n3 Adelie    Torgersen         52\n4 Chinstrap Biscoe            NA\n5 Chinstrap Dream             68\n6 Chinstrap Torgersen         NA\n7 Gentoo    Biscoe           124\n8 Gentoo    Dream             NA\n9 Gentoo    Torgersen         NA\n\n\n\nLa función pivot_wider() invierte el efecto de pivot_longer(). Lo dejamos como Ejercicio\nPuedes encontrar más información acerca de pivot_*() en tidyr.\nOtra cosa que podemos hacer con tidyr es “agrupar” datos de manera que cada grupo se convierte en una sola fila en un data frame. La función nest() genera datos anidados en un data frame con una fila por species y year.\n\n\nnested_penguins &lt;- penguins %&gt;% \n    nest(nested_data = \n           c(island, bill_length_mm, \n             bill_depth_mm,flipper_length_mm,\n             body_mass_g, sex))\nnested_penguins\n\n# A tibble: 9 × 3\n  species    year nested_data      \n  &lt;fct&gt;     &lt;int&gt; &lt;list&gt;           \n1 Adelie     2007 &lt;tibble [50 × 6]&gt;\n2 Adelie     2008 &lt;tibble [50 × 6]&gt;\n3 Adelie     2009 &lt;tibble [52 × 6]&gt;\n4 Gentoo     2007 &lt;tibble [34 × 6]&gt;\n5 Gentoo     2008 &lt;tibble [46 × 6]&gt;\n6 Gentoo     2009 &lt;tibble [44 × 6]&gt;\n7 Chinstrap  2007 &lt;tibble [26 × 6]&gt;\n8 Chinstrap  2008 &lt;tibble [18 × 6]&gt;\n9 Chinstrap  2009 &lt;tibble [24 × 6]&gt;\n\n\n\nLos datos anidados nested_penguins contienen tibbles con seis columnas cada uno y un número de observaciones que pueden ser distintos.\nLos datos anidados son útiles si queremos aplicar funciones a cada subgrupo de datos (por ejemplo, comparar estadísticos por especie).\nPara deshacer las estructuras de datos anidados se puede usar la función tidyr::unnest().\n\n\nnested_penguins %&gt;% unnest(cols = c(nested_data)) \n\n# A tibble: 344 × 8\n   species  year island    bill_length_mm bill_depth_mm flipper_length_mm\n   &lt;fct&gt;   &lt;int&gt; &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;\n 1 Adelie   2007 Torgersen           39.1          18.7               181\n 2 Adelie   2007 Torgersen           39.5          17.4               186\n 3 Adelie   2007 Torgersen           40.3          18                 195\n 4 Adelie   2007 Torgersen           NA            NA                  NA\n 5 Adelie   2007 Torgersen           36.7          19.3               193\n 6 Adelie   2007 Torgersen           39.3          20.6               190\n 7 Adelie   2007 Torgersen           38.9          17.8               181\n 8 Adelie   2007 Torgersen           39.2          19.6               195\n 9 Adelie   2007 Torgersen           34.1          18.1               193\n10 Adelie   2007 Torgersen           42            20.2               190\n# ℹ 334 more rows\n# ℹ 2 more variables: body_mass_g &lt;int&gt;, sex &lt;fct&gt;\n\n\n\nEjemplo: Dividir y combinar múltiples columnas en una sola.\n\n\npenguins %&gt;% unite(col = \"specie_sex\",\n                   c(species, sex), sep = \"_\", remove = T)\n\n# A tibble: 344 × 7\n   specie_sex  island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;       &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie_male Torge…           39.1          18.7               181        3750\n 2 Adelie_fem… Torge…           39.5          17.4               186        3800\n 3 Adelie_fem… Torge…           40.3          18                 195        3250\n 4 Adelie_NA   Torge…           NA            NA                  NA          NA\n 5 Adelie_fem… Torge…           36.7          19.3               193        3450\n 6 Adelie_male Torge…           39.3          20.6               190        3650\n 7 Adelie_fem… Torge…           38.9          17.8               181        3625\n 8 Adelie_male Torge…           39.2          19.6               195        4675\n 9 Adelie_NA   Torge…           34.1          18.1               193        3475\n10 Adelie_NA   Torge…           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 1 more variable: year &lt;int&gt;\n\n\n\nEjemplo: Separar una sola columna, que contiene varios valores, en varias columnas.\n\n\npenguins %&gt;% separate(bill_length_mm, sep = 2, into = c(\"cm\", \"mm\"))\n\n# A tibble: 344 × 9\n   species island  cm    mm    bill_depth_mm flipper_length_mm body_mass_g sex  \n   &lt;fct&gt;   &lt;fct&gt;   &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt;\n 1 Adelie  Torger… 39    \".1\"           18.7               181        3750 male \n 2 Adelie  Torger… 39    \".5\"           17.4               186        3800 fema…\n 3 Adelie  Torger… 40    \".3\"           18                 195        3250 fema…\n 4 Adelie  Torger… &lt;NA&gt;  &lt;NA&gt;           NA                  NA          NA &lt;NA&gt; \n 5 Adelie  Torger… 36    \".7\"           19.3               193        3450 fema…\n 6 Adelie  Torger… 39    \".3\"           20.6               190        3650 male \n 7 Adelie  Torger… 38    \".9\"           17.8               181        3625 fema…\n 8 Adelie  Torger… 39    \".2\"           19.6               195        4675 male \n 9 Adelie  Torger… 34    \".1\"           18.1               193        3475 &lt;NA&gt; \n10 Adelie  Torger… 42    \"\"             20.2               190        4250 &lt;NA&gt; \n# ℹ 334 more rows\n# ℹ 1 more variable: year &lt;int&gt;\n\n\n\nEjemplo: Separar una sola columna, que contiene varios valores, en varias filas.\n\n\npenguins %&gt;% separate_rows(island, sep = \"s\", convert = T)\n\n# A tibble: 564 × 8\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;chr&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torger           39.1          18.7               181        3750\n 2 Adelie  en               39.1          18.7               181        3750\n 3 Adelie  Torger           39.5          17.4               186        3800\n 4 Adelie  en               39.5          17.4               186        3800\n 5 Adelie  Torger           40.3          18                 195        3250\n 6 Adelie  en               40.3          18                 195        3250\n 7 Adelie  Torger           NA            NA                  NA          NA\n 8 Adelie  en               NA            NA                  NA          NA\n 9 Adelie  Torger           36.7          19.3               193        3450\n10 Adelie  en               36.7          19.3               193        3450\n# ℹ 554 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n1.7.6 Manejo de NAs\nSupongamos que tenemos una tibble que llamamos incompl_penguins y queremos hacer explícitos los valores perdidos porque la tabla luce de la siguiente forma:\n\n\n# A tibble: 4 × 3\n  species    year measurement\n  &lt;chr&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 Adelie     2007        41.6\n2 Adelie     2008        46.5\n3 Gentoo     2008        73.4\n4 Chinstrap  2007        NA  \n\n\nPara hacer explícitos los NAs, ejecutamos las siguientes instrucciones:\n\nincompl_penguins %&gt;% \n  complete(species, year, fill = list(measurement = NA))\n\n# A tibble: 6 × 3\n  species    year measurement\n  &lt;chr&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 Adelie     2007        41.6\n2 Adelie     2008        46.5\n3 Chinstrap  2007        NA  \n4 Chinstrap  2008        NA  \n5 Gentoo     2007        NA  \n6 Gentoo     2008        73.4\n\n\nSi, por el contrario, queremos hacer implícitos los valores perdidos, ejecutamos la instrucción:\n\nincompl_penguins %&gt;% \n  drop_na(measurement)\n\n# A tibble: 3 × 3\n  species  year measurement\n  &lt;chr&gt;   &lt;dbl&gt;       &lt;dbl&gt;\n1 Adelie   2007        41.6\n2 Adelie   2008        46.5\n3 Gentoo   2008        73.4\n\n\n\nPara reemplazar los valores faltantes por la entrada siguiente:\n\n\nincompl_penguins %&gt;% \n  fill(measurement, .direction = \"down\")\n\n# A tibble: 4 × 3\n  species    year measurement\n  &lt;chr&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 Adelie     2007        41.6\n2 Adelie     2008        46.5\n3 Gentoo     2008        73.4\n4 Chinstrap  2007        73.4\n\n\n\nReemplazar los valores que faltan por un valor predefinido.\n\n\nincompl_penguins %&gt;%\n  replace_na(replace = list(measurement = mean(.$measurement, na.rm = T)))\n\n# A tibble: 4 × 3\n  species    year measurement\n  &lt;chr&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 Adelie     2007        41.6\n2 Adelie     2008        46.5\n3 Gentoo     2008        73.4\n4 Chinstrap  2007        53.8\n\n\n\nMás información en la Hoja de trucos de tidyr.\nRecordad: Los argumentos de una función precedidos por un punto en el tidyverse pueden deberse a dos razones:\n\nLa función es todavía una versión no acabada, es decir, los desarrolladores aún piensan en la mejor manera de implementar y nombrar la función.\nLa función se aplica regularmente dentro de otra función para no confundir los argumentos de la función interna y la externa.\n\n\n\n\n\n1.7.7 Manipulación de datos dplyr\ndplyr proporciona un conjunto de funciones para manipular objetos tibbles. Las funciones están representadas por “acciones” que reflejan las operaciones subyacentes y siempre dan como resultado un tibble nuevo o modificado.\n\n1.7.7.1 Operaciones por filas\n\nfilter() selecciona las filas que cumplen uno o varios criterios lógicos\nslice() selecciona las filas en función de su ubicación en los datos\narrange() cambia el orden de las filas\n\nEjemplo: Seleccionar un data frame con los pingüinos de la especie “Adelie”.\n\npenguins %&gt;% \n  filter(species == \"Adelie\")\n\n# A tibble: 152 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 142 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nEjemplo: Encontrar los pingüinos con NAs en la variable bill_length_mm.\n\npenguins %&gt;% \n  filter(is.na(bill_length_mm) == T)\n\n# A tibble: 2 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen             NA            NA                NA          NA\n2 Gentoo  Biscoe                NA            NA                NA          NA\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nEjemplo: Seleccionar los pingüinos observados antes del año 2008 o después del año 2008 y que tienen masa corporal entre 3800 y 4000 gramos.\n\npenguins %&gt;% \n  filter(between(body_mass_g, 3800, 4000) & (year &lt; 2008 | year &gt; 2008))\n\n# A tibble: 28 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.5          17.4               186        3800\n 2 Adelie  Torgersen           38.6          21.2               191        3800\n 3 Adelie  Biscoe              35.9          19.2               189        3800\n 4 Adelie  Biscoe              38.2          18.1               185        3950\n 5 Adelie  Biscoe              38.8          17.2               180        3800\n 6 Adelie  Biscoe              35.3          18.9               187        3800\n 7 Adelie  Biscoe              40.5          18.9               180        3950\n 8 Adelie  Dream               37.2          18.1               178        3900\n 9 Adelie  Dream               40.9          18.9               184        3900\n10 Adelie  Dream               38.8          20                 190        3950\n# ℹ 18 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nEjemplo: Seleccionar los pingüinos de acuerdo a su ubicación.\n\npenguins %&gt;% \n  slice(23:27)\n\n# A tibble: 5 × 8\n  species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Biscoe           35.9          19.2               189        3800\n2 Adelie  Biscoe           38.2          18.1               185        3950\n3 Adelie  Biscoe           38.8          17.2               180        3800\n4 Adelie  Biscoe           35.3          18.9               187        3800\n5 Adelie  Biscoe           40.6          18.6               183        3550\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nEjemplo: Seleccionar los 5 primeros pingüinos\n\npenguins %&gt;% \n  slice_head(n = 5) \n\n# A tibble: 5 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n# alternativamente: \n#slice_head(frac = 0.05)\n\nEjemplo: Seleccionar una muestra aleatoria de n pingüinos\n\npenguins %&gt;% \n  slice_sample(n = 5)\n\n# A tibble: 5 × 8\n  species   island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;     &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Chinstrap Dream               52.8          20                 205        4550\n2 Adelie    Torgersen           37.3          20.5               199        3775\n3 Chinstrap Dream               43.2          16.6               187        2900\n4 Gentoo    Biscoe              47.5          14.2               209        4600\n5 Gentoo    Biscoe              52.2          17.1               228        5400\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\nEjemplo: Seleccionar los n pingüinos con el pico más grande.\n\n\npenguins %&gt;% \n  slice_max(bill_length_mm, n = 5)\n\n# A tibble: 5 × 8\n  species   island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;     &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Gentoo    Biscoe           59.6          17                 230        6050\n2 Chinstrap Dream            58            17.8               181        3700\n3 Gentoo    Biscoe           55.9          17                 228        5600\n4 Chinstrap Dream            55.8          19.8               207        4000\n5 Gentoo    Biscoe           55.1          16                 230        5850\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nEjemplo: Seleccionar los cinco pingüinos con menor masa corporal.\n\npenguins %&gt;% \n  arrange(body_mass_g) %&gt;% \n  slice_head(n = 5)  # equivalentente a: slice_min(body_mass_g, n = 3)\n\n# A tibble: 5 × 8\n  species   island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;     &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Chinstrap Dream            46.9          16.6               192        2700\n2 Adelie    Biscoe           36.5          16.6               181        2850\n3 Adelie    Biscoe           36.4          17.1               184        2850\n4 Adelie    Biscoe           34.5          18.1               187        2900\n5 Adelie    Dream            33.1          16.1               178        2900\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n1.7.7.2 Operaciones por columnas\n\nselect() selecciona determinadas columnas\nrename() cambia los nombres de las columnas\nrelocate() cambia el orden de las columnas\nmutate() transforma los valores de las columnas y/o crea nuevas columnas\n\nEjemplo: Selección por número de la(s) columna(s)\n\npenguins %&gt;% \n  select(1:3) %&gt;% \n  glimpse\n\nRows: 344\nColumns: 3\n$ species        &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie,…\n$ island         &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, …\n$ bill_length_mm &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, 42.…\n\n\nEjemplo: Selección por nombre de la(s) columna(s)\n\npenguins %&gt;% \n  select(species, island, bill_length_mm) %&gt;% \n  glimpse\n\nRows: 344\nColumns: 3\n$ species        &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie,…\n$ island         &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, …\n$ bill_length_mm &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, 42.…\n\n\nEjemplo: Seleccionar todas las columnas\n\npenguins %&gt;% \n  select(everything()) %&gt;% \n  glimpse\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n# select(last_col())\n\nEjemplo: Seleccionar las columnas cuyos nombres empiezan por un patrón específico\n\npenguins %&gt;% \n  select(starts_with(\"bill\")) %&gt;% \n  glimpse\n\nRows: 344\nColumns: 2\n$ bill_length_mm &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, 42.…\n$ bill_depth_mm  &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, 20.…\n\n# ends_with()\n\n\npenguins %&gt;% \n  select(contains(\"e\") & contains(\"a\")) %&gt;% \n  glimpse\n\nRows: 344\nColumns: 1\n$ year &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007,…\n\n\nEjemplo: Seleccionar columnas en base a una expresión regular (regex)\n\npenguins %&gt;% \n  select(matches(\"_\\\\w*_mm$\")) %&gt;% \n  glimpse\n\nRows: 344\nColumns: 3\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n\n\n\npenguins %&gt;% \n  select(where(is.numeric)) %&gt;% \n  glimpse\n\nRows: 344\nColumns: 5\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n\nEjercicio: ¿Qué columnas devuelven las siguientes consultas?\n\n\npenguins %&gt;% \n  select(ends_with(\"mm\"))\n\n\npenguins %&gt;% \n  select(-contains(\"mm\"))\n\n\npenguins %&gt;% \n  select(where(~ is.numeric(.))) %&gt;%  # select(where(is.numeric))\n  select(where(~ mean(., na.rm = T) &gt; 1000))\n\nEjemplo: Cambiar el nombre de la columna body_mass_g a bm y sex a gender.\n\npenguins %&gt;% rename(bm = body_mass_g, gender = sex) %&gt;% \n  colnames()\n\n[1] \"species\"           \"island\"            \"bill_length_mm\"   \n[4] \"bill_depth_mm\"     \"flipper_length_mm\" \"bm\"               \n[7] \"gender\"            \"year\"             \n\n\nEjemplo: Cambiar los nombres de las columnas que incluyen \"mm\" a mayúsculas.\n\npenguins %&gt;% rename_with(.fn = toupper, .cols = contains(\"mm\")) %&gt;% \n  colnames()\n\n[1] \"species\"           \"island\"            \"BILL_LENGTH_MM\"   \n[4] \"BILL_DEPTH_MM\"     \"FLIPPER_LENGTH_MM\" \"body_mass_g\"      \n[7] \"sex\"               \"year\"             \n\n\nEjemplo: Cambiar el orden de las columnas en la tibble de acuerdo al siguiente esquema:\n\ncolocar “especie” después de “masa corporal”.\ncolocar “sexo” antes de “especie”.\ncolocar “isla” al final\n\n\npenguins %&gt;% \n  relocate(species, .after = body_mass_g) %&gt;%\n  relocate(sex, .before = species) %&gt;%\n  relocate(island, .after = last_col()) %&gt;%\n  colnames()\n\n[1] \"bill_length_mm\"    \"bill_depth_mm\"     \"flipper_length_mm\"\n[4] \"body_mass_g\"       \"sex\"               \"species\"          \n[7] \"year\"              \"island\"           \n\n\nEjemplo: Crear una nueva variable bm_kg que ponga body_mass_g en kilogramos.\n\npenguins %&gt;% \n  mutate(bm_kg = body_mass_g/1000, .keep = \"all\", .after = island) %&gt;% \n  slice_head(n = 5)\n\n# A tibble: 5 × 9\n  species island    bm_kg bill_length_mm bill_depth_mm flipper_length_mm\n  &lt;fct&gt;   &lt;fct&gt;     &lt;dbl&gt;          &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;\n1 Adelie  Torgersen  3.75           39.1          18.7               181\n2 Adelie  Torgersen  3.8            39.5          17.4               186\n3 Adelie  Torgersen  3.25           40.3          18                 195\n4 Adelie  Torgersen NA              NA            NA                  NA\n5 Adelie  Torgersen  3.45           36.7          19.3               193\n# ℹ 3 more variables: body_mass_g &lt;int&gt;, sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\nUsamos .keep para especificar las columnas que se mantendrán después de la manipulación.\nUsamos .before/.after para especificar la posición de la nueva columna.\nPara anular una columna dada simplemente utiliza el mismo nombre de columna.\nPara mantener únicamente la nueva columna utiliza dplyr::transmute().\n\nEjemplo: Codificación de una variable categórica con “C” niveles de factor en “C-1” binarias o dummies.\n\npenguins %&gt;% \n  mutate(\n    sex_binary = case_when(\n      sex == \"male\" ~ 1,\n      sex == \"female\" ~ 0),\n    .keep = \"all\", .after = island\n  ) %&gt;% \n  slice_head(n = 3)\n\n# A tibble: 3 × 9\n  species island    sex_binary bill_length_mm bill_depth_mm flipper_length_mm\n  &lt;fct&gt;   &lt;fct&gt;          &lt;dbl&gt;          &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;\n1 Adelie  Torgersen          1           39.1          18.7               181\n2 Adelie  Torgersen          0           39.5          17.4               186\n3 Adelie  Torgersen          0           40.3          18                 195\n# ℹ 3 more variables: body_mass_g &lt;int&gt;, sex &lt;fct&gt;, year &lt;int&gt;\n\n\nEjemplo:\ncase_when: - Versión vectorizada de if_else\n\nFórmulas de dos lados: El LHS comprueba la condición, el RHS especifica el valor de sustitución\nPara los casos no coincidentes, la función devuelve NA\nUtiliza el LHS TRUE para capturar todos los casos no especificados explícitamente de antemano.\n\nEjemplo: Transformar las variables de medidas a metros\n\npenguins %&gt;% \n  mutate(\n    across(contains(\"mm\"), ~ ./1000),\n    .keep = \"all\"\n  ) %&gt;% \n  slice_head(n = 3)\n\n# A tibble: 3 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;int&gt;\n1 Adelie  Torgersen         0.0391        0.0187             0.181        3750\n2 Adelie  Torgersen         0.0395        0.0174             0.186        3800\n3 Adelie  Torgersen         0.0403        0.018              0.195        3250\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nEjemplo:\nacross:\n\nAplicar la misma transformación en varias columnas\nTe permite utilizar la semántica que conoces de la función select().\nNo requiere que se especifique explícitamente un nombre de columna, ya que sólo transforma las columnas existentes\n\nEjemplo: Definir species, island y sex como variables categóricas, es decir factores, usando across().\n\npenguins %&gt;% \n  mutate(\n    across(where(is.character), as.factor),\n    .keep = \"all\"\n  ) %&gt;% \n  slice_head(n = 3)\n\n# A tibble: 3 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n1.7.7.3 Operaciones sobre datos agrupados\n\ngroup_by() divide los datos en función de una o varias columnas\nsummarise() reduce un grupo de datos en una sola fila\n\nEjemplo: Partición de los datos por tipo de especie del pingüino\n\npenguins %&gt;% group_by(species)\n\n# A tibble: 344 × 8\n# Groups:   species [3]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nUtiliza group_keys(), group_indices() y group_vars() para acceder a las claves de agrupación, los índices de grupo por fila y las variables de agrupación.\n\ngroup_by() cambia la representación del tibble y lo transforma en un data frame agrupado (grouped_df). Esto nos permite operar en los subgrupos individualmente usando summarise().\n\nEjemplo:\n\npenguins %&gt;% group_by(species) %&gt;% \n  summarise(count = n(), .groups = \"drop\")\n\n# A tibble: 3 × 2\n  species   count\n  &lt;fct&gt;     &lt;int&gt;\n1 Adelie      152\n2 Chinstrap    68\n3 Gentoo      124\n\n\n\npenguins %&gt;% group_by(species, sex) %&gt;% summarise(count = n(), .groups = \"drop\")\n\n# A tibble: 8 × 3\n  species   sex    count\n  &lt;fct&gt;     &lt;fct&gt;  &lt;int&gt;\n1 Adelie    female    73\n2 Adelie    male      73\n3 Adelie    &lt;NA&gt;       6\n4 Chinstrap female    34\n5 Chinstrap male      34\n6 Gentoo    female    58\n7 Gentoo    male      61\n8 Gentoo    &lt;NA&gt;       5\n\n\n\npenguins %&gt;%\n  group_by(species) %&gt;%\n  summarise(\n    across(contains(\"mm\"), ~ mean(., na.rm = T), .names = \"{.col}_media\"),\n    .groups = \"drop\"\n  )\n\n# A tibble: 3 × 4\n  species   bill_length_mm_media bill_depth_mm_media flipper_length_mm_media\n  &lt;fct&gt;                    &lt;dbl&gt;               &lt;dbl&gt;                   &lt;dbl&gt;\n1 Adelie                    38.8                18.3                    190.\n2 Chinstrap                 48.8                18.4                    196.\n3 Gentoo                    47.5                15.0                    217.\n\n\n\nUtilizar group_by(), seguido de summarise() y ungroup() refleja el paradigma dividir-aplicar-combinar del análisis de datos: Dividir los datos en particiones, aplicar alguna función a los datos y luego combinar los resultados.\n\n\n.add = T permite añadir nuevas variables de agrupación (si no, se anula la primera)\n\n\npenguins %&gt;% \n  group_by(species) %&gt;% \n  group_by(year, .add = T)   # equivalente a: group_by(species, year)\n\n# A tibble: 344 × 8\n# Groups:   species, year [9]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\npenguins %&gt;%\n  group_by(species) %&gt;%\n  summarise(\n    across(\n      contains(\"mm\"),\n      list(media = ~ mean(., na.rm = T), sd = ~ sd(., na.rm = T)),\n      .names = \"{.col}_{.fn}\"\n    ),\n    .groups = \"drop\"\n  )\n\n# A tibble: 3 × 7\n  species   bill_length_mm_media bill_length_mm_sd bill_depth_mm_media\n  &lt;fct&gt;                    &lt;dbl&gt;             &lt;dbl&gt;               &lt;dbl&gt;\n1 Adelie                    38.8              2.66                18.3\n2 Chinstrap                 48.8              3.34                18.4\n3 Gentoo                    47.5              3.08                15.0\n# ℹ 3 more variables: bill_depth_mm_sd &lt;dbl&gt;, flipper_length_mm_media &lt;dbl&gt;,\n#   flipper_length_mm_sd &lt;dbl&gt;\n\n\nEjemplo: Las funciones de resumen, por ejemplo, mean() o sd() operan en particiones de los datos en lugar de en los datos completos\n\npenguins %&gt;%\n  group_by(species) %&gt;% \n  mutate(stand_bm = (body_mass_g - mean(body_mass_g, na.rm = T))\n         /sd(body_mass_g, na.rm = T)) %&gt;% \n  glimpse\n\nRows: 344\nColumns: 9\nGroups: species [3]\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n$ stand_bm          &lt;dbl&gt; 0.107591350, 0.216626878, -0.982763938, NA, -0.54662…\n\n\nEjemplo: Calcular franjas para la masa corporal de acuerdo a la cantidad de desviaciones estándar de la media. Agrupar los datos según estos intervalos.\n\nbm_breaks &lt;- mean(penguins$body_mass_g, \n                  na.rm = T) - (-3:3) *\n  sd(penguins$body_mass_g,na.rm = T)\n\npenguins %&gt;% \n  group_by(species, bm_bin = cut(body_mass_g, breaks = bm_breaks)) %&gt;%\n  summarise(count = n(), .groups = \"drop\")\n\n\n\n# A tibble: 12 × 3\n   species   bm_bin              count\n   &lt;fct&gt;     &lt;fct&gt;               &lt;int&gt;\n 1 Adelie    (2.6e+03,3.4e+03]      39\n 2 Adelie    (3.4e+03,4.2e+03]      87\n 3 Adelie    (4.2e+03,5e+03]        25\n 4 Adelie    &lt;NA&gt;                    1\n 5 Chinstrap (2.6e+03,3.4e+03]      11\n 6 Chinstrap (3.4e+03,4.2e+03]      50\n 7 Chinstrap (4.2e+03,5e+03]         7\n 8 Gentoo    (3.4e+03,4.2e+03]       6\n 9 Gentoo    (4.2e+03,5e+03]        56\n10 Gentoo    (5e+03,5.81e+03]       52\n11 Gentoo    (5.81e+03,6.61e+03]     9\n12 Gentoo    &lt;NA&gt;                    1\n\n\nEjemplo: Filtrar en las particiones en lugar de en la totalidad de los datos\n\npenguins %&gt;% \n  group_by(species, island) %&gt;% \n  filter(flipper_length_mm == max(flipper_length_mm, na.rm = T))\n\n# A tibble: 5 × 8\n# Groups:   species, island [5]\n  species   island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;     &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie    Dream               40.8          18.9               208        4300\n2 Adelie    Biscoe              41            20                 203        4725\n3 Adelie    Torgersen           44.1          18                 210        4000\n4 Gentoo    Biscoe              54.3          15.7               231        5650\n5 Chinstrap Dream               49            19.6               212        4300\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nEjemplo: Utilizar group_by() seguido de nest() para producir un data frame anidado\n\npenguins %&gt;% \n  group_by(species, year) %&gt;% \n  tidyr::nest()\n\n# A tibble: 9 × 3\n# Groups:   species, year [9]\n  species    year data             \n  &lt;fct&gt;     &lt;int&gt; &lt;list&gt;           \n1 Adelie     2007 &lt;tibble [50 × 6]&gt;\n2 Adelie     2008 &lt;tibble [50 × 6]&gt;\n3 Adelie     2009 &lt;tibble [52 × 6]&gt;\n4 Gentoo     2007 &lt;tibble [34 × 6]&gt;\n5 Gentoo     2008 &lt;tibble [46 × 6]&gt;\n6 Gentoo     2009 &lt;tibble [44 × 6]&gt;\n7 Chinstrap  2007 &lt;tibble [26 × 6]&gt;\n8 Chinstrap  2008 &lt;tibble [18 × 6]&gt;\n9 Chinstrap  2009 &lt;tibble [24 × 6]&gt;\n\n\nPuedes encontrar más información de group_by() ejecutando vignette(\"grouping\").\n\n\n1.7.7.4 Otras operaciones con dlpyr\n\ndistinct() selecciona sólo filas únicas\n\n\npenguins %&gt;% \n  distinct(species, island)\n\n# A tibble: 5 × 2\n  species   island   \n  &lt;fct&gt;     &lt;fct&gt;    \n1 Adelie    Torgersen\n2 Adelie    Biscoe   \n3 Adelie    Dream    \n4 Gentoo    Biscoe   \n5 Chinstrap Dream    \n\n\n\npull() extrae columnas individuales como vectores\n\n\npenguins %&gt;% \n  pull(year)  # equivalente a: penguins$year\n\n\nif_else() sentencia if-else vectorizada.\n\n\npenguins %&gt;% select(species, island, body_mass_g) %&gt;% \n  mutate(penguin_size = if_else(body_mass_g &lt; 3500,\n                                \"pequeño\",\n                                \"grande\"))\n\n# A tibble: 344 × 4\n   species island    body_mass_g penguin_size\n   &lt;fct&gt;   &lt;fct&gt;           &lt;int&gt; &lt;chr&gt;       \n 1 Adelie  Torgersen        3750 grande      \n 2 Adelie  Torgersen        3800 grande      \n 3 Adelie  Torgersen        3250 pequeño     \n 4 Adelie  Torgersen          NA &lt;NA&gt;        \n 5 Adelie  Torgersen        3450 pequeño     \n 6 Adelie  Torgersen        3650 grande      \n 7 Adelie  Torgersen        3625 grande      \n 8 Adelie  Torgersen        4675 grande      \n 9 Adelie  Torgersen        3475 pequeño     \n10 Adelie  Torgersen        4250 grande      \n# ℹ 334 more rows\n\n\n\nlag() desplaza los valores de las columnas n posiciones hacia adelante\n\n\npenguins %&gt;% select(species, body_mass_g) %&gt;% \n  mutate(lagged_bm = lag(body_mass_g, n = 1))\n\n# A tibble: 344 × 3\n   species body_mass_g lagged_bm\n   &lt;fct&gt;         &lt;int&gt;     &lt;int&gt;\n 1 Adelie         3750        NA\n 2 Adelie         3800      3750\n 3 Adelie         3250      3800\n 4 Adelie           NA      3250\n 5 Adelie         3450        NA\n 6 Adelie         3650      3450\n 7 Adelie         3625      3650\n 8 Adelie         4675      3625\n 9 Adelie         3475      4675\n10 Adelie         4250      3475\n# ℹ 334 more rows\n\n\n\nCombinar diferentes data frames haciendo coincidir las filas en función de la “key”\n\n\n\n\n\n\nNota\n\nEn el enlace de Tidydatatutor puedes escribir código R y Tidyverse en tu navegador y ver cómo cambia el data frame en cada paso del pipeline que has escrito.\n\nAquí os dejamos el enlace para visualizar las instrucciones más utilizadas de tidyverse con el ejemplo de los pingüinos:\n\narrange()\nfilter()\nmutate()\nselect()\ngroup_by() %&gt;% slice()\ngroup_by() %&gt;% summarize()\n\n\n\n\n1.7.8 Visualización de datos ggplot2\nggplot2 es un sistema para crear gráficos de forma declarativa, basado en The Grammar of Graphics. Permite producir “gráficos modernos para el análisis de datos”.\nLa sintaxis de ggplot2 ayuda a pensar los gráficos de una manera nueva y más general que R base, por ejemplo, facilita el agregar las leyendas, los ejes, los colores, en comparación con R base.\nEn R gallery, R charts y en ggplot2 extensions puedes encontrar muchos ejemplos de gráficos con los códigos que os servirán de inspiración.\n\n\n1.7.9 Gramática básica de ggplot2\nUn ggplot necesita al menos tres cosas que hay que especificar: \n\nDatos: Normalmente un tibble del que se seleccionan las variables a visualizar. Siempre empezamos con ggplot(data = df) que le dice a {ggplot2} que vamos a trabajar con los datos df.\nEstética: Propiedades visuales que deseamos tenga nuestro gráfico. Por ejemplo, si deseamos visualizar la relación entre dos variables de df: var1 en el eje x y var2 en el eje y, debemos especificar: aes(x = var1, y = var2).\nGeometría: Forma geométrica que deseamos utilizar para representar los datos: puntos, líneas continuas, curvas, entre otras. Se especifica con geom_*(). Por ejemplo, geom_point() para hacer un diagrama de puntos.\n\nEjemplo:\n\npenguins %&gt;% \n  ggplot(aes(x=bill_length_mm, y = flipper_length_mm)) +\n  geom_point(na.rm = TRUE)\n\n\n\n\n\n\n\n\n\n\n1.7.10 Más elementos de la gramática …\n\nEscalas: Permiten asignar los valores que hay en los datos a los valores visuales de una estética (anulando los valores por defecto de esa estética). La sintáxis es scale_*(). Existen escalas para asignar valores de manera discreta, continua o manual. También escalas de localización, para color, relleno; tamaño y figuras.\n\n\n\n\n\n\n\nResúmenes: Permiten construir nuevas variables de resumen para hacer un gráfico. Por ejemplo: conteo, cuantiles, proporciones, curvas ajustadas. Se especifica con stat_*().\n\n\n\n\n\n\n\nSistema de coordenadas: Permite especificar cómo deseamos las coordenadas del gráfico. La sintaxis es coord_*.\n\n\n\n\n\n\n\nFacetas: Dividen un gráfico en múltiples subgráficos de acuerdo a una o varias variables discretas. La sintaxis es facet_*.\n\n\n\n\n\n\n\nAjustes de las posiciones: Permite indicar qué hacer con geometrías (geoms) que ocuparían la misma posición en el gráfico.\n\n\n\n\n\n\n\nTema: Valores visuales generales de un gráfico, como el fondo, las cuadrículas, los ejes, el tipo de letra predeterminado, los tamaños y los colores.\n\n\n\n\n\n\n\n\n\n\nHay muchas cosas más, puedes descargar la hoja de trucos de ggplot2 en castellano.\n\n\n1.7.11 Visualizando una variable cualitativa con ggplot2\nVamos a visualizar una variable cualitativa, con un tema que no es el que trae por defecto ggplot con fondo gris.\n\npenguins %&gt;% \n  ggplot(aes(x = species)) +\n  geom_bar(fill=\"blue\") + \n  labs(x=\"Especie\", y=\"Número de pingüinos\") +\n  theme_bw() +\n  theme(axis.text = element_text(size=20),\n        axis.title = element_text(size=20, face = \"bold\")) \n\n\n\n\n\n\n\n\n\n\n1.7.12 Cruzando dos variables cualitativas con ggplot2\nPara observar la distribución conjunta de dos variables cualitativas podemos emplear un gráfico de barras. Las siguientes instrucciones nos ayudan a visualizar la relación entre las variables species e island del conjunto de datos de los pingüinos.\n\npenguins %&gt;% ggplot() + \n  geom_bar(aes(species, fill=island),\n           position=\"dodge\") + coord_flip() +\n  guides(fill = guide_legend(title = \"Isla\")) +\n  labs(x=\"Número de pingüinos\", y=\"Especie\") +\n  theme_bw() +\n  theme(axis.text = element_text(size=20),\n        axis.title = element_text(size=20, face = \"bold\"),\n        legend.title = element_text(size=20)) \n\n\n\n\n\n\n\n\n\n\nSi deseamos que cada barra represente el 100% de la categoría, se indica en el argumento de geom_bar.\n\npenguins %&gt;% ggplot() + \n  geom_bar(aes(species, fill=island),\n           position=\"fill\") + coord_flip() +\n  guides(fill = guide_legend(title = \"Isla\")) +\n  labs(y=\"Proporción de pingüinos\", x=\"Especie\") +\n  theme_bw() +\n  theme(axis.text = element_text(size=20),\n        axis.title = element_text(size=20, face = \"bold\"),\n        legend.title = element_text(size=20))\n\n\n\n\n\n\n\n\n\n\n1.7.13 Visualizando una variable cuantitativa con ggplot2\n\npenguins %&gt;% \n  ggplot(aes(x = flipper_length_mm)) +\n  geom_histogram(na.rm = TRUE) +\n    labs(x=\"Longitud de la aleta en mm\", \n         y=\"Frecuencia absoluta\") + \n  theme_bw() +\n  theme(axis.text = element_text(size=20),\n        axis.title = element_text(size=20, face = \"bold\"))\n\n\n\n\n\n\n\n\n\n\n1.7.14 Cruzando una variable cuantitativa con una cualitativa\n\nggplot(data = penguins, aes(x = flipper_length_mm)) +\n  geom_histogram(aes(fill = species), \n                 alpha = 0.5, \n                 position = \"identity\",\n                 na.rm = TRUE) +\n  scale_fill_manual(values = c(\"darkorange\",\"purple\",\"cyan4\")) +\n  labs(x = \"Longitud de la aleta en mm\",\n       y = \"Frecuencia absoluta\") +\n  guides(fill = guide_legend(title = \"Especie\")) +\n  theme_bw() +\n  theme(axis.text = element_text(size=20),\n        axis.title = element_text(size=20, face = \"bold\"),\n        legend.title = element_text(size=20))\n\n\n\n\n\n\n\n\n\n\nPara que el gráfico sea más claro, se los puede separar con facetas: facet_grid(.~species) justo después de la capa geom_histogram.\n\nggplot(data = penguins, aes(x = flipper_length_mm)) +\n  geom_histogram(aes(fill = species), \n                 alpha = 0.5, \n                 position = \"identity\",\n                 na.rm = TRUE) +\n  facet_grid(.~species) +\n  scale_fill_manual(values = c(\"darkorange\",\"purple\",\"cyan4\")) +\n  labs(x = \"Longitud de la aleta en mm\",\n       y = \"Frecuencia absoluta\") +\n  guides(fill = guide_legend(title = \"Especie\")) +\n  theme_bw() +\n  theme(axis.text = element_text(size=20),\n        axis.title = element_text(size=20, face = \"bold\"),\n        legend.title = element_text(size=20))\n\n\n\n\n\n\n\n\n\nLa capa facet_grid admite dos variables como argumento, separadas por ~, las categorías de la primera definen las filas y las de la segunda, las columnas. Si solo se usa una, se ubica un punto en el lugar de la otra.\nOtra forma muy conveniente de cruzar una variable cuantitativa con otra cualitativa es usando boxplots.\n\n\nggplot(data = penguins, aes(x = species, y = flipper_length_mm)) +\n  geom_boxplot(aes(color = species), width = 0.3, \n               show.legend = FALSE) + \n  geom_jitter(aes(color = species), alpha = 0.5, \n              show.legend = FALSE, \n              position = position_jitter(width = 0.2, seed = 0)) +\n  scale_color_manual(values = c(\"darkorange\",\"purple\",\"cyan4\")) +\n  labs(x = \"Epecie\", y = \"Longitud de la aleta en mm\")\n\n\n\n\n\n\n\n\n\n\n\n1.7.14.1 Cruzando dos variables cuantitativas\n\nggplot(penguins) +\n  geom_point(mapping = aes(x = flipper_length_mm,\n                           y = body_mass_g,\n                           color = sex), size=3)+ theme_bw() +\n  theme(axis.text = element_text(size=20),\n        axis.title = element_text(size=20, face = \"bold\"),\n        legend.title = element_text(size=20)) + \n  guides(fill = guide_legend(title = \"Sexo\"))\n\n\n\n\n\n\n\n\n\n\n\n\n1.7.14.2 Códigos de color html\n\nPara utilizar colores personalizados, puedes consultar los códigos de color HTML (también llamados códigos hexadecimales, por ejemplo, #ff0000 para el rojo) en lugar de especificar los colores por su nombre predefinido en R.\nLa familia de funciones scale_colour_*() permite ajustar los valores de la estética color (por ejemplo, scale_colour_brewer() selecciona una paleta del famoso proyecto ColorBrewer).\n\n\n\n\n1.7.15 Ejemplos integrando las herramientas de tidyverse\n\nEjemplo 1:\n\n\npenguins_long &lt;- penguins %&gt;% \n  tidyr::pivot_longer(\n    cols = contains(\"mm\"),\n    names_to = \"var\", values_to = \"val\") %&gt;% \n  tidyr::drop_na()\n\npenguins_long %&gt;% \n  ggplot(aes(x = var, y = val, fill=var)) +\n  geom_boxplot() +\n  theme_bw() +\n  theme(legend.position=\"none\") +\n  labs(x=\"\", y=\"Medida en mm\") +\n  theme(axis.text = element_text(size=20),\naxis.title = element_text(size=20, face = \"bold\"),\nlegend.title = element_text(size=20))\n\n\n\n\n\n\n\n\n\nEjemplo 2:\n\n\npenguins %&gt;%\n  dplyr::count(species) %&gt;%\n  dplyr::mutate(prop = n / sum(n)) %&gt;%\n  ggplot() + geom_col(aes(x = prop, y = species)) +\n  labs(x=\"Proporción\", y=\"\") + \n  theme(axis.text = element_text(size=20),\naxis.title = element_text(size=20, face = \"bold\"),\nlegend.title = element_text(size=20)) \n\n\n\n\n\n\n\n\n\n\n1.7.16 Extensiones de ggplot\nHay muchas extensiones de ggplot, algunos ejemplos son:\n\nGGally\n\n\nlibrary(GGally)\nlibrary(gapminder)\ngapminder %&gt;% select(-country,-year) %&gt;% \n  ggpairs(aes(color=continent))\n\n\n\n\n\n\n\n\n\n\n\nggpubr proporciona algunas funciones fáciles de usar para crear y personalizar gráficos útiles para una publicación.\n\n\n\n\n\n\n\nPuedes ver más ejemplos en ‘ggplot2’ Based Publication Ready Plots.\nEn la cuenta de gitHub de Allison Horst puedes encontrar otros ejemplos de gráficos con los datos de los pingüinos.\n\n\n\n1.7.17 Gráficos interactivos\nMencionaremos algunas librerías que podéis utilizar tanto en combinación con ggplot2 o por sí solas para crear visualizaciones interactivas en R.\n\n1.7.17.1 Combinando ggplot2y shiny\nshiny es un paquete de RStudio que hace fácil construir aplicaciones web interactivas con R. Para una introducción y ejemplos en vivo, visita la página web de Shiny.\nPara ver el uso potencial, puedes consultar los ejemplos de Hello Shiny, ejecuta las siguientes instrucciones en la consola de tu RStudio.\n\nlibrary(shiny)\nrunExample(\"01_hello\")\nrunExample(\"04_mpg\")\n\n\n\n1.7.17.2 Plotly\nPlot.ly es una herramienta para crear gráficos interactivos en línea y aplicaciones web.\nEl paquete plotly permite crear los gráficos interactivos a partir de tus gráficos ggplot desde R.\nPara ilustrar las herramientas de esta librería utilizaremos datos de los precios de Google desde enero de 2018 hasta el 31 de diciembre de 2019 que están disponibles en la librería tidyquant.\n\nlibrary(tidyquant)\nlibrary(plotly)\ngetSymbols(\"GOOG\",\n             from = \"2018-01-01\",\n             to = \"2019-12-31\")\n\n[1] \"GOOG\"\n\nstock &lt;- data.frame(GOOG$GOOG.Adjusted)\nstock$GOOG.Adjusted &lt;- stock$GOOG.Adjusted/stock$GOOG.Adjusted[1]\nstock &lt;- data.frame(stock,rownames(stock))\ncolnames(stock) &lt;- append('GOOG','date')\n\nfig &lt;- plot_ly(stock, type = 'scatter', mode = 'lines')%&gt;%\n  add_trace(x = ~date, y = ~GOOG, name = 'GOOG')%&gt;%\n  layout(showlegend = F)\noptions(warn = -1)\n\nfig &lt;- fig %&gt;%\n  layout(\n         xaxis = list(zerolinecolor = '#ffff',\n                      zerolinewidth = 2,\n                      gridcolor = 'ffff'),\n         yaxis = list(zerolinecolor = '#ffff',\n                      zerolinewidth = 2,\n                      gridcolor = 'ffff'),\n         plot_bgcolor='#e5ecf6', width = 900)\n\n\nfig\n\n\n\n\n\n\n\n1.7.17.3 ggiraph\nggiraph es un paquete de R que permite crear gráficos dinámicos ggplot2.\nPermite añadir tooltips (globo /herramienta de ayuda visual), animaciones y acciones JavaScript a los gráficos.\n\n\n\n1.7.18 Recursos adicionales\n\nggplot2: Elegant Graphics for Data Analysis por Hadley Wickham, disponible en acceso abierto.\nFundamentals of Data Visualization por Claus O. Wilke sobre la visualización de datos en general pero utilizando ggplot2. Puedes encontrar los códigos en su perfil de GitHub.\nCookbook for R por Winston Chang con recetas para producir gráficos en R.\nGalería 50 mejores visualizaciones de ggplot2.\nA ggplot2 Tutorial for Beautiful Plotting in R",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción al Análisis de Datos</span>"
    ]
  },
  {
    "objectID": "t1_intro.html#práctica-2",
    "href": "t1_intro.html#práctica-2",
    "title": "1  Introducción al Análisis de Datos",
    "section": "1.8 Práctica 2",
    "text": "1.8 Práctica 2\nLa base de datos mpg (ya precargada en tidyverse) contiene datos de vehículos y sus rendimientos tanto en ciudad como en carretera. La descripción de las variables es la siguiente:\n\nmanufacturer: nombre del fabricante.\nmodel: nombre del modelo.\ndispl: desplazamiento del motor, en litros.\nyear: año de fabricación.\ncyl: número de cilindros.\ntrans: tipo de transmisión (manual o automática)\ndrv: tipo de tracción, con f = tracción delantera, r = tracción trasera, 4 = tracción en cuatro ruedas.\ncty: rendimiento en ciudad, en millas por galón.\nhwy: rendimiento en carretera, en millas por galón.\nfl: tipo de combustible, con e = etanol, d = diesel, r = regular, p = premium, c = CNG (gas natural).\nclass: tipo de vehículo.\n\n\nDibujad un gráfico que permita visualizar el número de vehículos de cada fabricante del conjunto de datos.\nDibujad un gráfico para mostrar el rendimiento medio en ciudad para cada clase.\nConstruid un gráfico para mostrar el rendimiento medio en ciudad para cada clase y tipo de tracción simultáneamente.\nUtilizad un gráfico para mostrar la relación entre el tamaño del motor y el rendimiento en carretera, para cada clase de vehículo. ¿Qué podéis observar?\nComparad la distribución del rendimiento en ciudad para distintos tipos de tracción.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción al Análisis de Datos</span>"
    ]
  },
  {
    "objectID": "t2_em.html",
    "href": "t2_em.html",
    "title": "2  Análisis Multivariante",
    "section": "",
    "text": "2.1 Poblaciones: vectores aleatorios\nUn vector aleatorio de dimensión \\(p\\) es un vector formado por \\(p\\) variables aleatorias \\[\n\\underline{X}=(X_1,X_2,\\ldots,X_p).\n\\] Una realización de \\(\\underline{X}\\) es un vector \\((x_1,\\ldots,x_p)\\) formado por los valores \\(X_1,\\ldots,X_p\\) sobre un individuo.\nUna muestra de \\(\\underline{X}\\) es un conjunto de realizaciones.\nUsualmente, organizamos una muestra de \\(\\underline{X}\\) por medio de una tabla de datos con las columnas definidas por las variables \\(X_1,\\ldots,X_p\\) y donde cada fila es una realización de estas variables, es decir, un vector formado por los valores de \\(X_1,\\ldots,X_p\\) sobre un individuo de la muestra.\nEjemplo: Sea \\(\\textit{BMI}\\) la variable aleatoria que registra el índice de masa corporal (BMI) de una persona, \\(C\\) la variable aleatoria que registra el nivel de colesterol en mg/dl de una persona y \\(E\\) la variable aleatoria que da la edad de una persona en años, entonces \\[\n\\underline{X}=(\\textit{BMI},C,E)\n\\] es un vector aleatorio de dimensión 3. Cada vez que tomamos una persona y anotamos el BMI, el nivel de colesterol y la edad y organizamos estas medidas en este orden en un vector, obtenemos una realitzación de este vector aleatorio \\(\\underline{X}\\). Entonces,una muestra de \\(\\underline{X}\\) será un conjunto de vectores con el BMI, el nivel de colesterol y la edad de un grupo de personas de la población. Por ejemplo\nBMI\nC\nE\n\n\n\n\n18.3\n170\n49\n\n\n24.4\n202\n39\n\n\n24.6\n215\n50\n\n\n24.4\n218\n44\n\n\n22.2\n210\n40\n\n\n19.5\n210\n36\nSi \\(\\mathbf{X}=(X_1,\\ldots,X_p)\\) es un vector aleatorio con distribución absolutamente contínua y función de densidad \\(f(x_1,\\ldots,x_p)\\), \\(f\\) verifica:\nConocida \\(f(x_1,\\ldots,x_p)\\) podemos encontrar la función de densidad de cada variable marginal \\(X_j\\) mediante la integral\n\\[f_j(x_j)=\\int f(x_1,\\ldots,x_j,\\ldots,x_p)dx_1\\cdots dx_{j-1}dx_{j+1}\\cdots dx_p.\\]\nSean \\(\\underline{X}=(X_1,X_2,\\ldots,X_p)\\) un vector aleatorio y, \\(\\mu_i\\) y \\(\\sigma_i\\) la media y la desviación típica, respectivamente, de cada \\(X_i\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Análisis Multivariante</span>"
    ]
  },
  {
    "objectID": "t2_em.html#poblaciones-vectores-aleatorios",
    "href": "t2_em.html#poblaciones-vectores-aleatorios",
    "title": "2  Análisis Multivariante",
    "section": "",
    "text": "\\(f(x_1,\\ldots,x_p)\\ge0\\), para todo \\((x_1,\\ldots,x_p)\\in\\mathbb{R}^p\\).\n\\(\\int_{\\mathbb{R}^p}f(x_1,\\ldots,x_p)dx_1\\cdots dx_p=1.\\)\n\n\n\n\n\nEl valor esperado, o vector de medias, de \\(\\underline{X}\\) es el vector formado por los valores esperados, o medias de sus componentes: \\[\nE(\\underline{X})=(\\mu_1,\\ldots,\\mu_p).\n\\] Para abreviar, a veces indicaremos este vector simplemente con \\(\\boldsymbol\\mu\\).\nEl vector de varianzas de \\(\\underline{X}\\) es el vector formado por las varianzas de sus componentes: \\[\n\\sigma^2(\\underline{X})=(\\sigma_1^2,\\ldots,\\sigma_p^2).\n\\]\nEl vector de desviaciones típicas de \\(\\underline{X}\\) es el vector formado por las desviaciones típicas de sus componentes: \\[\n\\sigma(\\underline{X})=(\\sigma_1,\\ldots,\\sigma_p).\n\\]\n\n\n2.1.1 Covarianza\nLa covarianza de dos variables \\(X\\) e \\(Y\\) es una medida del comportamiento conjunto de estas dos variables.\nFormalmente, dadas dos variables aleatorias \\(X,Y\\) con medias \\(\\mu_X\\) y \\(\\mu_Y\\), respectivamente, su covarianza es \\[\n\\sigma_{X,Y}=E\\left((X-\\mu_X)\\cdot ( Y-\\mu_Y)\\right).\n\\] Es fácil comprobar que \\[\n\\sigma_{X,Y}=E(X\\cdot Y) -\\mu_X\\cdot \\mu_Y.\n\\]\nEn efecto, \\[\n\\begin{array}{rl}\n\\sigma_{X,Y} & =E((X-\\mu_X) ( Y-\\mu_Y))=\nE(XY-\\mu_XY-\\mu_YX+\\mu_X\\mu_Y)\\\\ &\n=E(XY)-\\mu_XE(Y)-\\mu_YE(X)+\\mu_X\\mu_Y\\\\ &=\nE(XY)-\\mu_X\\mu_Y-\\mu_Y\\mu_X+\\mu_X\\mu_Y\n=E(XY)-\\mu_X\\mu_Y\n\\end{array}\n\\]\nLa covarianza de \\(X\\) e \\(Y\\) puede tomar cualquier valor real (no como la varianza, que siempre es positiva), y mide el grado de variación conjunta de las variables en el sentido sigüiente:\n\n\\(\\sigma_{X,Y}&gt;0\\) significa que cuando \\(X\\) es más grande en un individuo 1 que en un individuo 2, \\(Y\\) tiende también a ser más gran en el individuo 1 que en el individuo 2.\n\n\n\n\n\n\n\n\n\n\n\n\\(\\sigma_{X,Y}&lt;0\\) significa que cuando \\(X\\) es más grande en un individuo 1 que en un individuo 2, \\(Y\\) tiende a ser más pequeña en el individuo 1 que en el individuo 2.\n\n\n\n\n\n\n\n\n\n\n\n\\(\\sigma_{X,Y}=0\\) significa que no hay ninguna tendencia en este sentido.\n\n\n\n\n\n\n\n\n\n\nEl signo de la covarianza refleja la “tendencia del crecimiento conjunto” de las variables:\n\nCovarianza positiva significa la misma tendencia: Si \\(X\\) aumenta, \\(Y\\) tiende a aumentar. Esto suele expresarse diciendo que hay asociación positiva entre \\(X\\) e \\(Y\\).\nCovarianza negativa significa la tendencia inversa: Si \\(X\\) aumenta, \\(Y\\) tiende a disminuir. Esto suele expresarse diciendo que hay asociación negativa entre \\(X\\) e \\(Y\\).\n\nSi \\(X\\) e \\(Y\\) son variables independientes, su covarianza es 0, porque en este caso \\(E(X\\cdot Y) =\\mu_X\\mu_Y\\) y por tanto \\[\n\\sigma_{X,Y}=E(X\\cdot Y) -\\mu_X\\cdot \\mu_Y=\\mu_X\\cdot \\mu_Y-\\mu_X\\cdot \\mu_Y=0.\n\\] Intuitivamente, si \\(X\\) e \\(Y\\) son independientes, significa que el hecho de que el valor de \\(X\\) aumente de un individuo a otro no tiene ningún efecto sobre el valor de \\(Y\\).\n¿Por qué, si \\(X\\) e \\(Y\\) son independientes, \\(E(X\\cdot Y) =\\mu_X\\mu_Y\\)?\nOs lo demostraremos en el caso discreto; el argumento en el caso continuo es lo mismo cambiando sumatorios por integrales. \\[\n\\begin{array}{rl}\nE(X\\cdot Y)\\!\\!\\! &\\displaystyle =\\sum_{x\\in D_X,y\\in D_Y} xyP(X=x,Y=y)\\\\\n&\\displaystyle =\\sum_{x\\in D_X,y\\in D_Y} xyP(X=x)P(Y=y)\\\\\n&\\text{(por la independencia de $X$ e $Y$)}\\\\\n&\\displaystyle =\\Big(\\sum_{x\\in D_X}xP(X=x)\\Big)\\Big(\\sum_{y\\in D_Y} yP(Y=y)\\Big)=E(X)E(Y)\n\\end{array}\n\\]\nEs importante remarcar que la igualdad \\(E(X\\cdot Y) =\\mu_X\\mu_Y\\) es equivalente a la igualdad \\(\\sigma(X+Y)^2 =\\sigma(X)^2+\\sigma(Y)^2\\) que decíamos que satisfacen las variables independientes. En efecto \\[\n\\begin{array}{l}\n\\sigma(X+Y)^2 -(\\sigma(X)^2+\\sigma(Y)^2)\\\\\n\\quad = E((X+Y)^2)-E(X+Y)^2-(E(X^2)-E(X)^2+E(Y^2)-E(Y)^2)\\\\\n\\quad = E(X^2+2XY+Y^2)-(E(X)+E(Y))^2\\\\\n\\qquad\\qquad -E(X^2)+E(X)^2-E(Y^2)+E(Y)^2\\\\\n\\quad = E(X^2)+2E(XY)+E(Y^2)-E(X)^2-2E(X)E(Y)-E(Y)^2\\\\\n\\qquad\\qquad -E(X^2)+E(X)^2-E(Y^2)+E(Y)^2\\\\\n\\quad = 2E(XY)-2E(X)E(Y)=2(E(XY)-\\mu_X\\mu_Y)\n\\end{array}\n\\] y por tanto \\[\n\\sigma^2(X+Y) -(\\sigma^2(X)+\\sigma^2(Y))=0 \\Longleftrightarrow E(XY)-\\mu_X\\mu_Y=0\n\\]\nSi \\(X\\) e \\(Y\\) son variables independendientes, su covarianza es 0, pero la implicación al contrario es falsa: Dos variables aleatorias pueden tener covarianza 0 y no ser independientes.\nVeamos un ejemplo de este hecho:\nSupongamos que tenemos un dado tetraédrico no trucado con las caras marcadas con los valores -2, -1, 1 y 2. Sean \\(X\\) la variable aleatoria que consiste en llanzar el dado y anotar el resultado (la cara que queda en tierra), e \\(Y\\) la variable aleatoria que consite en lanzar el dado y anotar el cuadrado del resultado obtenido. Como las cuatro caras del dado son equiprobables, \\[\n\\begin{array}{l}\n\\displaystyle P(X=-2)=P(X=-1)=P(X=1)=P(X=2)=\\frac{1}{4}\\\\\n\\displaystyle P(Y=1)=P(Y=4)=\\frac{1}{2}\n\\end{array}\n\\]\nComp \\(Y\\) es función de \\(X\\), ya que \\(Y=X^2\\), \\(X\\) e \\(Y\\) no pueden ser independientes. Veamos que, en efecto, no lo son. Observad que los únicos posibles valores para el vector \\((X,Y)\\) en una tirada del dado son (-2,4), (-1,1), (1,1) y (2,4), cada uno con probabilidad 1/4. Entonces, por ejemplo, la probabilidad de obtener en una tirada \\(X=-1\\) i \\(Y=4\\) es 0, porque es imposible, mientras que \\[\nP(X=-1)\\cdot P(Y=4)=\\frac{1}{4}\\cdot\\frac{1}{2}=\\frac{1}{8}\\neq 0.\n\\]\nVeamos ahora que la covarianza de \\(X\\) e \\(Y\\) es 0. Para calcularla, primero necesitamos calcular los valores esperados de las variables: \\[\n\\begin{array}{l}\n\\displaystyle \\mu_X=(-2)\\cdot \\frac{1}{4}+(-1)\\cdot \\frac{1}{4}+1\\cdot \\frac{1}{4}+2\\cdot \\frac{1}{4}=0\\\\\n\\displaystyle \\mu_Y=1\\cdot \\frac{1}{2}+4\\cdot \\frac{1}{2}=2.5\n\\end{array}\n\\] Por tanto \\[\n\\begin{array}{l}\n\\sigma_{X,Y}=E\\big(X\\cdot Y\\big)-\\mu_X\\cdot \\mu_Y=E\\big(X\\cdot Y\\big)-0\\cdot 2.5=E\\big(X\\cdot Y\\big)\\\\\n\\qquad =P\\big(X=-2,Y=4\\big)\\cdot (-2\\cdot 4)+P\\big(X=-1,Y=1\\big)\\cdot (-1\\cdot 1)\\\\\n\\qquad\\qquad\\qquad +P\\big(X=1,Y=1\\big)\\cdot (1\\cdot 1)+P\\big(X=2,Y=4\\big)\\cdot (2\\cdot 4)\\\\\n\\qquad =\\displaystyle \\frac{1}{4}\\cdot (-8)+\\frac{1}{4}\\cdot (-1)+\\frac{1}{4}\\cdot 1+\\frac{1}{4}\\cdot 8=0.\n\\end{array}\n\\] Así pues, \\(X\\) e \\(Y\\) son variables dependientes, pero su covarianza es 0.\nDos propiedades importantes más de la covarianza:\n\nLa covarianza es simétrica: \\[\n\\begin{array}{rl}\n\\sigma_{X,Y}\\!\\!\\! & =E((X-\\mu_X)\\cdot ( Y-\\mu_Y))\\\\\n& =E(( Y-\\mu_Y)\\cdot (X-\\mu_X))=\\sigma_{Y,X}\n\\end{array}\n\\]\nLa covarianza de una variable aleatoria con ella misma es su varianza: \\[\n\\sigma_{X,X}=E((X-\\mu_X)^2)=\\sigma^2(X)\n\\]\n\nLa matriz de covarianzas de un vector aleatorio \\(\\underline{X}=(X_1,\\ldots,X_p)\\) es la matriz formada por las covarianzas de los pares de variables que la formen: \\[\n\\sigma_{\\underline{X},\\underline{X}}=\\begin{pmatrix} \\sigma_{X_1,X_1} & \\sigma_{X_1,X_2} & \\ldots & \\sigma_{X_1,X_p}\\\\\n\\sigma_{X_2,X_1} & \\sigma_{X_2,X_2} & \\ldots & \\sigma_{X_2,X_p}\\\\\n\\vdots & \\vdots &\\ddots  & \\vdots\\\\\n\\sigma_{X_p,X_1} & \\sigma_{X_p,X_2} & \\ldots & \\sigma_{X_p,X_p}\\\\\n\\end{pmatrix}\n\\]\nEsta matriz es simétrica y las entradas de la diagonal son las varianzas de las variables del vector, porque \\(\\sigma_{X_i,X_i}=\\sigma^2_{X_i}\\).\n\n\n2.1.2 Correlación\nComo hemos dicho, el signo de la covarianza tiene una interpretación sencilla, puesto que refleja la tendencia del crecimiento conjunto de las variables. Pero, su magnitud no tiene una interpretación sencilla.\nComo alternativa, se puede medir la tendencia de que haya una relación lineal entre dos variables aleatorias continuas empleando el llamado coeficiente de correlación lineal de Pearson (o, para abreviar, la correlación), que viene a ser una versión normalizada de la covarianza.\nEn concreto, la correlación de las variables \\(X\\) e \\(Y\\) se define como el cociente de su covarianza entre el producto de sus desviaciones típicas: \\[\n\\rho_{X,Y}=\\frac{\\sigma_{X,Y}}{\\sigma_{X} \\sigma_{Y}}\n\\]\nLa correlación tiene las propiedades importantes siguientes:\n\nNo tiene unidades (porque las unidades de \\(\\sigma_X\\) son las de \\(X\\), las unidades de \\(\\sigma_Y\\) son las de \\(Y\\), y las unidades de \\(\\sigma_{X,Y}\\) son las de \\(X\\) por las de \\(Y\\))\nToma valores entre -1 y 1: \\(-1\\leqslant \\rho_{X,Y}\\leqslant 1\\)\nEs simétrica, \\(\\rho_{X,Y}= \\rho_{Y,X}\\)\nLa correlación de una variable con ella misma es 1: \\(\\rho_{X,X}=1\\)\n\\(\\rho_{X,Y}=\\pm 1\\) si, y solo si, las variables \\(X,Y\\) tienen una relación lineal perfecta. Es decir, \\(\\rho_{X,Y}=\\pm 1\\) si, y solo si, existen \\(a,b\\in \\mathbb{R}\\) con \\(a\\neq 0\\) y tales que \\(Y= X+b\\). La pendiente \\(a\\) de esta recta tiene el mismo signo que \\(\\rho_{X,Y}\\).\nCuanto más se acerca \\(|\\rho_{X,Y}|\\) a 1, más se acerca \\(Y\\) a ser función lineal de \\(X\\).\n\n\nSi \\(\\rho_{X,Y}&gt;0\\), la función es creciente\nSi \\(\\rho_{X,Y}&lt;0\\), la función es decreciente\n\n\nSi \\(\\rho_{X,Y}=0\\), decimos que las variables \\(X\\) e \\(Y\\) son incorreladas. Notamos que la correlación es 0 si, y solo si, la covarianza es 0. Por lo tanto, si \\(X\\) y \\(Y\\) son independientes, también son incorreladas. El recíproco en general es falso.\n\nSi una de las dos variables tiene desviación típica 0, en la fórmula de la correlación aparece un 0 en el denominador y no la podemos calcular. En este caso, se toma \\(\\rho_{X,Y}=0\\). El motivo intuitivo es que una variable constante es siempre independiente de cualquier otra variable (haga lo que haga la otra, siempre toma el mismo valor) y hemos quedado que las variables independientes son incorreladas.\nLa matriz de correlaciones de un vector aleatorio \\(\\underline{X}=(X_1,\\ldots,X_p)\\) es la matriz formada por las correlaciones de pares de sus variables: \\[\n\\rho(\\underline{X})\n=\\begin{pmatrix} 1 & \\rho_{X_1,X_2} & \\ldots & \\rho_{X_1,X_p}\\\\\n\\rho_{X_2,X_1} & 1 & \\ldots & \\rho_{X_2,X_p}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n\\rho_{X_p,X_1} & \\rho_{X_p,X_2} & \\ldots & 1\\\\\n\\end{pmatrix}.\n\\] Esta matriz es simétrica por la simetría de la correlación.\nLa correlación de Pearson de dos variables continuas mide la tendencia de las variables a variar conjuntamente de manera lineal. En particular, por ejemplo, si \\(\\rho_{X,Y}&gt;0\\), \\(Y\\) tiende a crecer cuando \\(X\\) crece. Pero esto no significa que un aumento del valor de \\(X\\) cause que el valor de \\(Y\\) tienda a aumentar:\n\nCorrelación no implica causalidad!\n\nLa tendencia al crecimiento simultáneo de \\(X\\) y \\(Y\\) se puede deber a una tercera variable que las haga crecer las dos, o puede ser puramente espuria.\nSi entráis a la página web Spurious Correlations podréis explorar un montón de correlaciones espurias. Nuestra preferida es una correlación de 0.947 entre la variable \\(X\\)= “Tomo un año y anoto el consumo per capita* de queso en los EE. UU.” e \\(Y\\)= “Tomo un año y anoto el número de muertes por estrangulamiento accidental con las sábanas de la cama en los EE. UU.”.\n\n\n\n\n\nHay un ejemplo de correlación negativa que es importante tener presente para no dejarse engañar.\nTeorema: Si \\(X_1\\), \\(X_2\\) son dos copias independientes de una misma variable aleatoria \\(X\\), \\[\n\\rho_{X_1,X_2-X_1}=-\\frac{1}{\\sqrt{2}}\\approx -0.71\n\\]\nEsto nos dice que, sea cual sea la variable \\(X\\), si la medimos en dos momentos independientes o sobre dos individuos elegidos de manera independiente, la diferencia entre los dos valores tiene una tendencia destacada a decrecer linealmente en el primer valor. Por ejemplo:\n\nHacéis un test y sacáis una nota muy baja (\\(X_1\\)). El día siguiente hacéis otro test similar (\\(X_2\\)) sin haber estudiado más. Lo más probable es que, por puro azar, saques una nota más alta (que \\(X_2-X_1\\) sea grande, por lo tanto positivo).\nHacéis un test y sacáis una nota muy alta (\\(X_1\\)). El día siguiente hacéis otro test (\\(X_2\\)) sin haber estudiado más. Lo más probable es que, por puro azar, saques una nota más baja (que \\(X_2-X_1\\) sea pequeño, por lo tanto negativo).\n\nPor si alguno necesita una demostración del teorema anterior, recordamos que \\[\n\\rho_{X_1,X_2-X_1}=\\dfrac{\\sigma_{X_1,X_2-X_1}}{\\sigma_{X_1}\\sigma_{X_2-X_1}}\n\\] Ahora \\[\n\\begin{array}{l}\n\\sigma_{X_2-X_1}=\\sqrt{\\sigma^2_{X_2-X_1}}\\\\[2ex]\n\\quad =\\sqrt{\\sigma^2_{X_1}+\\sigma^2_{X_2}}\\ \\text{(porque son independientes)}\\\\[2ex]\n\\quad =\\sqrt{\\sigma^2_{X}+\\sigma^2_{X}}\\ \\text{(porque $X_1,X_2$ son copias de $X$)}\\\\[2ex]\n\\quad =\\sqrt{2\\sigma^2_{X}}=\\sigma_{X}\\sqrt{2}\n\\end{array}\n\\] Luego, \\[\n\\begin{array}{l}\n\\sigma_{X_1,X_2-X_1}=E(X_1(X_2-X_1))-E(X_1)E(X_2-X_1)\\\\[1ex]\n\\quad =E(X_1X_2-X_1^2)-E(X_1)(E(X_2-E(X_1))\\\\[1ex]\n\\quad =E(X_1X_2)-E(X_1^2)-E(X_1)E(X_2)+E(X_1)E(X_1)\\\\[1ex]\n\\quad =E(X_1)E(X_2)-E(X_1^2)-E(X_1)E(X_2)+E(X_1)E(X_1)\\\\[1ex]\n\\quad \\text{(porque  $X_1,X_2$ son independents)}\\\\[1ex]\n\\quad =-E(X_1^2)+E(X_1)E(X_1)=-\\sigma^2_{X_1}=-\\sigma^2_{X}\n\\end{array}\n\\]\nCombinando lo anterior: \\[\n\\rho_{X_1,X_2-X_1}=\\dfrac{\\sigma_{X_1,X_2-X_1}}{\\sigma_{X_1}\\sigma_{X_2-X_1}}=\\dfrac{-\\sigma^2_{X}}{\\sigma_{X}\\cdot \\sigma_{X}\\sqrt{2}} =-\\frac{1}{\\sqrt{2}}\n\\]\nComo ejemplo, generaremos una muestra \\(X\\) de 101 “notas” aleatorias entre 0 y 100 con distribución binomial \\(B(100,0.5)\\). Tomaremos como \\(X_1\\) el vector de las primeras 100 notas, \\[\nX_1=(x_1,x_2,\\ldots,x_{100})\n\\] y como \\(X_2-X_1\\) el vector de las diferencias de cada nota \\(x_y\\), \\(y\\geqslant 2\\), con la anterior: \\[\nX_2-X_1=(x_2-x_1,x_3-x_2,\\ldots,x_{101}-x_{100}).\n\\] Calcularemos la correlación entre \\(X_1\\) y \\(X_2-X_1\\), y lo ilustraremos con un gráfico.\n\nX=rbinom(101,100,0.5)\nX1=X[-101]\nX2.menos.X1=diff(X)\nplot(X1,X2.menos.X1,pch=20,xlab=expression(X[1]),\n     ylab=expression(X[2]-X[1]))\n\n\n\n\n\n\n\ncor(X1,X2.menos.X1)\n\n[1] -0.7068097\n\n\nLa correlación predicha por el teorema anterior es\n\n-1/sqrt(2) \n\n[1] -0.7071068",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Análisis Multivariante</span>"
    ]
  },
  {
    "objectID": "t2_em.html#estadística-descriptiva-muestras",
    "href": "t2_em.html#estadística-descriptiva-muestras",
    "title": "2  Análisis Multivariante",
    "section": "2.2 Estadística descriptiva: Muestras",
    "text": "2.2 Estadística descriptiva: Muestras\n\n2.2.1 Covarianza\nSean \\(X=(x_1,\\ldots,x_n)\\) y \\(Y=(y_1,\\ldots,y_n)\\) dos vectores obtenidos de mediciones de dos variables aleatorias cuantitativas sobre una misma muestra ordenada de individuos de tamño \\(n\\) de una población. Sean \\(\\overline{X}\\) y \\(\\overline{Y}\\) sus medias muestrales. Entonces su covarianza muestral es \\[\n\\widetilde{S}_{X,Y} =\\frac{1}{n-1} \\sum_{i =1}^n\\big((x_{i}-\\overline{{X}})(y_i-\\overline{Y})\\big)\n\\] y su covarianza (a secas) es \\[\n{S}_{X,Y} =\\frac{1}{n} \\sum_{i =1}^n\\big((x_{i}-\\overline{{X}})(y_i-\\overline{Y})\\big)=\\frac{n-1}{n}\\widetilde{S}_{X,Y}.\n\\] Es decir, la diferencia entre la versión “muestral” y la versión “a secas” recae en el denominador, \\(n-1\\) y \\(n\\) respectivamente.\n\nLa covarianza de dos vectores solo tiene sentido cuando estos vectores representan los valores de dos variables cuantitativas sobre los mismos individuos, o sobre dos muestras emparejadas, y en el mismo orden. En particular, los dos vectores tienen que tener la misma longitud.\n\nComo en el caso poblacional, la covarianza entre dos vectores mide la tendencia que tienen los datos a variar conjuntamente:\n\nCuando \\(\\widetilde{S}_{X,Y}&gt;0\\), si \\(x_i&gt;x_j\\) entonces \\(y_i\\) tiende a ser más grande que \\(y_j\\)\nCuando \\(\\widetilde{S}_{X,Y}&lt;0\\), si \\(x_i&gt;x_j\\) entonces \\(y_i\\) tiende a ser más pequeño que \\(y_j\\)\nCuando \\(\\widetilde{S}=0\\), no hay ninguna tendencia en este sentido\n\nEs fácil comprobar que:\n\nLas dos covarianzas son simétricas \\[\n\\widetilde{S}_{X,Y}=\\widetilde{S}_{Y,X},\\ {S}_{X,Y}={S}_{Y,X}\n\\]\nLa varianza de un vector es su covarianza con él mismo \\[\n\\widetilde{S}_{X,X}=\\widetilde{S}^2_{X},\\ {S}_{X,X}={S}^2_{X}.\n\\]\n\nEjemplo\nHemos medido el índice de masa corporal, BMI, y el nivel de colesterol en 5 individuos sanos. Guardamos los resultados en un dataframe y calculamos las medias:\n\nBMI= c(18.3,24.4,24.6,24.4,22.2,19.5)\nChol=c(170,202,215,218,210,210)\nDF=data.frame(BMI,Chol)\nmean(BMI)\n\n[1] 22.23333\n\nmean(Chol)\n\n[1] 204.1667\n\n\nEntonces la covarianza muestral de estos dos vectores es \\[\n\\begin{array}{l}\n\\dfrac{1}{5}\\Big((18.3-22.23)(170-204.17)+(24.4-22.23)(202-204.17)\\\\\n\\qquad +(24.6-22.23)(215-204.17)+(24.4-22.23)(218-204.17)\\\\\n\\qquad +(22.2-22.23)(210-204.17)+(19.5-22.23)(210-204.17)\\Big)=33.8333\n\\end{array}\n\\]\nEntonces la covarianza muestral de estos dos vectores es \\[\n\\begin{array}{l}\n\\dfrac{1}{6}\\Big((18.3-22.23)(170-204.17)+(24.4-22.23)(202-204.17)\\\\\n\\qquad +(24.6-22.23)(215-204.17) +(24.4-22.23)(218-204.17)\\\\\n\\qquad +(22.2-22.23)(210-204.17)+(19.5-22.23)(210-204.17)\\Big)=28.1944\n\\end{array}\n\\]\nLa covarianza muestral de dos vectores numéricos de la misma longitud \\(n\\) se calcula en R con la función cov.\n\ncov(BMI,Chol)\n\n[1] 33.83333\n\n\nPara obtener su covarianza a secas, hay que multiplicar el resultado de cov por \\((n-1)/n\\).\n\nn=length(BMI)\ncov(BMI,Chol)*(n-1)/n\n\n[1] 28.19444\n\n\nConsideramos una tabla de datos cuantitativos de la forma \\[\n\\begin{array}{cccc}\nX_1 & X_2 & \\ldots & X_p\\\\ \\hline\nx_{1 1} & x_{1 2} &\\ldots & x_{1 p}\\\\\nx_{2 1} & x_{2 2} &\\ldots & x_{2 p}\\\\\n\\vdots & \\vdots   &   \\ddots    &\\vdots\\\\\nx_{n 1} & x_{n 2} &\\ldots & x_{n p}\n\\end{array}\n\\] donde cada columna representa los valores de cierta variable \\(X_i\\) y cada fila un individuo de una muestra de la población, de forma que la entrada \\(x_{ij}\\) de esta tabla es el valor de \\(X_j\\) sobre el individuo \\(i\\)-ésimo de la muestra.\nLa matriz de covarianzas muestrales de esta tabla es la matriz \\[\n\\widetilde{{S}}=\n\\begin{pmatrix}  \n\\widetilde{S}^2_{X_1} & \\widetilde{S}_{X_1,X_2} & \\ldots & \\widetilde{S}_{X_1,X_p}\\\\\n\\widetilde{S}_{X_2,X_1} &  \\widetilde{S}^2_{X_2} & \\ldots & \\widetilde{S}_{X_2,X_p}\\\\\n  \\vdots & \\vdots  &  \\ddots      & \\vdots\\\\\n\\widetilde{S}_{X_p,X_1} & \\widetilde{S}_{X_p,X_2} & \\ldots &  \\widetilde{S}^2_{X_p}\n\\end{pmatrix}\n\\] y la matriz de covarianzas (a secas) se define de manera similar, pero con las covarianzas a secas: \\[\n{S}=\n\\begin{pmatrix}  \nS^2_{X_1} & S_{X_1,X_2} & \\ldots & S_{X_1,X_p}\\\\\nS_{X_2,X_1} & S^2_{X_2} & \\ldots & S_{X_2,X_p}\\\\\n  \\vdots & \\vdots  &  \\ddots      & \\vdots\\\\\nS_{X_p,X_1} & S_{X_p,X_2} & \\ldots & S^2_{X_p}\n\\end{pmatrix}\n\\] Las dos son simétricas.\nLa matriz de covarianzas muestrales se calcula con la función cov aplicada a la matriz o al data frame de variables numéricas que almacena la tabla de datos. Para calcular la matriz de covarianzas a secas, se multiplica el resultado de cov por \\((n-1)/n\\), donde \\(n\\) es el número de filas de la tabla.\nEjemplo\nAñadiremos a los datos del ejemplo anterior una tercera variable con las edades de los 5 individuos.\n\nDF$Edad=c(49,39,50,44,40,36)\nDF\n\n   BMI Chol Edad\n1 18.3  170   49\n2 24.4  202   39\n3 24.6  215   50\n4 24.4  218   44\n5 22.2  210   40\n6 19.5  210   36\n\n\nLa matriz de covarianzas muestrales de esta tabla es\n\ncov(DF)\n\n           BMI      Chol   Edad\nBMI   7.586667  33.83333   1.14\nChol 33.833333 309.76667 -33.00\nEdad  1.140000 -33.00000  32.00\n\n\nPodréis observar que es simétrica, que la entrada (2,1) coincide con la covarianza de BMI y Chol que hemos calculado antes, y que en la diagonal obtenemos las varianzas muestrales de las variables de la tabla:\n\napply(DF,MARGIN=2,FUN=var)\n\n       BMI       Chol       Edad \n  7.586667 309.766667  32.000000 \n\n\n\n\n2.2.2 Correlación de Pearson\nSean \\(X=(x_1,\\ldots,x_n)\\) e \\(Y=(y_1,\\ldots,y_n)\\) dos vectores obtenidos midiendo dos variables aleatorias continuas sobre una misma muestra de individuos de medida \\(n\\) de una población.\nLa correlación de Pearson de \\(X\\) e \\(Y\\) es su covarianza muestral dividida por el producto de sus desviaciones típicas muestrales: \\[\nR_{X,Y}=\\frac{\\widetilde{S}_{X,Y}}{\\widetilde{S}_X\\cdot \\widetilde{S}_Y}.\n\\]\nLa correlación de Pearson de \\(X\\) e \\(Y\\) también es igual a su covarianza a secas dividida por el producto de sus desviaciones típicas a secas, porque los cambios de denominador se cancelan: \\[\nR_{X,Y}=\\frac{\\widetilde{S}_{X,Y}}{\\widetilde{S}_X\\cdot \\widetilde{S}_Y}=\n\\frac{\\frac{n}{n-1}\\cdot {S}_{X,Y}}{\\sqrt{\\frac{n}{n-1}}\\cdot {S}_X \\cdot\\sqrt{\\frac{n}{n-1}}\\cdot{S}_Y}=\n\\frac{S_{X,Y}}{S_X \\cdot S_Y}=R_{X,Y}.\n\\]\nEjemplo\nVolvemos a la situación del ejemplo anterior. La covarianza muestral y las desviaciones típicas muestrales de los vectores BMI y Chol son\n\ncov(BMI,Chol)\n\n[1] 33.83333\n\nsd(BMI)\n\n[1] 2.75439\n\nsd(Chol)\n\n[1] 17.60019\n\n\ny por tanto su correlación de Pearson es \\[\nR_{BMI,Chol}=\\frac{33.833}{2.754\\cdot 17.6}= 0.698\n\\]\nAlgunas propiedades importantes de la correlación de Pearson:\n\nLa correlación de Pearson es simétrica: \\[\nR_{X,Y}=R_{Y,X}\n\\]\nLa correlación de Pearson toma valores solo entre -1 y 1: \\[\n-1\\leqslant R_{X,Y}\\leqslant 1\n\\]\nLa correlación de Pearson de un vector con él mismo es 1: \\[\nR_{X,X}=1\n\\]\n\\(R_{X,Y}\\) tiene el mismo signo que \\(S_{X,Y}\\), y por tanto este signo tiene el mismo significado que a la covarianza:\nSi \\(R_{X,Y}&gt;0\\) y si \\(x_i&gt;x_j\\), \\(y_i\\) tiende a ser más grande que \\(y_j\\)\nSi \\(R_{X,Y}&lt;0\\) y si \\(x_i&gt;x_j\\), \\(y_i\\) tiende a ser más pequeño que \\(y_j\\)\nSi \\(R=0\\), no hay ninguna tendencia en este sentido\n\\(R_{X,Y}=\\pm 1\\) si, y solo si, todos los puntos \\((x_i,y_i)\\) están sobre una recta \\(y=ax+b\\) con \\(a\\neq 0\\). La pendiente \\(a\\) de esta relación lineal tiene el mismo signo que \\(R_{X,Y}\\). Por lo tanto, la recta es creciente si \\(R_{X,Y}=1\\) y decreciendo si \\(R_{X,Y}=- 1\\).\nEl coeficiente de determinación \\(R^2\\) de la regresión lineal por mínimos cuadrados de \\(Y\\) respecto de \\(X\\) es igual al cuadrado de su correlación de Pearson: \\[\nR^2=R_{X,Y}^2\n\\]\n\n\nPor lo tanto, cuanto más se acerca la correlación de Pearson de \\(X\\) e \\(Y\\) a 1 o a -1, más se acercan los puntos \\((x_i,y_i)\\) a estar sobre una recta. El signo de \\(R_{X,Y}\\) indica si esta recta es creciente (\\(R_{X,Y}&gt;0\\)) o decreciendo (\\(R_{X,Y}&lt;0\\)).\n\nComo en el caso poblacional, cuando uno de los vectores es constante, la correlación es igual a 0.\nCon R, la correlación de Pearson de dos vectores se puede calcular con la función cor. Por ejemplo, la correlación del Pearson de los vectores BMI y Chol se obtiene con\n\ncor(BMI,Chol)\n\n[1] 0.6979141\n\n\nVeamos que su cuadrado es igual al \\(R^2\\) de la regresión lineal de Chol en función de BMI:\n\ncor(BMI,Chol)^2\n\n[1] 0.487084\n\nsummary(lm(Chol~BMI))$r.squared\n\n[1] 0.487084\n\n\nPara hacernos una idea de qué representa este valor de la correlación, veamos el gráfico de los puntos (BMI,Chol) con su recta de regresión lineal:\n\nplot(BMI,Chol,pch=20)\nabline(lm(Chol~BMI),col=\"red\",lwd=1.5)\n\n\n\n\n\n\n\n\nPodemos observar como Chol tiende a crecer cuando BMI crece, pero los puntos (BMI,Chol) no tienden a estar sobre una recta.\nCuidado Es conveniente acompañar el cálculo de correlación o covarianza de dos vectores \\(x,y\\) con un gráfico de los puntos \\((x_i,y_i)\\), porque conjuntos muy diferentes de puntos pueden dar lugar a la misma correlación.\nUn ejemplo clásico de este hecho son los cuatro conjuntos de datos \\((x_{1,i},y_{1,i})_{i=1,\\ldots,11}\\), \\((x_{2,i},y_{2,i})_{i=1,\\ldots,11}\\), \\((x_{3,i},y_{3,i})_{i=1,\\ldots,11}\\), \\((x_{4,i},y_{4,i})_{i=1,\\ldots,11}\\) que forman el dataframe anscombe de R:\n\nstr(anscombe)\n\n'data.frame':   11 obs. of  8 variables:\n $ x1: num  10 8 13 9 11 14 6 4 12 7 ...\n $ x2: num  10 8 13 9 11 14 6 4 12 7 ...\n $ x3: num  10 8 13 9 11 14 6 4 12 7 ...\n $ x4: num  8 8 8 8 8 8 8 19 8 8 ...\n $ y1: num  8.04 6.95 7.58 8.81 8.33 ...\n $ y2: num  9.14 8.14 8.74 8.77 9.26 8.1 6.13 3.1 9.13 7.26 ...\n $ y3: num  7.46 6.77 12.74 7.11 7.81 ...\n $ y4: num  6.58 5.76 7.71 8.84 8.47 7.04 5.25 12.5 5.56 7.91 ...\n\n\nLas correlaciones de los cuatro pares de vectores son muy parecidas:\n\ncor(anscombe$x1,anscombe$y1)\n\n[1] 0.8164205\n\ncor(anscombe$x2,anscombe$y2)\n\n[1] 0.8162365\n\ncor(anscombe$x3,anscombe$y3)\n\n[1] 0.8162867\n\ncor(anscombe$x4,anscombe$y4)\n\n[1] 0.8165214\n\n\nPero si los dibujamos veréis que los cuatro conjuntos de puntos son muy diferentes:\n\nplot(anscombe$x1,anscombe$y1,pch=20,main=\"Conjunto de datos 1\",cex=1.25)\nabline(lm(y1~x1,data=anscombe),col=\"red\",lwd=1.5)\nplot(anscombe$x2,anscombe$y2,pch=20,main=\"Conjunto de datos 2\",cex=1.25)\nabline(lm(y2~x2,data=anscombe),col=\"red\",lwd=1.5)\nplot(anscombe$x3,anscombe$y3,pch=20,main=\"Conjunto de datos 3\",cex=1.25)\nabline(lm(y3~x3,data=anscombe),col=\"red\",lwd=1.5)\nplot(anscombe$x4,anscombe$y4,pch=20,main=\"Conjunto de datos 4\",cex=1.25)\nabline(lm(y4~x4,data=anscombe),col=\"red\",lwd=1.5)\n\n\n\n\n\n\n\n\n\n\nEjemplos más espectaculares se pueden obtener con las funciones del paquete datasaurus, que permiten crear conjuntos de puntos de “formas” diferentes con los mismos estadísticos, y en particular la misma correlación. Empleando este paquete, hemos creado dos pares de vectores de datos dino y star, que hemos recogido en la siguiente tabla de datos. Esta tabla de datos tiene tres variables: una variable dataset que indica el conjunto de datos, y las variables x e y que dan las coordenadas de los puntos que forman cada conjunto de datos. Comprobaremos que los dos pares de vectores de datos tienen el mismo coeficiente de correlación (al menos hasta la séptima cifra decimal) y los dibujaremos.\n\ndatasaure=read.table(\"https://raw.githubusercontent.com/AprendeR-UIB/MatesII/master/Dades/Datasaurus.txt\",header=TRUE,sep=\"\\t\")\nstr(datasaure)\n\n'data.frame':   284 obs. of  3 variables:\n $ dataset: chr  \"dino\" \"dino\" \"dino\" \"dino\" ...\n $ x      : num  55.4 51.5 46.2 42.8 40.8 ...\n $ y      : num  97.2 96 94.5 91.4 88.3 ...\n\ndino=datasaure[datasaure$dataset==\"dino\",2:3]\nstar=datasaure[datasaure$dataset==\"star\",2:3]\ncor(dino$x,dino$y)\n\n[1] -0.0629611\n\ncor(star$x,star$y)\n\n[1] -0.0629611\n\n\n\nplot(dino,pch=20,cex=1.25)\nplot(star,pch=20,cex=1.25)\n\n\n\n\n\n\n\n\n\n\nAhora supongamos que tenemos una tabla de datos numéricos de la forma \\[\n\\begin{array}{cccc}\nX_1 & X_2 & \\ldots & X_p\\\\ \\hline\nx_{1 1} & x_{1 2} &\\ldots & x_{1 p}\\\\\nx_{2 1} & x_{2 2} &\\ldots & x_{2 p}\\\\\n\\vdots & \\vdots   &   \\ddots    &\\vdots\\\\\nx_{n 1} & x_{n 2} &\\ldots & x_{n p}\n\\end{array}\n\\]\ndonde cada columna representa los valores de cierta variable \\(X_i\\) y cada fila un individuo de una muestra de la población, de forma que la entrada \\(x_{ij}\\) de esta tabla es el valor de \\(X_j\\) sobre el individuo \\(i\\)-ésimo de la muestra.\nSu matriz de correlaciones de Pearson es la matriz simétrica \\[\n\\begin{pmatrix}  \n1 & R_{X_1,X_2} & \\ldots & R_{X_1,X_p}\\\\\nR_{X_2,X_1} & 1 & \\ldots & R_{X_2,X_p}\\\\\n  \\vdots & \\vdots  &  \\ddots      & \\vdots\\\\\nR_{X_p,X_1} & R_{X_p,X_2} & \\ldots & 1\n\\end{pmatrix}\n\\]\nEsta matriz de correlaciones se calcula con la función cor aplicada a la matriz o el data frame de variables numéricas que almacena la tabla de datos. Por ejemplo, la matriz de correlaciones de Pearson de la tabla de datos DF del Ejemplo que venimos trabajando es\n\ncor(DF)\n\n            BMI       Chol        Edad\nBMI  1.00000000  0.6979141  0.07316517\nChol 0.69791406  1.0000000 -0.33145274\nEdad 0.07316517 -0.3314527  1.00000000\n\n\n\n\n2.2.3 Estimación\nLas covarianzas de dos vectores obtenidos midiendo dos variables \\(X,Y\\) sobre una muestra aleatoria simple de sujetos de una población estiman la covarianza poblacional de las variables \\(X,Y\\) que han producido los vectores:\n\nLa covarianza muestral \\(\\widetilde{S}_{X,Y}\\) siempre es un estimador no sesgado de la covarianza poblacional \\(\\sigma_{X,Y}\\)\nLa covarianza \\({S}_{X,Y}=\\frac{n-1}{n}\\widetilde{S}_{X,Y}\\) es un estimador sesgado de la covarianza poblacional \\(\\sigma_{X,Y}\\), con sesgo que tiende a 0, y es más eficiente que \\(\\widetilde{S}_{X,Y}\\)\nLa covarianza \\(S_{X,Y}\\) es el estimador máximo verosímil de \\(\\sigma_{X,Y}\\) cuando la distribución conjunta de las variables \\(X,Y\\) es la normal bivariante que estudiaremos próximamente.\n\nLa correlación de Pearson de dos vectores obtenidos midiendo dos variables continuas \\(X,Y\\) sobre una muestra aleatoria simple de sujetos de una población estima la correlación poblacional de las variables \\(X,Y\\). En concreto:\n\n\\(R_{X,Y}\\) es un estimador máximo verosímil de \\(\\rho_{X,Y}\\) cuando la distribución conjunta de \\(X,Y\\) se normal bivariante. se un estimador sesgado, pero su sesgo tiende a 0.\n\n\n\n2.2.4 Correlación de Spearman\nLa correlación de Pearson mide específicamente la tendencia de dos variables continuas a depender linealmente la una de la otra. Si no esperamos que esta dependencia lineal exista, o si nuestras variables son discretas o simplemente ordinales, emplear la correlación de Pearson para analizar la relación entre dos variables no es lo más adecuado. Entre las propuestas alternativas, la más popular es la correlación de Spearman.\nIntuitivamente, la correlación de Spearman mide la tendencia que si \\(x_i&gt;x_j\\), pase que \\(y_i&gt;y_j\\). Su valor es 1 si, para todos \\(y,j\\), \\[\nx_i&gt;x_j\\Longleftrightarrow y_i&gt;y_j\n\\] y su valor es -1 si, para todo \\(i,j\\), \\[\nx_i&gt;x_j\\Longleftrightarrow y_i&lt;y_j\n\\] Cuanto más se acerca la correlación de Spearman a 1 (o -1), para más parejas de índices \\((i,j)\\) tales que \\(x_i&gt;x_j\\) se tiene que \\(y_i&gt;y_j\\) (\\(y_i&lt;y_j\\), si se acerca a -1).\nFormalmente, la correlación de Spearman de dos vectores \\(X\\) e \\(Y\\) se define como la correlación de Pearson de los vectores de rangos de \\(X\\) e \\(Y\\). El vector de rangos de un vector \\(X\\) se obtiene sustituyendo cada valor de \\(X\\) por su posición en el vector ordenado de menor a mayor, y en caso de empates asignando a grupos de valores empatados la media de las posiciones que ocuparían. Por ejemplo, el vector de rangos de\n\\[\nx=(4,5,1,5,1,3,4,4)\n\\] es \\[\n(5,7.5,1.5,7.5,1.5,3,5,5)\n\\] ¿Cómo hemos calculado este vector?\n\nPrimero asignamos a cada valor del vector su posición si estuvieran ordenados de menor a mayor, y en caso de empate por ahora los ordenaremos de izquierda a derecha: \\[\n\\begin{array}{r|cccccccc}\nx & 4& 5 & 1 & 5 & 1 & 3 & 4 & 4\\\\ \\hline\n\\text{Posició} & 4 & 7 & 1 & 8 & 2 & 3 & 5 & 6\\\\\n\\end{array}\n\\]\nAhora, para asignar los rangos finales:\n\n\nEl rango de los dos elementos 1 de \\(x\\) es la media de las posiciones 1, 2 del vector ordenado: 1.5.\nComo que solo hay un 3 en \\(x\\), su rango es su posición en el vector ordenado: 3.\nEl rango de los tres elementos 4 es la media de las posiciones 4, 5 y 6 del vector ordenado: 5.\nFinalmente, el rango de los dos elementos 5 es la media de las posiciones 7, 8 del vector ordenado: 7.5.\n\n\\[\n\\begin{array}{r|cccccccc}\nx & 4& 5 & 1 & 5 & 1 & 3 & 4 & 4\\\\ \\hline\n\\text{Posició} & 4 & 7 & 1 & 8 & 2 & 3 & 5 & 6\\\\ \\hline\n\\text{Rang} & 5 & 7.5 & 1.5 & 7.5 & 1.5 & 3 & 5 & 5\n\\end{array}\n\\]\nPor cierto, con R el vector de rangos se calcula con la función rank:\n\nrank(c(4,5,1,5,1,3,4,4))\n\n[1] 5.0 7.5 1.5 7.5 1.5 3.0 5.0 5.0\n\n\nCon R, la correlación de Spearman se calcula directamente con la función cor especificando el parámetro method=\"spearman\". (El valor por defecto del parámetro method es \"pearson\" y por eso no lo indicamos cuando calculamos la correlación de Pearson.)\nEjemplo Consideremos los vectores BMI y Chol del ejemplo que venimos trabajando. Lo primero que haremos será calcular sus vectores de rangos:\n\\[\n\\begin{array}{|c|c||c|c|}\n\\hline\nBMI & \\text{Rangs} & Chol&  \\text{Rangs}\n\\\\\\hline\\hline\n18.3& 1 & 170& 1 \\\\\n24.4&4.5 & 202 & 2\\\\\n24.6&6 & 215& 5 \\\\\n24.4&4.5 & 218&  6\\\\\n22.2&3 & 210&  3.5\\\\\n19.5&2 & 210&  3.5\\\\\\hline\n\\end{array}\n\\]\nPor tanto, la correlación de Spearman de \\[\n\\mathit{BMI}=(18.3, 24.4, 24.6, 24.4, 22.2, 19.5)\\mbox{ i }\\mathit{Chol}=(170, 202, 215, 218, 210, 210)\n\\] es la correlación de Pearson de \\[\n(1, 4.5, 6, 4.5, 3, 2)\\mbox{ i }(1, 2, 5, 6, 3.5, 3.5)\n\\]\nComprobémoslo:\n\ncor(BMI,Chol,method=\"spearman\")\n\n[1] 0.6470588\n\ncor(c(1,4.5,6,4.5,3,2),c(1,2,5,6,3.5,3.5))\n\n[1] 0.6470588",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Análisis Multivariante</span>"
    ]
  },
  {
    "objectID": "t2_em.html#contrastes-de-correlación",
    "href": "t2_em.html#contrastes-de-correlación",
    "title": "2  Análisis Multivariante",
    "section": "2.3 Contrastes de correlación",
    "text": "2.3 Contrastes de correlación\nEn un contraste de correlación de dos variables poblacionales continuas \\(X\\) e \\(Y\\), la hipótesis nula es que no hay correlación entre las dos variables, lo cual traduce que no hay ninguna relación entre ellas. \\[\n\\left\\{\n\\begin{array}{ll}\nH_0: & \\rho_{XY}=0\\\\\nH_1: & \\rho_{XY}&gt; 0\\text{ o }\\rho_{XY}&lt; 0\\text{ o }\\rho_{XY}\\neq 0\n\\end{array}\\right.\n\\]\n\nSi en un contraste de correlación rechazamos la hipótesis nula, en particular concluimos que las variables \\(X\\) e \\(Y\\) son dependientes (porque si fueran independientes, su correlación seria 0).\n\nNo explicaremos cómo se hace a mano este contraste ni qué hipótesis tienen que satisfacer las variables poblacionales para que el resultado sea fiable. Si estáis interesados en el detalle, podéis consultar la correspondiente entrada de la Wikipedia. Simplemente tenéis que saber que se efectúa con la función cor.plot. Su sintaxis es similar a la de las otras funciones que efectúan contrastes.\nEjemplo Queremos contrastar si hay correlación positiva entre el BMI y el nivel de colesterol de un adulto sano, con un nivel de significación del 5%.\nVariables poblacionales de interés:\n\n\\(\\mathit{BMI}\\): “Tomamos un adulto sano y anotamos su BMI”\n\\(\\mathit{Chol}\\): “Tomamos un adulto sano y anotamos el nivel de colesterol en mg/l”\n\nContrast: \\[\n\\left\\{\n\\begin{array}{ll}\nH_0: & \\rho_{\\textit{BMI,Chol}}=0\\\\\nH_1: & \\rho_{\\textit{BMI,Chol}}&gt;0\n\\end{array}\\right.\n\\]\nEmpleamos las muestras de BMI y Chol del Ejemplo que venimos trabajando.\n\ncor.test(BMI,Chol,alternative=\"greater\")\n\n\n    Pearson's product-moment correlation\n\ndata:  BMI and Chol\nt = 1.949, df = 4, p-value = 0.06155\nalternative hypothesis: true correlation is greater than 0\n95 percent confidence interval:\n -0.08621998  1.00000000\nsample estimates:\n      cor \n0.6979141 \n\n\nConclusión: No hemos obtenido evidencia estadísticamente significativa que el BMI y el nivel de colesterol de un adulto sano tengan correlación positiva (test de correlación, p-valor 0.06, IC para \\(\\rho\\) 95% [-0.086 a 1]).\nCuidado La conclusión es que no hemos obtenido evidencia que el BMI y el nivel de colesterol tengan correlación positiva en la población de los adultos sanos. No en nuestra muestra, que sí que ha dado correlación positiva. Recordad que los contrastes siempre se refieren a la población.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Análisis Multivariante</span>"
    ]
  },
  {
    "objectID": "t2_em.html#gráficos-para-datos-multidimensionales",
    "href": "t2_em.html#gráficos-para-datos-multidimensionales",
    "title": "2  Análisis Multivariante",
    "section": "2.4 Gráficos para datos multidimensionales",
    "text": "2.4 Gráficos para datos multidimensionales\n\n2.4.1 Otro gráfico para datos bivariantes\nEl boxplot bivariante es un gráfico de dispersión que incluye dos elipses estimadas, la interior que contiene aproximadamente el 50% de los datos y la exterior que contiene aproximadamente el 95% de los datos. Este tipo de gráficos nos ayuda a localizar datos atípicos. Veamos un ejemplo con los datos de los pingüinos.\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.3.3\n\n\nWarning: package 'forcats' was built under R version 4.3.3\n\nlibrary(MVA)\nlibrary(palmerpenguins)\n\na2&lt;- penguins %&gt;%\n  select(body_mass_g,bill_length_mm) %&gt;%\n  na.omit %&gt;% as.matrix()\n\nbvbox(a2,xlab = \"Peso del pingüino en gr\", \n           ylab = \"Longitud del pico en mm\",\n      pch = 19, cex = 1.25, col = \"red\")\n\n\n\n\n\n\n\n\nLas dos rectas dentro de las elipses del gráfico anterior son estimaciones de la recta de regresión.La recta más oscura es la habitual de mínimos cuadrados utilizando todas las observaciones. La recta más clara es una estimación más robusta que reduce la influencia de cualquier valor extremo.\n\n\n2.4.2 Matriz de dispersión\nEs posible ver los gráficos de dispersión por pares entre diversas variables cuantitativas utilizando una matriz. Se puede generar utilizando la función “pairs()” de R base. Veamos un ejemplo con nuestros datos de pingüinos.\n\na&lt;-penguins %&gt;%\n  select(3:7) %&gt;%\n  na.omit\n\npairs(a,\n      col = c(\"red\", \"blue\")[as.integer(a$sex)], \n      pch = 18)\n\n\n\n\n\n\n\n\nExisten muchas otras funciones para crear matrices de gráficos de dispersión. Una que nos gusta es ggpairs() del paquete GGally.\n\nlibrary(GGally)\nggpairs(a)\n\n\n\n\n\n\n\n\nPor defecto, la diagonal principal de la matriz contiene la curva de densidad para cada variable. Por debajo de la diagonal principal se muestran los gráficos de dispersión y la correlación (para las variables cuantitativas) o el cruce entre variables si se trata de una variable categórica.\n\n\n2.4.3 Caras de Chernoff\nCada variable del conjunto de datos se usa para representar una característica de la cara. Chernoff usó hasta 18 variables para representar diferentes rasgos faciales como cabeza, nariz, ojos, cejas, boca y orejas. El gráfico de Chernoff tiene como ventaja la facilidad humana para reconocer patrones de caras. El inconveniente es que la representación es muy dependiente de las variables escogidas para representar cada rasgo. Por ejemplo: la boca y la forma de la cabeza son rasgos más llamativos que las orejas o la longitud de la nariz, por tanto, el mismo conjunto puede sugerir distintos patrones de similitud entre las observaciones. Veamos un ejemplo.\n\nlibrary(\"aplpack\")\nb&lt;- penguins %&gt;% \n      filter(species==\"Adelie\", \n             island==\"Torgersen\",\n             sex==\"female\",\n             year==2007)\n\nfaces(b[,3:6],face.type = 1, scale =TRUE,print.info = TRUE)\n\n\n\n\n\n\n\n\neffect of variables:\n modified item       Var                \n \"height of face   \" \"bill_length_mm\"   \n \"width of face    \" \"bill_depth_mm\"    \n \"structure of face\" \"flipper_length_mm\"\n \"height of mouth  \" \"body_mass_g\"      \n \"width of mouth   \" \"bill_length_mm\"   \n \"smiling          \" \"bill_depth_mm\"    \n \"height of eyes   \" \"flipper_length_mm\"\n \"width of eyes    \" \"body_mass_g\"      \n \"height of hair   \" \"bill_length_mm\"   \n \"width of hair   \"  \"bill_depth_mm\"    \n \"style of hair   \"  \"flipper_length_mm\"\n \"height of nose  \"  \"body_mass_g\"      \n \"width of nose   \"  \"bill_length_mm\"   \n \"width of ear    \"  \"bill_depth_mm\"    \n \"height of ear   \"  \"flipper_length_mm\"\n\n\nAlternativamente, podemos representar cada elemento por una figura geométrica, donde las similitudes entre figuras indican las similitudes entre los elementos.\n\nstars(a[,1:4], key.loc = c(44, 1.5),cex=0.45,\n      labels=row.names(a[,1:4]), draw.segments=TRUE)\n\n\n\n\n\n\n\n\nPor supuesto que ggplot2 nos da opciones más modernas de este tipo de gráficos. Ver, por ejemplo:\n\nggradar\nfmsb library\n\n\n\n2.4.4 Matriz de correlaciones\nLas correlaciones por pares entre varias variables se muestran visualmente mediante un mapa de calor de la matriz de correlaciones. La función ggcorrplot() del paquete “ggcorrplot” de R puede utilizarse para construirlo. A continuación se muestra un ejemplo que utiliza el conjunto de datos de los pingüinos:\n\nlibrary(ggcorrplot)\npenguins %&gt;%\n  select(3:6) %&gt;%\n  na.omit(.) %&gt;% \n  cor(.) %&gt;% \n  ggcorrplot(., hc.order = TRUE,\n        type = \"lower\",\n        colors = c(\"#6D9EC1\",\n                    \"yellow\", \"#E46726\"))\n\n\n\n\n\n\n\n\nEn el gráfico, el color naranja denota correlaciones positivas y el gris correlaciones negativas. Al especificar hc.order,las variables también se ordenan.\n\n\n2.4.5 Gráficos de mosaico\nHasta ahora, hemos explorado métodos para visualizar relaciones entre variables cuantitativas. Pero, ¿qué hacemos si hay más de dos variables categóricas?\nUn método consiste en utilizar gráficos de mosaico, en los que las frecuencias de una tabla de contingencia multidimensional se representan mediante regiones rectangulares anidadas que son proporcionales a su frecuencia de celda. La función mosaicplot() de la librería vcd proporciona características más amplias que la de R base.\nSi se añade la opción opción shade=TRUE colorea la figura basándose en los residuos estandarizados de un modelo loglineal para la tabla. La idea central detrás de un modelo loglineal es modelar las probabilidades de ocurrencia de cada categoría en la tabla de contingencia y el modelo se ajusta utilizando técnicas de regresión logística.\n\nlibrary(vcd)\na&lt;- penguins %&gt;%\n  select(island,species,sex) %&gt;%\n  na.omit()\na2&lt;- table(a)\n\nmosaicplot(a2,shade=TRUE, main=\"\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Análisis Multivariante</span>"
    ]
  },
  {
    "objectID": "t2_em.html#práctica-3",
    "href": "t2_em.html#práctica-3",
    "title": "2  Análisis Multivariante",
    "section": "2.5 Práctica 3",
    "text": "2.5 Práctica 3\nDe las fuentes de datos que se citan abajo o de alguna otra fuente que sea de libre acceso, seleccionad un conjunto de datos que os guste y pensad en responder una pregunta a partir de estos. El conjunto de datos debe cumplir con las siguientes condiciones:\n\nQue contenga al menos un par de variables cualitativas (una ordinal y otra nominal).\nQue contenga al menos cinco variables cuantitativas, cuantas más mejor.\nSi tenéis que unir varias tablas de datos, usad las herramientas que hemos presentado de tidyverse.\nAlgunas fuentes de datos:\n\nCompendium of Data Sources for Data Science, Machine Learning, and Artificial Intelligence.\ndatos.gob.es\nIBESTAT\nDatasets 2023 Makeover Mondey\n\n\n\nCread un repositorio en Github para vuestro grupo con un nombre que sea fácilmente identificable para los profesores de la asignatura. Cread un proyecto nuevo en RStudio conectado al repositorio anterior que contenga los documentos/scripts y presentación del proyecto.\nEl proyecto debe incluir:\n\nResumen del problema que trataréis, su contexto y la fuente de datos. Describid cada una de las variables que conforman la base de datos del estudio. Plantead una pregunta u objetivo del proyecto.\nTibble o dataframe con la base de datos limpia. Indicad los pasos e instrucciones que habéis utilizado para lograr convertir vuestros datos en tidy data (si aplica).\nUn análisis multivariante exploratorio de los datos con visualizaciones apropiadas y su interpretación en el contexto del problema.\nLa estimación del vector de medias y la matriz de covarianza/correlación de las variables cuantitativas, así como un contraste de correlación.\nEscribid una conclusión con los resultados importantes del análisis.\nPreparad una presentación de máximo 15 minutos en la que cada miembro del grupo exponga una parte del trabajo.\nCada grupo tiene 15 minutos para exponer y 5 para responder las dudas y preguntas de los compañeros y de los profesores de la asignatura. La calificación se realizará por medio de la rúbrica publicada en Aula Digital que podéis encontrar en la sección “Bloque I”.\nEl peso de la evaluación es el siguiente: 0.8 \\(\\cdot\\) Nota de la Exposición + 0.2 \\(\\cdot\\) Nota del repositorio$. Para calcular la nota de la Exposición, se tomará la calificación media de vuestros compañeros (excluidos los integrantes del propio grupo) y la asignada por los profesores. Si la diferencia de notas entre los profesores y los compañeros es mayor a 2 puntos, contará únicamente la nota de los profesores.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Análisis Multivariante</span>"
    ]
  },
  {
    "objectID": "t2_em.html#distancias",
    "href": "t2_em.html#distancias",
    "title": "2  Análisis Multivariante",
    "section": "2.6 Distancias",
    "text": "2.6 Distancias\nComo hemos mencionado, la información multivariante es una matriz de datos de orden \\(n \\times p\\), con \\(n\\) el número de filas y \\(p\\) el número de variables consideradas. A veces, para determinar cuáles observaciones multivariantes son semejantes y cuáles no, se emplean matrices de distancias o similaridades. Dependiendo del contexto, hay que seleccionar la más apropiada para obtener resultados confiables. A continuación, presentamos algunas de las distancias más utilizadas.\n\n2.6.1 Distancia euclídea\nLa distancia euclídea entre dos realizaciones \\(\\underline{X}=(x_1,\\ldots,x_p)\\) e \\(\\underline{Y}=(y_1,\\ldots,y_p)\\) se define como\n\\[d_E(\\underline{X},\\underline{Y})=\\sqrt{\\displaystyle\\sum_{k=1}^p (x_k-y_k)^2}\\]\nEsta distancia equivale a la suma de las longitudes de los segmentos que unen cada par de posiciones. La siguiente imagen muestra el perfil de dos usuarios en base a las valoraciones que han hecho de 10 ítems.\n\nlibrary(tidyverse)\nusuario_a &lt;- c(4, 4.5, 4, 7, 7, 6, 5, 5.5, 5, 6)\nusuario_b &lt;- c(4, 4.5, 4, 7, 7, 6, 5, 5.5, 5, 6) + 3\ndatos &lt;- data.frame(usuario = rep(c(\"a\", \"b\"), each = 10),\n                    valoracion = c(usuario_a, usuario_b),\n                    item = 1:10)\nggplot(data = datos, aes(x = as.factor(item), y = valoracion,\n                         colour = usuario)) +\n  geom_path(aes(group = usuario)) +\n  geom_point() +\n  geom_line(aes(group = item), colour = \"firebrick\", linetype = \"dashed\") +\n  labs(x = \"item\") +\n  theme_bw() + theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nLa distancia euclideana entre los dos usuarios es:\n\nsqrt(sum((usuario_a-usuario_b)^2))\n\n[1] 9.486833\n\n\nLas funciones dist() de R base y get_dist de la librería factoextra se pueden utilizar para calcular las distancias entre todas las filas (observaciones) de una matriz o data frame. Por defecto, las funciones devuelven una matriz triangular inferior.\nEjemplo: Para el data frame de los pingüinos, calculamos las distancias euclideanas entre las ocho hembras de la especie Adelie que fueron registradas en Torgensen el año 2007.\n\np_ATf&lt;- penguins %&gt;% \n  filter(species==\"Adelie\", \n         island==\"Torgersen\",\n         sex==\"female\",\n         year==2007) %&gt;% select(3:5)\n\ndist(p_ATf)\n\n          1         2         3         4         5         6         7\n2  9.055385                                                            \n3  7.774960  4.318565                                                  \n4  5.051732 14.071247 12.291867                                        \n5  4.312772 13.030733 11.968709  2.424871                              \n6  3.093542 10.664427  8.140025  4.614109  5.412024                    \n7  9.176056  1.886796  2.844293 14.052758 13.293607 10.288343          \n8  5.568662 12.488795  9.332738  5.441507  7.037755  2.489980 11.825819\n\nlibrary(factoextra)\nget_dist(p_ATf, stand=TRUE)\n\n         1        2        3        4        5        6        7\n2 1.819868                                                      \n3 3.293836 2.530682                                             \n4 1.077437 2.510238 3.181155                                    \n5 1.039894 2.343332 3.710845 1.048543                           \n6 1.443639 2.415665 2.600833 1.244814 2.113671                  \n7 2.840218 1.637506 1.061075 2.989495 3.228280 2.641200         \n8 2.750331 3.317954 2.286750 2.271394 3.259136 1.338307 2.850519\n\n\nLas distancias grandes indican mayores disimilitudes entre las observaciones.\nObserva que hemos calculado la distancia euclideana con la función get_dist pero estandarizando los datos especificando stand=TRUE, es decir, para cada variable (columna), restamos el valor medio de la columna y dividimos por la desviación típica de la columna, lo cual tiene sentido hacerlo cuando las variables del conjunto de datos están en escalas diferentes. El problema con la distancia euclideana es que no toma en cuenta la correlación entre las variables consideradas.\n\n\n2.6.2 Distancia de Minkowski\nOtra distancia que podría ser considerada es la de Minkowski, cuya definición es la siguiente:\n\\[d_{M_q}(\\underline{X}_i,\\underline{X}_j)=\\left(\\displaystyle\\sum_{k=1}^p|x_{ik}-x_{jk}|^q \\right)^{1/q}, \\;\\; q&gt;0\\] Presenta los mismos inconvenientes que \\(d_E\\) \\((d_E = d_{M_2})\\).\n\nSi \\(q=1\\), se le llama distancia de Manhattan\nSi \\(q \\rightarrow \\infty\\), se le llama distancia dominante \\[d_{M_\\infty}(\\underline{X}_i,\\underline{X}_j)=\\max\\{ |x_{i1}-x_{j1}|,\\ldots,|x_{ip}-y_{jp}|\\}\\]\n\nLa siguiente imagen muestra una comparación entre la distancia euclídea (segmento azul) y la distancia de Manhattan (segmento rojo y verde) en un espacio bidimensional. Existen múltiples caminos para unir dos puntos con el mismo valor de distancia de Manhattan, ya que su valor es igual al desplazamiento total en cada una de las dimensiones.\n\ndatos &lt;- data.frame(observacion = c(\"a\", \"b\"), x = c(2,7), y = c(2,7))\nmanhattan &lt;- data.frame(\n              x = rep(2:6, each = 2),\n              y = rep(2:6, each = 2) + rep(c(0,1), 5),\n              xend = rep(2:6, each = 2) + rep(c(0,1), 5),\n              yend = rep(3:7, each = 2))\n\nmanhattan_2 &lt;- data.frame(\n                x = c(2, 5, 5, 7),\n                y = c(2, 2, 4, 4),\n                xend = c(5, 5, 7, 7),\n                yend = c(2, 4, 4, 7))\n\nggplot(data = datos, aes(x = x, y = y)) +\ngeom_segment(aes(x = 2, y = 2, xend = 7, yend = 7), color = \"blue\", size = 1.2) +\ngeom_segment(data = manhattan, aes(x = x, y = y, xend = xend, yend = yend),\n             color = \"red\", size = 1.2) +\ngeom_segment(data = manhattan_2, aes(x = x, y = y, xend = xend, yend = yend),\n             color = \"green3\", size = 1.2) +\ngeom_point(size = 3) +\ntheme(panel.grid.minor = element_blank(),\n      panel.grid.major = element_line(size = 2),\n      panel.background = element_rect(fill = \"gray\",\n                                      colour = \"white\",\n                                      size = 0.5, linetype = \"solid\"))\n\n\n\n\n\n\n\n\n\n\n2.6.3 Distancias basadas en la correlación\nLas distancias basadas en la correlación son útiles utilizadas cuando la similitud queremos establecerla en términos del patrón o forma y no de desplazamiento o magnitud; por ejemplo en el análisis de datos de expresión génica.\n\\[d_{cor}(\\underline{X}_i,\\underline{X}_j)=1-Cor(\\underline{X}_i,\\underline{X}_j)\\]\ndonde la correlación puede ser de distintos tipos (Pearson, Spearman, Kendall…)\nEn la siguiente imagen mostramos el perfil de 3 usuarios. Veamos qué ocurre si utilizamos la distancia euclideana y la de correlación de Pearson para compararlos.\n\nusuario_a &lt;- c(4, 4.5, 4, 7.5, 7, 6, 5, 5.5, 5, 6)\nusuario_b &lt;- c(4, 4.5, 4, 7.5, 7, 6, 5, 5.5, 5, 6) + 4\nusuario_c &lt;- c(5, 5.5, 4.8, 5.4, 4.7, 5.6, 5.3, 5.5, 5.2, 4.8)\n\ndatos &lt;- data.frame(usuario = rep(c(\"a\", \"b\", \"c\"), each = 10),\n                    valoracion = c(usuario_a, usuario_b, usuario_c),\n                    item = 1:10)\n\nggplot(data = datos, aes(x = as.factor(item),\n                         y = valoracion, \n                         colour = usuario)) +\n  geom_path(aes(group = usuario)) +\n  geom_point() +\n  labs(x = \"item\") +\n  theme_bw() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\ndist(x = rbind(usuario_a, usuario_b, usuario_c), method = \"euclidean\")\n\n          usuario_a usuario_b\nusuario_b  12.64911          \nusuario_c   3.75100  13.98821\n\n1 - cor(x = cbind(usuario_a, usuario_b, usuario_c), method = \"pearson\")\n\n          usuario_a usuario_b usuario_c\nusuario_a 0.0000000 0.0000000 0.9466303\nusuario_b 0.0000000 0.0000000 0.9466303\nusuario_c 0.9466303 0.9466303 0.0000000\n\n\nDe acuerdo a la distancia euclideana, los usuarios a y c son los más similares, mientras que acorde a la correlación de Pearson, a y b son los más parecidos. Este ejemplo pone de manifiesto que no existe una única medida de distancia que sea mejor que las demás, sino que, dependiendo del contexto, una será más adecuada que otra.\n\n\n2.6.4 Distancia de Mahalanobis\n\\[d^2_M \\left( \\underline{X}_i,\\underline{X}_j \\right)= \\left(\\underline{X}_i-\\underline{X}_j \\right)^t {\\bf S}^{-1}\n\\left(\\underline{X}_i-\\underline{X}_j\\right),\\]\ndonde \\({\\bf S}\\) es la matriz de covarianzas muestrales de la matriz de datos \\({\\bf X}\\).\nEs adecuada como medida de discrepancia entre datos, porque:\n\nEs invariante frente a transformaciones lineales invertibles de las variables.\n\\(d_E=d_M\\) cuando \\({\\bf S}={\\bf I}\\) y \\(d_K = d_M\\) cuando \\({\\bf S}=diag(s^2_1,\\ldots,s^2_p),\\)\nEsta distancia tiene en cuenta las correlaciones entre las variables.\nNo aumenta por el simple hecho de aumentar el número de variables registradas, sino que solamente aumentará cuando las nuevas variables no sean redundantes con respecto de la información aportada por las anteriores.\nEsta es la distancia más utilizada cuando todas las variables son cuantitativas.\n\n\n\n2.6.5 Distancias para variables binarias:\nSi \\(X_1,\\ldots,X_p\\) son variables que toman valores \\(\\{0,1\\}\\), existen muchos coeficientes de similaridad \\(s_{ij}\\) entre dos observaciones \\(i,j\\), calculados a partir de las frecuencias tales que \\(a+b+c+d=p\\), donde:\n\n\\(a\\): el número de variables con respuesta 1 en ambos individuos,\n\\(b\\): el número de variables con respuesta 0 en la observación \\(i\\) y con respuesta 1 en el individuo \\(j\\),\n\\(c\\): el número de variables con respuesta 1 en la observación \\(i\\) y con respuesta 0 en la observación \\(j\\),\n\\(d\\): el número de variables con respuesta 0 en ambos individuos.\n\nAlgunos coeficientes de similaridad son:\nSokal y Michener: \\(s_{ij}=\\frac{a+d}{p}\\), Jaccard: \\(s_{ij} =\\frac{a}{a+b+c}\\)\nAplicando uno de estos coeficientes a un conjunto de \\(n\\) objetos se obtiene una matriz de similaridades \\({\\bf S}=(s_{ij})_{n\\times n}\\)\nEjemplo: Se han medido 6 variables sobre 3 individuos:\n\n\n\nind.\n\\(X_1\\)\n\\(X_2\\)\n\\(X_3\\)\n\\(X_4\\)\n\\(X_5\\)\n\\(X_6\\)\n\n\n\n\n1\n1\n1\n0\n0\n1\n1\n\n\n2\n1\n1\n1\n0\n0\n1\n\n\n3\n1\n0\n0\n1\n0\n1\n\n\n\n\n\n\n\n\nLas matrices de similaridad son: \\[{\\bf S}_{Sokal}=\\left(\\begin{array}{ccc}\n1 & 0.6667 & 0.5\\\\\n0.667 & 1 & 0.5\\\\\n0.5 & 0.5 & 1\n\\end{array}\\right), \\;\\;\\; {\\bf S}_{Jaccard}=\\left(\\begin{array}{ccc}\n1 & 0.6 & 0.4\\\\\n0.6 & 1 & 0.4\\\\\n0.4 & 0.4 & 1\n\\end{array}\\right) \\]\n\n\n2.6.6 Distancias para variables categóricas:\nSe mide una variable categórica nominal con \\(k\\) niveles sobre una muestra de \\(n = n_1+\\cdots+n_g\\) observaciones provenientes de \\(g\\) poblaciones diferentes. Se desea obtener una medida de disimilaridad entre estas poblaciones.\nEn estas condiciones, el vector de frecuencias de cada población \\({\\bf n}_{\\alpha}=(n_{\\alpha1},\\ldots,n_{\\alpha k})\\), para \\(\\alpha=1,\\ldots, g\\), tiene una distribución conjunta multinomial con parámetros \\((n_{\\alpha}, {\\bf p}_{\\alpha})\\), donde \\(n_{\\alpha}=n_{\\alpha1}+\\ldots +n_{\\alpha k}\\) y \\({\\bf p}_{\\alpha}=(p_{\\alpha 1},\\ldots,p_{\\alpha k}\\) es el vector de probabilidades de los \\(k\\) niveles en la población \\(\\alpha\\) (con \\(p_{\\alpha 1}+\\ldots + p_{\\alpha k}=1\\)).\nEjemplo: La siguiente tabla contiene las proporciones génicas observadas entre 10 poblaciones. Observa que las filas suman 1.\n\n\n\nPoblación\ngrupo A\ngrupo AB\ngrupo B\ngrupo O\n\n\n\n\nfrancesa\n0.21\n0.06\n0.06\n0.67\n\n\ncheca\n0.25\n0.04\n0.14\n0.57\n\n\ngermánica\n0.22\n0.06\n0.08\n0.64\n\n\nvasca\n0.19\n0.04\n0.02\n0.75\n\n\nchina\n0.18\n0.00\n0.15\n0.67\n\n\nainu\n0.23\n0.00\n0.28\n0.49\n\n\nesquimal\n0.30\n0.00\n0.06\n0.64\n\n\nnegra USA\n0.10\n0.06\n0.13\n0.71\n\n\nespañola\n0.27\n0.04\n0.06\n0.63\n\n\negipcia\n0.21\n0.05\n0.20\n0.54\n\n\n\nDos medidas de disimilaridad para este tipo de variables son:\n\nDistancia de Bhattacharyya \\[d^2_{ij}=arccos\\left( \\displaystyle\\sum_{l=1}^k \\sqrt{p_{il} \\; p_{jl}}\\right)\\]\nDistancia de Balakrishnan-Sanghvi \\[d^2_{ij}= 2 \\displaystyle\\sum_{l=1}^k \\frac{(p_{il}-p_{jl})^2}{p_{il}+p_{jl}} \\]\n\nPara los datos del ejemplo, la matriz de distancias de Bhattacharyya es:\n\npoblacion=c(\"francesa\",\"checa\",\"germánica\",\"vasca\",\"china\",\"ainu\",\"esquimal\",\"negra USA\",\"española\",\"egipcia\")\ngrupoA=c(0.21,0.25,.22,.19,.18,.23,.3,.1,.27,.21)\ngrupoAB=c(.06,.04,.06,.04,.00,.00,.00,.06,.04,.05)\ngrupoB=c(.06,.14,.08,.02,.15,.28,.06,.13,.06,.2)\ngrupoO=c(.67,.57,.64,.75,.67,.49,.64,.71,.63,.54)\ndatos=data.frame(poblacion,grupoA,grupoAB,grupoB,grupoO)\n\n\nlibrary(philentropy)\ndistance(datos[,-1], method = \"bhattacharyya\")\n\n              v1          v2           v3          v4         v5         v6\nv1  0.0000000000 0.012324966 0.0009483429 0.007777165 0.04156128 0.08081738\nv2  0.0123249656 0.000000000 0.0066949713 0.035943482 0.02529075 0.03431210\nv3  0.0009483429 0.006694971 0.0000000000 0.013834658 0.03731750 0.06762516\nv4  0.0077771654 0.035943482 0.0138346582 0.000000000 0.05279229 0.11642553\nv5  0.0415612774 0.025290748 0.0373174959 0.052792291 0.00000000 0.01879186\nv6  0.0808173801 0.034312099 0.0676251647 0.116425530 0.01879186 0.00000000\nv7  0.0347713474 0.030975271 0.0343982089 0.034376078 0.01808704 0.04888218\nv8  0.0172057129 0.022070358 0.0157266766 0.032991841 0.03716560 0.07012655\nv9  0.0032054844 0.009334768 0.0030312958 0.011543973 0.03561201 0.06782787\nv10 0.0244823653 0.004028694 0.0160452869 0.057509566 0.03136123 0.02962590\n            v7         v8          v9         v10\nv1  0.03477135 0.01720571 0.003205484 0.024482365\nv2  0.03097527 0.02207036 0.009334768 0.004028694\nv3  0.03439821 0.01572668 0.003031296 0.016045287\nv4  0.03437608 0.03299184 0.011543973 0.057509566\nv5  0.01808704 0.03716560 0.035612005 0.031361231\nv6  0.04888218 0.07012655 0.067827866 0.029625898\nv7  0.00000000 0.06655156 0.020625956 0.052957776\nv8  0.06655156 0.00000000 0.030016960 0.020075979\nv9  0.02062596 0.03001696 0.000000000 0.024651208\nv10 0.05295778 0.02007598 0.024651208 0.000000000\n\n\nLas poblaciones más cercanas (según la distancia de Battacharyya) son la francesa y la germánica con \\(d_{1,3}=0.00095\\). Mientras que los más alejados son las poblaciones francesa y ainu con \\(d_{1,6}=0.08082\\).\n\n\n2.6.7 Distancias para variables mixtas\nCuando disponemos de un conjunto de observaciones sobre variables tanto cuantitativas como cualitativas.\nSe define la distancia de Gower como \\(d^2_{ij}=1-s_{ij}\\), donde \\[s_{ij}=\\frac{\\sum_{k=1}^{p_1}(1-|x_{ik}-x_{jk}|/G_k)+a+\\alpha}{p_1+(p_2-d)+p_3}\\] es el coeficiente de similaridad de Gower,\n\n\\(p_1\\) es el número de variables cuantitativas continuas,\n\\(p_2\\) es el número de variables binarias,\n\\(p_3\\) es el número de variables cualitativas (no binarias),\n\\(a\\) es el número de coincidencias (1,1) en las variables binarias,\n\\(d\\) es el número de coincidencias (0,0) en las variables binarias,\n\\(\\alpha\\) es el número de coincidencias en las variables cualitativas (no binarias) y\n\\(G_h\\) es el rango de la \\(k\\)-ésima variable cuantitativa.\n\nEjemplo: Considera 7 variables registradas sobre 50 jugadores de la liga española de fútbol.\n\n\n\n\n\n\n\\(X_1\\) es el número de goles marcados, \\(X_2\\) edad (en años),\n\\(X_3\\) altura (m), \\(X_4\\) peso (kg),\n\\(X_5\\) pierna buena del jugador (1=drecha, 0=izquierda),\n\\(X_6\\) nacionalidad: (1=Argentina, 2=Brasil, 3=Camerun, 4=Italia, 5=España, 6=Francia, 7=Uruguay, 8=Portugal, 9=Inglaterra).\n\\(X_7\\) tipo de estudios (1=sin estudios, 2=básicos, 3=medios, 4=superiores)\n\n\nfutbol=read.table(\"datos/jugadores.txt\")\nlibrary(StatMatch)\ndistancia=gower.dist(futbol)\nhead(distancia,2)\n\n          [,1]      [,2]      [,3]      [,4]      [,5]      [,6]      [,7]\n[1,] 0.0000000 0.2422439 0.4123377 0.3910534 0.3887085 0.2007576 0.3412698\n[2,] 0.2422439 0.0000000 0.3331530 0.3317100 0.2575758 0.3510101 0.1863276\n          [,8]      [,9]     [,10]     [,11]     [,12]     [,13]     [,14]\n[1,] 0.4585137 0.4370491 0.3645382 0.4132395 0.3329726 0.3098846 0.5128066\n[2,] 0.3555195 0.3181818 0.2334055 0.2229437 0.2736291 0.1747835 0.3419913\n         [,15]     [,16]     [,17]     [,18]     [,19]    [,20]     [,21]\n[1,] 0.3879870 0.3872655 0.3910534 0.3324315 0.3823954 0.333153 0.4880952\n[2,] 0.3088023 0.2323232 0.2321429 0.1659452 0.2869769 0.463925 0.3930375\n         [,22]     [,23]     [,24]     [,25]     [,26]     [,27]     [,28]\n[1,] 0.2436869 0.3890693 0.3311688 0.3154762 0.3551587 0.4462482 0.3059163\n[2,] 0.3715729 0.2341270 0.2718254 0.4462482 0.2521645 0.2754329 0.4366883\n         [,29]     [,30]     [,31]     [,32]     [,33]    [,34]     [,35]\n[1,] 0.3977273 0.1803752 0.2561328 0.3612915 0.3450577 0.247114 0.3481241\n[2,] 0.2427850 0.3309885 0.1369048 0.2063492 0.2500000 0.403860 0.4788961\n         [,36]     [,37]     [,38]     [,39]     [,40]     [,41]     [,42]\n[1,] 0.4202742 0.3151154 0.4197330 0.3250361 0.2247475 0.2023810 0.2676768\n[2,] 0.2783189 0.1969697 0.3405483 0.2061688 0.3786075 0.4446248 0.1484488\n         [,43]     [,44]     [,45]     [,46]     [,47]     [,48]     [,49]\n[1,] 0.1801948 0.3403680 0.3762626 0.1524170 0.4889971 0.3986291 0.2987013\n[2,] 0.4224387 0.2453102 0.2812049 0.3318903 0.3340548 0.2278139 0.1935426\n         [,50]\n[1,] 0.4224387\n[2,] 0.2516234\n\n\nCon la librería stats de R base puedes calcular la distancia: “euclidean”, “maximum”, “manhattan”, “canberra”, “binary” o “minkowski”.\nLa función distance()de la libreria philentropy es capaz de calcular 46 diferentes distancias/similaridades.\n\ngetDistMethods()\n\n [1] \"euclidean\"         \"manhattan\"         \"minkowski\"        \n [4] \"chebyshev\"         \"sorensen\"          \"gower\"            \n [7] \"soergel\"           \"kulczynski_d\"      \"canberra\"         \n[10] \"lorentzian\"        \"intersection\"      \"non-intersection\" \n[13] \"wavehedges\"        \"czekanowski\"       \"motyka\"           \n[16] \"kulczynski_s\"      \"tanimoto\"          \"ruzicka\"          \n[19] \"inner_product\"     \"harmonic_mean\"     \"cosine\"           \n[22] \"hassebrook\"        \"jaccard\"           \"dice\"             \n[25] \"fidelity\"          \"bhattacharyya\"     \"hellinger\"        \n[28] \"matusita\"          \"squared_chord\"     \"squared_euclidean\"\n[31] \"pearson\"           \"neyman\"            \"squared_chi\"      \n[34] \"prob_symm\"         \"divergence\"        \"clark\"            \n[37] \"additive_symm\"     \"kullback-leibler\"  \"jeffreys\"         \n[40] \"k_divergence\"      \"topsoe\"            \"jensen-shannon\"   \n[43] \"jensen_difference\" \"taneja\"            \"kumar-johnson\"    \n[46] \"avg\"",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Análisis Multivariante</span>"
    ]
  },
  {
    "objectID": "t2_em.html#visualización-de-matrices-de-distancias",
    "href": "t2_em.html#visualización-de-matrices-de-distancias",
    "title": "2  Análisis Multivariante",
    "section": "2.7 Visualización de matrices de distancias",
    "text": "2.7 Visualización de matrices de distancias\nUna solución sencilla para visualizar las matrices de distancia es utilizar la función fviz_dist() de la librería factoextra. Otros métodos especializados, como la agrupación jerárquica aglomerativa o el mapa de calor, las presentaremos más adelante.\nEjemplo: Visualizamos la tabla de los jugadores de fútbol.\n\nlibrary(factoextra)\nfviz_dist(as.dist(distancia))\n\n\n\n\n\n\n\n\nSi el color es rojo, la similitud es alta (es decir, distancia cero). Si el color es azul, la similitud es baja. El nivel de color es proporcional al valor de la distancia entre las observaciones. Se han ordenado a los jugadores por similitud.\nEjemplo: Otro ejemplo que podéis consultar son los mapas de calor que utilizamos en BIOCOM (grupo de investigación de la UIB en Biología Computacional y Bioinformática) para presentar las diferencias entre kernels (“distancias”) para comparar grafos del metabolismo entre organismos Figura 3 - Exploring the expressiveness of abstract metabolic networks\nObservamos una buena separación entre animales, plantas y hongos en todos los mapas de calor. Los protistas están separados, pero en general están más cerca de los hongos que del resto de Eucariotas. Además, en todos los mapas de calor, los animales están bien separados de los demás reinos y muestran un patrón interno adicional que merece un análisis más detallado. También cabe destacar que todos los kernels ponen de manifiesto una marcada diferencia entre los animales y las plantas. Las plantas están más cerca de los hongos que de los animales.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Análisis Multivariante</span>"
    ]
  },
  {
    "objectID": "t3_inferencia.html",
    "href": "t3_inferencia.html",
    "title": "3  Inferencia multivariante en poblaciones normales",
    "section": "",
    "text": "3.1 Distribuciones multivariantes",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inferencia multivariante en poblaciones normales</span>"
    ]
  },
  {
    "objectID": "t3_inferencia.html#distribuciones-multivariantes",
    "href": "t3_inferencia.html#distribuciones-multivariantes",
    "title": "3  Inferencia multivariante en poblaciones normales",
    "section": "",
    "text": "3.1.1 Distribución normal multivariante\n\n3.1.1.1 Definición\nSea \\(X\\) una variable aleatoria con distribución \\(N(\\mu,\\sigma^2)\\), es decir, con media \\(\\mu\\) y varianza \\(\\sigma^2\\). La función de densidad es:\n\\[f(x;\\mu,\\sigma^2)=\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}(x-\\mu)^2/\\sigma^2}=\\frac{(\\sigma^2)^{-1/2}}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}(x-\\mu)^2/\\sigma^2}. \\tag{3.1}\\]\nEvidentemente se verifica\n\\[X=\\mu+\\sigma Y\\quad\\text{siendo}\\quad Y\\sim N(0,1), \\tag{3.2}\\]\ndonde el símbolo \\(\\sim\\) significa “distribuido como”. Vamos a introducir la distribución normal multivariante \\(N_p(\\boldsymbol{\\mu},\\mathbf{\\Sigma})\\) como una generalización de la normal univariante. Por una parte, Ecuación 3.1 sugiere definir la densidad de \\(\\mathbf{X}=(X_1,\\ldots,X_p)'\\sim N_p(\\boldsymbol{\\mu},\\mathbf{\\Sigma})\\) según\n\\[f(\\mathbf{x};\\boldsymbol{\\mu},\\mathbf{\\Sigma})=\\frac{|\\mathbf{\\Sigma}|^{-1/2}}{\\left(\\sqrt{2\\pi}\\right)^p}e^{-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})'\\mathbf{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})}, \\tag{3.3}\\]\nsiendo \\(\\mathbf{x}=(x_1,\\ldots,x_p)'\\), \\(\\boldsymbol{\\mu}=(\\mu_1,\\ldots,\\mu_p)\\) y \\(\\mathbf{\\Sigma}=(\\sigma_{ij})\\) una matriz definida positiva, que como veremos, es la matriz de covarianzas. Por otra parte, Ecuación 3.2 sugiere definir la distribución \\(\\mathbf{X}=(X_1,\\ldots,X_p)'\\sim N_p(\\boldsymbol{\\mu},\\mathbf{\\Sigma})\\) como una combinación lineal de \\(p\\) variables \\(Y_1,\\ldots,Y_p\\) independientes con distribución \\(N(0,1)\\)\n\\[\\begin{align*}\n  X_1 & = & \\mu_1+a_{11}Y_1&+\\cdots+a_{1p}Y_p,\\\\\n  \\vdots & & & \\vdots \\\\\n  X_p & = & \\mu_p+a_{p1}Y_1&+\\cdots+a_{pp}Y_p,\n\\end{align*} \\tag{3.4}\\]\nque podemos escribir como\n\\[\\mathbf{X}=\\boldsymbol{\\mu}+\\mathbf{AY} \\tag{3.5}\\]\nsiendo \\(\\mathbf{Y}=(Y_1,\\ldots,Y_p)'\\) y \\(A=(a_{ij})\\) una matriz \\(p\\times p\\) que verifica \\(\\mathbf{AA'}=\\mathbf{\\Sigma}\\).\n\nProposición 3.1 Las dos definiciones Ecuación 3.3 y Ecuación 3.4 son equivalentes.\n\n\nPrueba. Según la fórmula del cambio de variable\n\\[f_X(x_1,\\ldots,x_p)=f_Y(y_1(x),\\ldots,y_p(x))\\left|\\frac{\\partial y}{\\partial x}\\right|,\\]\nSiendo \\(y_i=y_i(x_1,\\ldots,x_p),\\ i=1,\\ldots,p\\), el cambio y \\(J=\\frac{\\partial y}{\\partial x}\\) el jacobiano del cambio. De Ecuación 3.5 tenemos\n\\[\\mathbf{y}=\\mathbf{A}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\Rightarrow\\left|\\frac{\\partial y}{\\partial x}\\right|=\\left|\\mathbf{A}^{-1}\\right|\\]\ny como las \\(p\\) variable \\(Y_i\\) son \\(N(0,1)\\) independientes:\n\\[f_X(x_1,\\ldots,x_p)=\\left(1/\\sqrt{2\\pi}\\right)^pe^{-\\frac{1}{2}\\sum^p_{i=1}y_i^2}|\\mathbf{A}^{-1}|. \\tag{3.6}\\]\nPero \\(\\mathbf{\\Sigma}^{-1}=(\\mathbf{A}^{-1})'(\\mathbf{A}^{-1})\\) y por lo tanto\n\\[\\mathbf{y'y}=(\\mathbf{x}-\\boldsymbol{\\mu})'(\\mathbf{A}^{-1})'(\\mathbf{A}^{-1})(\\mathbf{x}-\\boldsymbol{\\mu})=(\\mathbf{x}-\\boldsymbol{\\mu})'(\\mathbf{\\Sigma}^{-1})(\\mathbf{x}-\\boldsymbol{\\mu}). \\tag{3.7}\\]\nSustituyendo Ecuación 3.7 en Ecuación 3.6 y de \\(|\\mathbf{A}|^{-1}=|\\mathbf{\\Sigma}|^{-1/2}\\) obtenemos Ecuación 3.3.\n\n\n\n3.1.1.2 Propiedades\n\nDe Ecuación 3.5 es inmediato que \\(E(\\mathbf{X})=\\boldsymbol{\\mu}\\) y que la matriz de covarianzas es\n\n\\[E[(\\mathbf{X}-\\boldsymbol{\\mu})(\\mathbf{X}-\\boldsymbol{\\mu})']=E(\\mathbf{AYY'A'})=\\mathbf{AI}_p\\mathbf{A'}=\\mathbf{\\Sigma}.\\]\n\nLa distribución de cada marginal \\(X_i\\) es normal multivariante:\n\n\\[X_i\\sim N(\\mu_i,\\sigma_{ii}),\\quad i=1,\\ldots,p.\\]\nEs consecuencia de la definición Ecuación 3.4.\n\nLa combinación lineal de todas las variables \\(X_1,\\ldots,X_p\\)\n\n\\[Z=b_0+b_1X_1+\\cdots+b_pX_p\\]\nes también normal univariante. En efecto, de Ecuación 3.4 resulta que \\(Z\\) es combinación lineal de \\(N(0,1)\\) independientes.\n\nSi \\(\\mathbf{\\Sigma}=diag(\\sigma_{11},\\ldots,\\sigma_{pp})\\) es matriz diagonal, es decir, \\(\\sigma_{ij}=0,i\\not=j\\), entonces las variables \\((X_1,\\ldots,X_p)\\) son estocásticamente independientes. En efecto, la función de densidad conjunta resulta igual al producto de las funciones de densidad marginales:\n\n\\[f(X_1,\\ldots,x_p;\\boldsymbol{\\mu},\\mathbf{\\Sigma})=f(   x_1;\\mu_1,\\sigma_{11})\\times\\cdots\\times f(x_p;\\mu_p,\\sigma_{pp})\\]\n\nLa distribución de la forma cuadrática\n\n\\[U=(\\mathbf{x}-\\boldsymbol{\\mu})'\\mathbf{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\]\nes ji-cuadrado con \\(p\\) grados de libertad. En efecto, de Ecuación 3.5 \\(U=\\mathbf{Y'Y}=\\sum_{i=1}^pY_i^2\\) es suma de los cuadrados de \\(p\\) variables \\(N(0,1)\\) independientes.\n\n\n3.1.1.3 Caso bivariante\nCuando \\(p=2\\), la función de densidad de la normal bivariante se puede expresar en función de medias y varianzas \\(\\mu_1,\\sigma^2_1,\\mu_2,\\sigma^2_2\\) y del coeficiente de correlación \\(\\rho=cor(X_1,X_2)\\):\n\\[\\begin{align*}\nf(x_1,x_2) & =\\frac{1}{2\\pi\\sigma_1\\sigma_2\\sqrt{1-\\rho^2}}\\\\\n& \\times exp\\left[-\\frac{1}{2}\\frac{1}{1-\\rho^2}\\left\\{\\frac{(x_1-\\mu_1)^2}{\\sigma^2_1}-2\\rho\\frac{(x_1-\\mu_1)}{\\sigma_1}\\frac{(x_2-\\mu_2)}{\\sigma_2}+\\frac{(x_2-\\mu_2)^2}{\\sigma^2_2}\\right\\}\\right],\n\\end{align*}\\]\nsiendo \\(-1&lt;\\rho&lt;+1\\) Figura 3.1. Se verifica:\n\nHay independencia estocástica si y sólo si \\(\\rho=0\\).\nLa distribución de la variable marginal \\(X_i\\) es \\(N(\\mu_i,\\sigma_i^2), i=1,2\\).\nLa función de densidad de \\(X_2\\) condicionada a \\(X_1=x_1\\) es\n\n\\[f(x_2|x_1)=\\frac{1}{\\sigma_2\\sqrt{2\\pi(1-\\rho^2)}}exp\\left\\{-\\frac{[x_2-\\mu_2-\\rho(\\sigma_2/\\sigma_1)(x_1-\\mu_1)]^2}{2\\sigma_2^2(1-\\rho^2)}\\right\\}\\]\ndensidad de la distribución normal \\(N(\\mu_2+\\rho(\\sigma_2/\\sigma_1)(x_1-\\mu_1),\\sigma_2^2(1-\\rho^2)).\\)\n\n\n\n\n\n\nFigura 3.1: Función de densidad de una distribución normal bivariante de medias 1 y 1, desviaciones típicas 2 y 2, coeficiente de correlación 0.8\n\n\n\n\nLa regresión es de tipo lineal, es decir, las curvas de regresión de la media\n\n\\[x_2=E(X_2|X_1=x_1),\\qquad x_1=E(X_1|X_2=x_2),\\]\nson las rectas de regresión.\n\n\n\n3.1.2 Distribución de Wishart\nLa distribución de Wishart es la que sigue una matriz aleatoria simétrica definida positiva, generaliza la distribución ji-cuadrado y juega un papel importante en inferencia multivariante. Un ejemplo destacado lo constituye la distribución de la matriz de covarianzas \\(\\mathbf{S}\\), calculada a partir de una matriz de datos donde las filas son observaciones normales multivariantes.\n\nDefinición 3.1 Si las filas de la matriz \\(\\mathbf{Z}_{n\\times p}\\) son independientes \\(N_p(\\mathbf{0},\\mathbf{\\Sigma})\\) entonces diremos que la matriz \\(\\mathbf{Q}=\\mathbf{Z'Z}\\) es Wishart \\(W_p(\\mathbf{\\Sigma},n)\\), con parámetros \\(\\mathbf{\\Sigma}\\) y \\(n\\) grados de libertad. Cuando \\(\\mathbf{\\Sigma}\\) es definida positiva y \\(n\\ge p\\), la densidad de \\(\\mathbf{Q}\\) es\n\\[f(\\mathbf{Q})=c|\\mathbf{Q}|^{(n-p-1)}exp\\left[-\\frac{1}{2}tr(\\mathbf{\\Sigma})^{-1}\\mathbf{Q}\\right],\\]\nsiendo\n\\[c^{-1}=2^{np/2}\\pi^{p(p-1)/4}|\\mathbf{\\Sigma}|^{n/2}\\prod_{i=1}^p\\Gamma\\left[\\frac{1}{2}(n+1-i)\\right].\\]\n\nPropiedades:\n\nSi \\(\\mathbf{Q}_1,\\mathbf{Q}_2\\) son independientes Wishart \\(W_p(\\mathbf{\\Sigma},m),W_p(\\mathbf{\\Sigma},n)\\), entonces la suma \\(\\mathbf{Q}_1+\\mathbf{Q}_2\\) también es Wishart \\(W_p(\\mathbf{\\Sigma},m+n)\\).\nSi \\(\\mathbf{Q}\\) es \\(W_p(\\mathbf{\\Sigma},n)\\), y separamos las \\(p\\) variables en dos conjuntos de \\(p_1\\) y \\(p_2\\) variables, y consideramos las particiones correspondientes de \\(\\mathbf{\\Sigma}\\) y \\(\\mathbf{Q}\\)\n\n\\[\\mathbf{\\Sigma}=\\left( \\begin{array}{cc}\n\\mathbf{\\Sigma}_{11} & \\mathbf{\\Sigma}_{12} \\\\\n\\mathbf{\\Sigma}_{21} & \\mathbf{\\Sigma}_{22}\n\\end{array} \\right),\\qquad\n\\mathbf{Q}=\\left( \\begin{array}{cc}\n\\mathbf{Q}_{11} & \\mathbf{Q}_{12} \\\\\n\\mathbf{Q}_{21} & \\mathbf{Q}_{22}\n\\end{array} \\right),\\]\nentonces \\(\\mathbf{Q}_{11}\\) es \\(W_{p_1}(\\mathbf{\\Sigma}_{11},n)\\) y \\(\\mathbf{Q}_{22}\\) es \\(W_{p_2}(\\mathbf{\\Sigma}_{22},n)\\).\n\nSi \\(\\mathbf{Q}\\) es \\(W_p(\\mathbf{\\Sigma},n)\\), y \\(\\mathbf{T}\\) es una matriz \\(p\\times q\\) de constantes, entonces \\(\\mathbf{T'QT}\\) es \\(W_q(\\mathbf{T'\\Sigma T}, n)\\). En particular, si \\(\\mathbf{t}\\) es un vector, entonces\n\n\\[\\frac{\\mathbf{t'Qt}}{\\mathbf{t'\\Sigma t}}\\sim\\chi_n^2.\\]\n\n\n3.1.3 Distribución de Hotelling\nIndiquemos por \\(F^m_n\\) la distribución F de Fisher-Snedecor, con \\(m\\) y \\(n\\) grados de libertad en el numerador y denominador, respectivamente. La distribución de Hotelling es una generalización multivariante de la distribución t de Student.\n\nDefinición 3.2 Si \\(\\mathbf{y}\\) es \\(N_p(\\mathbf{0}, \\mathbf{I})\\), independiente de \\(\\mathbf{Q}\\) que es Wishart \\(W_p(\\mathbf{I}, m)\\), entonces\n\\[T^2 = m\\mathbf{y'Q}^{-1}\\mathbf{y}\\]\nsigue la distribución \\(T^2\\) de Hotelling, que se indica por \\(T^2(p,m)\\).\n\nPropiedades:\n\nSi \\(\\mathbf{x}\\) es \\(N_p(\\boldsymbol{\\mu},\\mathbf{\\Sigma})\\) independiente de \\(\\mathbf{M}\\) que es \\(W_p(\\mathbf{\\Sigma}, m)\\), entonces\n\n\\[T^2 = m(\\mathbf{x}-\\boldsymbol{\\mu})'\\mathbf{M}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\sim T^2(p,m).\\]\n\n\\(T^2\\) está directamente relacionada con la distribución F de Fisher-Snedecor\n\n\\[T^2(p,m)\\equiv\\frac{mp}{m-p+1}F^p_{m-p+1}.\\]\n\nSi \\(\\mathbf{\\overline{x}}, \\mathbf{S}\\) son el vector de medias y la matriz de covarianzas de la matriz \\(\\mathbf{X}_{n\\times p}\\) con filas independientes \\(N_p(\\boldsymbol{\\mu},\\mathbf{\\Sigma})\\), entonces\n\n\\[(n-1)(\\mathbf{\\overline{x}}-\\boldsymbol{\\mu})'\\mathbf{S}^{-1}(\\mathbf{\\overline{x}}-\\boldsymbol{\\mu})\\sim T^2(p,n-1)\\]\ny por lo tanto\n\\[\\frac{n-p}{p}(\\mathbf{\\overline{x}}-\\boldsymbol{\\mu})'\\mathbf{S}^{-1}(\\mathbf{\\overline{x}}-\\boldsymbol{\\mu})\\sim F^p_{n-p}.\\]\n\nSi \\(\\mathbf{\\overline{x}}, \\mathbf{S}_1, \\mathbf{\\overline{y}}, \\mathbf{S}_2\\) son el vector de medias y la matriz de covarianzas de las matrices \\(\\mathbf{X}_{n_1\\times p}, \\mathbf{Y}_{n_2\\times p}\\), respectivamente, con filas independientes \\(N_p(\\boldsymbol{\\mu},\\mathbf{\\Sigma})\\), y consideramos la estimación conjunta centrada (o insesgada) de \\(\\mathbf{\\Sigma}\\)\n\n\\[\\mathbf{\\widehat{S}}=(n_1\\mathbf{S}_1+n_2\\mathbf{S}_2)/(n_1+n_2-2),\\]\nentonces\n\\[T^2=\\frac{n_1n_2}{n_1+n_2}(\\mathbf{\\overline{x}}-\\mathbf{\\overline{y}})'\\mathbf{\\widehat{S}}^{-1}(\\mathbf{\\overline{x}}-\\mathbf{\\overline{y}})\\sim T^2(p,n_1+n_2-2)\\]\ny por lo tanto\n\\[\\frac{n_1+n_2-1-p}{(n_1+n_2-2)p}T^2\\sim F^p_{n_1+n_2-1-p}.\\]\n\n\n3.1.4 Distribución de Wilks\nLa distribución F con \\(m\\) y \\(n\\) grados de libertad surge considerando el cociente \\[F=\\frac{A/m}{B/n},\\] donde \\(A, B\\) son ji-cuadrados estocásticamente independientes con \\(m\\) y \\(n\\) grados de libertad. Si consideramos la distribución\n\\[\\Lambda = \\frac{A}{A+B},\\]\nla relación entre \\(\\Lambda\\) y \\(F^m_n\\), así como la inversa \\(F^n_m\\), es\n\\[F^m_n=\\frac{n}{m}\\frac{\\Lambda}{1-\\Lambda},\\qquad F^m_n=\\frac{m}{n}\\frac{1-\\Lambda}{\\Lambda}.\\]\nLa distribución de Wilks generaliza esta relación.\n\nDefinición 3.3 Si las matrices \\(\\mathbf{A}, \\mathbf{B}\\) de orden \\(p\\times p\\) son independientes Wishart \\(W_p(\\mathbf{\\Sigma}, m), W_p(\\mathbf{\\Sigma}, n)\\), respectivamente, con \\(m\\ge p\\), la distribución del cociente de determinantes\n\\[\\Lambda = \\frac{|\\mathbf{A}|}{|\\mathbf{A}+\\mathbf{B}|}\\]\nes, por definición, la distribución lambda de Wilks, que indicaremos por \\(\\Lambda(p,m,n)\\).\n\nPropiedades:\n\n\\(0\\le\\Lambda\\le1\\) y además \\(\\Lambda\\) no depende de \\(\\mathbf{\\Sigma}\\). Por lo tanto, podemos estudiarla suponiendo \\(\\mathbf{\\Sigma}=\\mathbf{I}\\).\nSu distribución es equivalente a la del producto de \\(n\\) variables beta independientes:\n\n\\[\\Lambda(p,m,n)=\\prod_{i=1}^nU_i,\\]\ndonde \\(U_i\\) es beta \\(B\\left(\\frac{1}{2}(m+i-p),\\frac{1}{2}p\\right)\\). (Obsérvese que debe ser \\(m\\ge p\\)).\n\nLos parámetros se pueden permutar manteniendo la misma distribución. Concretamente: \\(\\Lambda(p, m, n)\\sim\\Lambda(n,m+n-p,p)\\).\nPara valores 1 y 2 de \\(p\\) y \\(n\\); la distribución de \\(\\Lambda\\) equivale a la distribución F, según las fórmulas:\n\n\\[\\begin{align*}\n  & \\frac{1-\\Lambda}{\\Lambda}\\frac{m}{n} & \\sim\\ & F^n_m & (p=1)\\\\\n  & \\frac{1-\\Lambda}{\\Lambda}\\frac{m-p+1}{p} & \\sim\\ & F^p_{m-p+1} & (n=1)\\\\\n  & \\frac{1-\\sqrt{\\Lambda}}{\\sqrt{\\Lambda}}\\frac{m-1}{n} & \\sim\\ & F^{2n}_{2(m-1)} & (p=2)\\\\\n  & \\frac{1-\\sqrt{\\Lambda}}{\\sqrt{\\Lambda}}\\frac{m-p+1}{p} & \\sim\\ & F^{2p}_{2(m-p+1)} & (n=2)\n\\end{align*} \\tag{3.8}\\]\n\nEn general, una transformación de \\(\\Lambda\\) equivale, exacta o asintóticamente, a la distribución F. Si \\(\\Lambda(p,n-q,q)\\) es Wilks con \\(n\\) relativamente grande, consideremos\n\n\\[F=\\frac{ms-2\\lambda}{pq}\\frac{1-\\Lambda^{1/s}}{\\Lambda^{1/s}} \\tag{3.9}\\]\ncon \\(m=n-(p+q+1)/2,\\lambda=(pq-2)/4,s=\\sqrt{(p^2q^2-4)/(p^2+q^2-5)}\\). Entonces \\(F\\) sigue asintóticamente la distribución F con \\(pq\\) y \\((ms-2\\lambda)\\) grados de libertad.\n\n\n3.1.5 Relaciones entre Wilks, Hotelling y F\nA. Probemos la relación entre \\(\\Lambda\\) y \\(F\\) cuando \\(p=1\\): Sean \\(A\\sim\\chi^2_m,B\\sim\\chi^2_n\\) independientes. Entonces \\(\\Lambda=A/(A+B)\\sim\\Lambda(1,m,n)\\) y \\(F=(n/m)A/B=(n/m)\\overline{F}\\sim F^m_n\\). Tenemos que \\(\\Lambda=(A/B)/(A/B+1)=\\overline{F}(1+\\overline{F})\\), luego \\(\\overline{F}=\\Lambda(1-\\Lambda)\\Rightarrow(n/m)\\Lambda/(1-\\Lambda)\\sim F^m_n\\). Mas si \\(f\\sim F^m_n\\) entonces \\(1/F\\sim F^n_m\\). Hemos demostrado que\n\\[\\frac{1-\\Lambda(1,m,n)}{\\Lambda(1,m,n)}\\frac{m}{n}\\sim F^n_m \\tag{3.10}\\]\nB. Recordemos que \\(\\mathbf{y}\\) es un vector columna y por lo tanto \\(\\mathbf{yy'}\\) es una matriz \\(p\\times p\\). Probemos la relación entre las distribuciones \\(T^2\\) y \\(F\\). Tenemos \\(T^2=m\\mathbf{y'Q}^{-1}\\mathbf{y}\\), donde \\(\\mathbf{Q}\\) es \\(W_p(\\mathbf{I},m)\\), y \\(\\mathbf{yy'}\\) es \\(W_p(\\mathbf{I},1)\\). Se cumple\n\\[|\\mathbf{Q}+\\mathbf{yy'}|=|\\mathbf{Q}||1+\\mathbf{y'Q}^{-1}\\mathbf{y}|,\\]\nque implica\n\\[1+\\mathbf{y'Q}^{-1}\\mathbf{y}=|\\mathbf{Q}+\\mathbf{yy'}|/|\\mathbf{Q}|=1/\\Lambda,\\]\ndonde \\(\\Lambda=|\\mathbf{Q}|/|\\mathbf{Q}+\\mathbf{yy'}|=\\Lambda(p,m,1)=\\Lambda(1,m+1-p,p)\\). Además, \\(\\mathbf{y'Q}^{-1}\\mathbf{y}=1/\\Lambda-1=(1-\\Lambda)/\\Lambda\\). De Ecuación 3.10 tenemos que \\(\\mathbf{y'Q}^{-1}\\mathbf{y}(m+1-p)=F^p_{m+1-p}\\) y por lo tanto\n\\[T^2=m\\mathbf{y'Q}^{-1}\\mathbf{y}=\\frac{mp}{m+1-p}F^p_{m+1-p}.\\]\n\n\n3.1.6 Distribución multinomial\nSupongamos que la población es la reunión disjunta de \\(k\\) sucesos excluyentes \\(A_1,\\ldots,A_k\\),\n\\[\\Omega=A_1+\\cdots+A_k,\\]\ncon probabilidades positivas \\(P(A_1)=p_1,\\ldots,P(A_k)=p_k\\); verificando\n\\[p_1+\\cdots+p_k=1.\\]\nConsideremos \\(n\\) observaciones independientes y sea \\((f_1,\\ldots,f_k)\\) el vector con las frecuencias observadas de \\(A_1,\\ldots,A_k\\), siendo\n\\[f_1+\\cdots+f_k=n. \\tag{3.11}\\]\nLa distribución multinomial es la distribución de \\(\\mathbf{f}=(f_1,\\ldots,f_k)\\) con función de densidad discreta\n\\[f(f_1,\\ldots,f_k) = \\frac{n!}{f_1!\\cdots f_k!}p_1^{f_1}\\cdots p_k^{f_k}.\\]\nEn el caso \\(k = 2\\) tenemos la distribución binomial.\nIndiquemos \\(\\mathbf{p}=(p_1,\\ldots,p_k)'\\). 1. El vector de medias de \\(\\mathbf{f}\\) es \\(\\boldsymbol{\\mu}=n\\mathbf{p}\\). 2. La matriz de covarianzas de \\(\\mathbf{f}\\) es \\(\\mathbf{C} = n[diag(\\mathbf{p})-\\mathbf{pp'}]\\). Es decir:\n\\[\\begin{align*}\n  c_{ii} & = np_i(1-p_i),\\\\\n  c_{ij} & = -p_ip_j\\qquad \\text{si } i\\not=j.\n\\end{align*}\\]\nPuesto que \\(\\mathbf{C1}=\\mathbf{0}\\), la matriz \\(\\mathbf{C}\\) es singular. La singularidad se debe a que se verifica Ecuación 3.11. Una g-inversa de C es:\n\\[\\mathbf{C}^-=\\frac{1}{n}diag(p_1^{-1},\\ldots,p_k^{-1}). \\tag{3.12}\\]\nPuesto que \\(\\mathbf{C}(\\mathbf{I}-\\mathbf{11'})=\\mathbf{C}\\), es fácil ver que otra g-inversa es\n\\[\\mathbf{C}^-=\\frac{1}{n}diag(p_1^{-1},\\ldots,p_k^{-1})(\\mathbf{I}-\\mathbf{11'}).\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inferencia multivariante en poblaciones normales</span>"
    ]
  },
  {
    "objectID": "t3_inferencia.html#distribuciones-con-marginales-dadas-opcional",
    "href": "t3_inferencia.html#distribuciones-con-marginales-dadas-opcional",
    "title": "3  Inferencia multivariante en poblaciones normales",
    "section": "3.2 Distribuciones con marginales dadas (OPCIONAL)",
    "text": "3.2 Distribuciones con marginales dadas (OPCIONAL)\nSea \\(H(x,y)\\) la función de distribución bivariante de dos variables aleatorias \\((X,Y)\\). La función \\(H\\) es\n\\[H(x,y) = P(X\\le x,Y\\le y).\\]\nConsideremos las distribuciones marginales, es decir, las distribuciones univariantes de \\(X,Y\\):\n\\[\\begin{align*}\n  F(x) & = P(X\\le x) = H(x,\\infty),\\\\\n  G(y) & = P(Y\\le y) = H(\\infty,y).\n\\end{align*}\\]\nUn procedimiento para la obtención de modelos de distribuciones bivariantes consiste en encontrar \\(H\\) a partir de \\(F,G\\) y posiblemente algún parámetro. Si suponemos \\(X,Y\\) independientes, una primera distribución es\n\\[H^0(x,y)=F(x)G(y).\\]\nM. Fréchet introdujo las distribuciones bivariantes\n\\[\\begin{align*}\n  H^-(x,y) & = max\\{F(x)+G(y)-1,0\\}, \\\\\n  H^+(x,y) & = min\\{F(x),G(y)\\},\n\\end{align*}\\]\ny demostró la desigualdad\n\\[H^-(x,y)\\le H(x,y)\\le H^+(x,y).\\]\nCuando la distribución es \\(H^-\\), entonces se cumple la relación funcional entre \\(X, Y\\)\n\\[F(X)+G(Y)=1,\\]\ny la correlación entre \\(X,Y\\) (si existe) \\(\\rho^-\\) es mínima. Cuando la distribución es \\(H^+\\), entonces se cumple la relación funcional entre \\(X,Y\\)\n\\[F(X)=G(Y),\\]\ny la correlación entre X; Y (si existe) \\(\\rho^+\\) es máxima. Previamente W. Hoeffding había probado la siguiente formula para la covarianza\n\\[cov(X,Y)=\\int_{\\mathbb{R}^2}[H(x,y)-F(x)G(y)]dxdy,\\]\ny demostrado la desigualdad\n\\[\\rho^-\\le\\rho\\le\\rho^+,\\]\ndonde \\(\\rho^-,\\rho\\) y \\(\\rho^+\\) son las correlaciones entre \\(X,Y\\) cuando la distribución bivariante es \\(H^-,H\\) y \\(H^+\\) , respectivamente.\nPosteriormente, diversos autores han propuesto distribuciones bivariantes paramétricas a partir de las marginales F; G, que en algunos casos contienen a \\(H^-,H^0\\) y \\(H^+\\). Escribiendo \\(F,G,H\\) para indicar \\(F(x),G(y),H(x,y)\\), algunas familias son:\n\nFarlie-Gumbel-Morgenstern: \\[H_\\theta = FG[1+\\theta(1-F)(1-G)],\\quad-1\\le\\theta\\le1.\\]\nClayton-Oakes: \\[H_\\alpha=[max(F^{-\\alpha}+G^{-\\alpha}-1,0]^{-1/\\alpha},\\quad-1\\le\\alpha\\le\\infty.\\]\nAli-Mikhail-Haq: \\[H_\\theta = FG/[1-\\theta(1-F)(1-G)],\\quad-1\\le\\theta\\le1.\\]\nCuadras-Augé: \\[H_\\theta=(min\\{F,G\\})^\\theta(FG)^{1-\\theta},\\quad0\\le\\theta\\le1.\\]\nFamilia de corrección \\[H_\\theta(x,y) = \\theta F(min\\{x,y\\})+(1-\\theta)F(x)J(y),\\quad-1\\le\\theta\\le1.\\]\n\nsiendo \\(J(y)=[G(y)-\\theta F(y)]/(1-\\theta)\\) una función de distribución univariante.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inferencia multivariante en poblaciones normales</span>"
    ]
  },
  {
    "objectID": "t3_inferencia.html#introducción-a-la-inferencia-multivariante",
    "href": "t3_inferencia.html#introducción-a-la-inferencia-multivariante",
    "title": "3  Inferencia multivariante en poblaciones normales",
    "section": "3.3 Introducción a la inferencia multivariante",
    "text": "3.3 Introducción a la inferencia multivariante\nPara la exploración de datos multivariantes, no se suelen emplear modelos formales para dar respuestas a las preguntas formuladas, sin embargo, en algunas situaciones es posible ajustar modelos “formales” para probar una hipótesis sobre los parámetros de la función de densidad de probabilidad de esa población. La función de densidad de probabilidad asumida casi universalmente como la base de las inferencias para los datos multivariantes es la normal multivariante.\n\n3.3.1 Conceptos básicos\nSea \\(f(x,\\boldsymbol{\\theta})\\) un modelo estadístico. La función “score” se define como\n\\[\\mathbf{z}(\\mathbf{x},\\boldsymbol{\\theta})=\\frac{\\partial}{\\partial\\boldsymbol{\\theta}}log\\: f(\\mathbf{x},\\boldsymbol{\\theta}).\\]\nUna muestra multivariante está formada por las \\(n\\) filas \\(\\mathbf{x}_1',\\ldots,\\mathbf{x}_n'\\) independientes de una matriz de datos \\(\\mathbf{X}_{n\\times p}\\): La función de verosimilitud es\n\\[L(\\mathbf{X},\\boldsymbol{\\theta})=\\prod_{i=1}^nf(x_i,\\boldsymbol{\\theta}).\\]\nLa función “score” de la muestra es\n\\[\\mathbf{z}(\\mathbf{X},\\boldsymbol{\\theta})=\\sum_{i=1}^n\\:\\frac{\\partial}{\\partial\\boldsymbol{\\theta}}log\\:f(x_i,\\boldsymbol{\\theta}).\\]\nLa matriz de información de Fisher \\(F(\\boldsymbol{\\theta})\\) es la matriz de covarianzas de \\(\\mathbf{z}(\\mathbf{X},\\boldsymbol{\\theta})\\). Cuando un modelo estadístico es regular se verifica:\n\\[\\text{a) }E[\\mathbf{z}(\\mathbf{X},\\boldsymbol{\\theta})]=\\mathbf{0},\\quad \\text{b) }F(\\boldsymbol{\\theta})=E[\\mathbf{z}(\\mathbf{X},\\boldsymbol{\\theta})\\mathbf{z}(\\mathbf{X},\\boldsymbol{\\theta})'].\\]\nUn estimador \\(\\mathbf{t}(\\mathbf{X})\\) de \\(\\boldsymbol{\\theta}\\) es insesgado si \\(E[\\mathbf{t}(\\mathbf{X})]=\\boldsymbol{\\theta}\\). La desigualdad de Cramér-Rao dice que si \\(cov(\\mathbf{t}(\\mathbf{X}))\\) es la matriz de covarianzas de \\(\\mathbf{t}(\\mathbf{X})\\), entonces\n\\[cov(\\mathbf{t}(\\mathbf{X}))\\ge F(\\boldsymbol{\\theta})^{-1},\\]\nen el sentido de que la diferencia \\(cov(\\mathbf{t}(\\mathbf{X}))-F(\\boldsymbol{\\theta})^{-1}\\) es una matriz semidefinida positiva.\nUn estimador \\(\\widehat{\\boldsymbol{\\theta}}\\) del parámetro desconocido \\(\\boldsymbol{\\theta}\\) es máximo verosímil si maximiza la función \\(L(\\mathbf{X},\\boldsymbol{\\theta})\\). En condiciones de regularidad, podemos obtener \\(\\widehat{\\boldsymbol{\\theta}}\\) resolviendo la ecuación\n\\[\\sum_{i=1}^n\\:\\frac{\\partial}{\\partial\\boldsymbol{\\theta}}log\\:f(x_i,\\boldsymbol{\\theta})=\\mathbf{0}\\]\nEntonces el estimador máximo verosímil \\(\\widehat{\\boldsymbol{\\theta}}_n\\) obtenido a partir de una muestra de tamaño \\(n\\) satisface:\n\nEs asintóticamente normal con vector de medias \\(\\boldsymbol{\\theta}\\) y matriz de covarianzas \\((nF_1(\\boldsymbol{\\theta}))^{-1}\\), donde \\(F_1(\\boldsymbol{\\theta})\\) es la matriz de información de Fisher para una sola observación.\nSi \\(\\mathbf{t}(\\mathbf{X})\\) es estimador insesgado de \\(\\boldsymbol{\\theta}\\) tal que \\(cov(\\mathbf{t}(\\mathbf{X}))=(nF_1(\\boldsymbol{\\theta}))^{-1}\\), entonces \\(\\widehat{\\boldsymbol{\\theta}}_n=\\mathbf{t}(\\mathbf{X})\\).\n\\(\\widehat{\\boldsymbol{\\theta}}_n\\) converge en probabilidad a \\(\\boldsymbol{\\theta}\\).\n\n\n\n3.3.2 Estimación de medias y covarianzas\nSi las \\(n\\) filas \\(\\mathbf{x}_1',\\ldots,\\mathbf{x}_n'\\) de \\(\\mathbf{X}_{n\\times p}\\) son independientes \\(N_p(\\boldsymbol{\\mu},\\mathbf{\\Sigma})\\) la función de verosimilitud es\n\\[L(\\mathbf{X},\\boldsymbol{\\mu},\\mathbf{\\Sigma})=det(2\\pi\\mathbf{\\Sigma})^{-n/2}exp\\left\\{-\\frac{1}{2}\\sum_{i=1}^n(x_i-\\boldsymbol{\\mu})'\\mathbf{\\Sigma}^{-1}(x_i-\\boldsymbol{\\mu})\\right\\}.\\]\nSea \\(\\mathbf{b}_i=\\mathbf{x}_i-\\overline{\\mathbf{x}}\\). Se verifica\n\\[\\begin{align*}\n  \\sum_{i=1}^n(\\mathbf{x}_i-\\boldsymbol{\\mu})'\\mathbf{\\Sigma}^{-1}(\\mathbf{x}_i-\\boldsymbol{\\mu}) & =\\sum_{i=1}^n\\mathbf{d}_i'\\mathbf{\\Sigma}^{-1}\\mathbf{d}_i+n(\\mathbf{x}_i-\\boldsymbol{\\mu})'\\mathbf{\\Sigma}^{-1}(\\mathbf{x}_i-\\boldsymbol{\\mu})\\\\\n  &=tr\\left[\\mathbf{\\Sigma}^{-1}\\sum_{i=1}^n\\mathbf{d}_i\\mathbf{d}_i'\\right]+n(\\mathbf{x}_i-\\boldsymbol{\\mu})'\\mathbf{\\Sigma}^{-1}(\\mathbf{x}_i-\\boldsymbol{\\mu}).\n\\end{align*}\\]\nPor la tanto el logaritmo de \\(L\\) se puede expresar como\n\\[log\\ L(\\mathbf{X},\\boldsymbol{\\mu},\\mathbf{\\Sigma})=-\\frac{n}{2}log\\ det(2\\pi\\mathbf{\\Sigma})-\\frac{n}{2}tr(\\mathbf{\\Sigma}^{-1}\\mathbf{S})-\\frac{n}{2}(\\overline{\\mathbf{x}}-\\boldsymbol{\\mu})'\\mathbf{\\Sigma}^{-1}(\\overline{\\mathbf{x}}-\\boldsymbol{\\mu}).\\]\nDerivando matricialmente respecto de \\(\\boldsymbol{\\mu}\\) y de \\(\\mathbf{\\Sigma}^{-1}\\) tenemos\n\\[\\begin{align*}\n  & \\frac{\\partial}{\\partial\\boldsymbol{\\mu}}log\\ L & =\\ & n\\mathbf{\\Sigma}^{-1}(\\overline{\\mathbf{x}}-\\boldsymbol{\\mu})=\\mathbf{0},\\\\\n  & \\frac{\\partial}{\\partial\\mathbf{\\Sigma}^{-1}}log\\ L & =\\ & \\frac{n}{2}[\\mathbf{\\Sigma}-\\mathbf{S}-(\\overline{\\mathbf{x}}-\\boldsymbol{\\mu})(\\overline{\\mathbf{x}}-\\boldsymbol{\\mu})']=\\mathbf{0}.\n\\end{align*}\\]\nLas estimaciones máximo-verosímiles de \\(\\boldsymbol{\\mu},\\mathbf{\\Sigma}\\) son pues\n\\[\\widehat{\\boldsymbol{\\mu}}=\\overline{\\mathbf{x}},\\quad\\widehat{\\mathbf{\\Sigma}}=\\mathbf{S}.\\]\nSin embargo \\(\\mathbf{S}\\) no es estimador insesgado de \\(\\mathbf{\\Sigma}\\). La estimación centrada es \\(\\widehat{\\mathbf{S}} = \\mathbf{X}'\\mathbf{HX}/(n-1)\\).\nSi solo \\(\\boldsymbol{\\mu}\\) es desconocido, la matriz de información de Fisher es\n\\[F(\\boldsymbol{\\mu})=E\\left[n\\mathbf{\\Sigma}^{-1}(\\overline{\\mathbf{x}}-\\boldsymbol{\\mu})n\\Sigma^{-1}(\\overline{\\mathbf{x}}-\\boldsymbol{\\mu})'\\right]=n\\mathbf{\\Sigma}^{-1},\\]\ny como \\(cov(\\overline{\\mathbf{x}})=\\mathbf{\\Sigma}/n\\), tenemos que \\(\\overline{\\mathbf{x}}\\) alcanza la cota de Cramér-Rao.\nProbaremos más adelante que:\n\n\\(\\overline{\\mathbf{x}}\\) es \\(N_p(\\boldsymbol{\\mu},\\mathbf{\\Sigma}/n)\\).\n\\(\\overline{\\mathbf{x}}\\) y \\(\\mathbf{S}\\) son estocásticamente independientes.\n\\(n\\mathbf{S}\\) sigue la distribución de Wishart.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inferencia multivariante en poblaciones normales</span>"
    ]
  },
  {
    "objectID": "t3_inferencia.html#contraste-de-hipótesis-multivariantes",
    "href": "t3_inferencia.html#contraste-de-hipótesis-multivariantes",
    "title": "3  Inferencia multivariante en poblaciones normales",
    "section": "3.4 Contraste de hipótesis multivariantes",
    "text": "3.4 Contraste de hipótesis multivariantes\nUn primer método para construir contrastes sobre los parámetros de una población normal, se basa en las propiedades anteriores, que dan lugar a estadísticos con distribución conocida (ji-cuadrado, F).\n\n3.4.1 Test sobre la media: una población\nSupongamos que las filas de \\(\\mathbf{X}_{n\\times p}\\) son independientes \\(N_p(\\boldsymbol{\\mu},\\mathbf{\\Sigma})\\). Sea \\(\\boldsymbol{\\mu}_0\\) un vector de medias conocido. Queremos realizar un test sobre la hipótesis\n\\[H_0:\\boldsymbol{\\mu}=\\boldsymbol{\\mu}_0\\]\n\nSi \\(\\mathbf{\\Sigma}\\) es conocida, como \\(\\overline{\\mathbf{x}}\\) es \\(N_p(\\boldsymbol{\\mu},\\mathbf{\\Sigma}/n)\\), el estadístico de contraste es\n\n\\[n(\\overline{\\mathbf{x}}-\\boldsymbol{\\mu}_0)'\\Sigma^{-1}(\\overline{\\mathbf{x}}-\\boldsymbol{\\mu}_0)\\sim\\chi^2_p.\\]\n\nSi \\(\\mathbf{\\Sigma}\\) es desconocida, como \\((n-1)(\\overline{\\mathbf{x}}-\\boldsymbol{\\mu})'\\mathbf{S}^{-1}(\\overline{\\mathbf{x}}-\\boldsymbol{\\mu})\\sim T^2(p,n-1)\\), el estadístico de contraste es\n\n\\[\\frac{n-p}{n}(\\overline{\\mathbf{x}}-\\boldsymbol{\\mu}_0)'\\mathbf{S}^{-1}(\\overline{\\mathbf{x}}-\\boldsymbol{\\mu}_0)\\sim F^p_{n-p}.\\] {#eq-13}\nEn ambos casos se rechaza \\(H_0\\) para valores grandes significativos del estadístico.\n\n\n3.4.2 Test sobre la media: dos poblaciones\nSupongamos ahora que tenemos dos matrices de datos independientes \\(\\mathbf{X}_{n_1\\times p},\\mathbf{Y}_{n_2\\times p}\\) que provienen de distribuciones \\(N_p(\\boldsymbol{\\mu}_1,\\mathbf{\\Sigma}),N_p(\\boldsymbol{\\mu}_2,\\mathbf{\\Sigma})\\). Queremos construir un test sobre la hipótesis\n\\[H_0:\\boldsymbol{\\mu}_1=\\boldsymbol{\\mu}_2.\\]\n\nSi \\(\\mathbf{\\Sigma}\\) es conocida, como \\((\\overline{\\mathbf{x}}-\\overline{\\mathbf{y}})\\) es \\(N_p(\\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_2,(1/n_1+1/n_2)\\mathbf{\\Sigma})\\) el estadístico de contraste es\n\n\\[\\frac{n_1n_2}{n_1+n_2}(\\overline{\\mathbf{x}}-\\overline{\\mathbf{y}})'\\Sigma^{-1}(\\overline{\\mathbf{x}}-\\overline{\\mathbf{y}})\\sim\\chi^2_p.\\]\n\nSi \\(\\mathbf{\\Sigma}\\) es desconocida, el estadístico de contraste es\n\n\\[\\frac{n_1+n_2-1-p}{(n_1+n_2-2)p}\\frac{n_1n_2}{n_1+n_2}(\\overline{\\mathbf{x}}-\\overline{\\mathbf{y}})'\\widehat{\\mathbf{S}}^{-1}(\\overline{\\mathbf{x}}-\\overline{\\mathbf{y}})\\sim F^p_{n_1+n_2-1-p}.\\]\nSiendo \\(\\widehat{\\mathbf{S}}=(n_1\\mathbf{S}_1+n_2\\mathbf{S}_2)/(n_1+n_2-2)\\) la estimación centrada (es decir, insesgada) de \\(\\mathbf{\\Sigma}\\).\n\n\n3.4.3 Comparación de varias medias\nSupongamos que las filas de \\(g\\) matrices de datos son independientes, y -que provienen de la observación de \\(g\\) poblaciones normales multivariantes:\n\n\n\nTabla 3.1: Propiedades de las observaciones\n\n\n\n\n\n\n\n\n\n\n\n\nmatriz\norden\nmedia\ncovarianza\ndistribución\n\n\n\n\n\\(\\mathbf{X}_1\\)\n\\(n_1\\times p\\)\n\\(\\overline{\\mathbf{x}}_1\\)\n\\(\\mathbf{S}_1\\)\n\\(N_p(\\boldsymbol{\\mu}_1,\\mathbf{\\Sigma})\\)\n\n\n\\(\\mathbf{X}_2\\)\n\\(n_2\\times p\\)\n\\(\\overline{\\mathbf{x}}_2\\)\n\\(\\mathbf{S}_2\\)\n\\(N_p(\\boldsymbol{\\mu}_2,\\mathbf{\\Sigma})\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(\\mathbf{X}_g\\)\n\\(n_g\\times p\\)\n\\(\\overline{\\mathbf{x}}_g\\)\n\\(\\mathbf{S}_g\\)\n\\(N_p(\\boldsymbol{\\mu}_g,\\mathbf{\\Sigma})\\)\n\n\n\n\n\n\nEl vector de medias generales y la estimación centrada (o insesgada) de la matriz de covarianzas común \\(\\mathbf{\\Sigma}\\) son\n\\[\\overline{\\mathbf{x}}=\\frac{1}{n}\\sum_{i=1}^gn_i\\overline{\\mathbf{x}}_i,\\quad \\widehat{\\mathbf{S}}=\\frac{1}{n-g}\\sum_{i=1}^gn_i\\mathbf{S}_i,\\]\nsiendo \\(\\mathbf{S}_i=n_i^{-1}\\mathbf{X}_i'\\mathbf{HX}_i\\) y \\(n=\\sum_{i=1}^gn_i\\).\nDeseamos construir un test para decidir si podemos aceptar la hipótesis de igualdad de medias\n\\[H_0:\\boldsymbol{\\mu}_1=\\boldsymbol{\\mu}_2=\\cdots=\\boldsymbol{\\mu}_g.\\]\nIntroduciremos las siguientes matrices:\n\\[\\begin{align*}\n\\mathbf{B}=\\ & \\sum_{i=1}^gn_i(\\overline{\\mathbf{x}}_i-\\overline{\\mathbf{x}})(\\overline{\\mathbf{x}}_i-\\overline{\\mathbf{x}})'\\quad & (\\text{dispersión entre grupos})\\\\\n\\mathbf{W}=\\ & \\sum_{i=1}^g\\sum_{\\alpha=1}^{n_i}(\\mathbf{x}_{i\\alpha}-\\overline{\\mathbf{x}}_i)(\\mathbf{x}_{i\\alpha}-\\overline{\\mathbf{x}}_i)'\\quad & (\\text{dispersión dentro grupos})\\\\\n\\mathbf{T}=\\ & \\sum_{i=1}^g\\sum_{\\alpha=1}^{n_i}(\\mathbf{x}_{i\\alpha}-\\overline{\\mathbf{x}})(\\mathbf{x}_{i\\alpha}-\\overline{\\mathbf{x}})'\\quad & (\\text{dispersión total})\n\\end{align*}\\]\nSe verifica que \\(\\mathbf{W}=(n-g)\\widehat{\\mathbf{S}}\\) y la relación:\n\\[\\mathbf{T}=\\mathbf{B}+\\mathbf{W}.\\]\nSi la hipótesis nula es cierta, se verifica además\n\\[\\begin{align*}\n& \\mathbf{B}\\sim W_p(\\mathbf{\\Sigma},g-1),\\ \\mathbf{W}\\sim W_p(\\mathbf{\\Sigma},n-g),\\ \\mathbf{T}\\sim W_p(\\mathbf{\\Sigma},n-1),\\\\\n& \\mathbf{B},\\mathbf{W}\\text{ son estocásticamente independientes.}\n\\end{align*}\\]\nPor lo tanto, si \\(H_0\\) es cierta\n\\[\\Lambda=\\frac{|\\mathbf{W}|}{|\\mathbf{W}+\\mathbf{B}|}\\sim\\Lambda(p,n-g,n-1).\\]\nRechazaremos \\(H_0\\) si \\(\\Lambda\\) es un valor pequeño y significativo, o si la transformación a una \\(F\\) es grande y significativa.\n\n\n3.4.4 Teorema de Cochran\nAlgunos resultados de la sección anterior son una consecuencia del Teorema 3.1, conocido como teorema de Cochran.\n\nLema 3.1 Sea \\(\\mathbf{X}(n\\times p)\\) una matriz de datos \\(N_p(\\boldsymbol{\\mu},\\mathbf{\\Sigma})\\) y \\(\\mathbf{u}\\), \\(\\mathbf{v}\\) dos vectores \\(n\\times1\\) tales que \\(\\mathbf{u}'\\mathbf{u}=\\mathbf{v}'\\mathbf{v}=1\\), \\(\\mathbf{u}'\\mathbf{v}=0\\).\n\nSi \\(\\boldsymbol{\\mu}=\\mathbf{0}\\) entonces \\(\\mathbf{y}'=\\mathbf{u}'\\mathbf{X}\\) es \\(N_p(\\mathbf{0},\\mathbf{\\Sigma})\\).\n\\(\\mathbf{y}'=\\mathbf{u}'\\mathbf{X}\\) es independiente de \\(\\mathbf{z}'=\\mathbf{v}'\\mathbf{X}\\)\n\n\n\nPrueba. Sean \\(\\mathbf{x}_1',\\ldots,\\mathbf{x}_n'\\) las filas (independientes) de \\(\\mathbf{X}\\). Si \\(\\mathbf{u}=(u_1,\\ldots,u_n)'\\) entonces \\(\\mathbf{y}'=\\mathbf{u}'\\mathbf{X}=\\sum_{i=1}^nu_i\\mathbf{x_i}\\) es normal multivariante con \\(\\boldsymbol{\\mu}=\\mathbf{0}\\) y matriz de covarianzas\n\\[\\begin{align*}\nE(\\mathbf{y}\\mathbf{y}') & =\\ E\\left(\\sum_{i=1}^nu_i\\mathbf{x}_i\\right)\\left(\\sum_{i=1}^nu_i\\mathbf{x}_i\\right)'=E\\left(\\sum_{i,j=1}^nu_iu_j\\mathbf{x}_i\\mathbf{x}_j'\\right)\\\\\n& =\\ \\sum_{i,j=1}^nu_iu_jE\\left(\\mathbf{x}_i\\mathbf{x}_j'\\right)=\\sum_{i=1}^nu_i^2E\\left(\\mathbf{x}_i\\mathbf{x}_i'\\right)\\\\\n& =\\ \\sum_{i=1}^nu_i^2\\mathbf{\\Sigma}=\\mathbf{\\Sigma}.\n\\end{align*}\\]\nAnálogamente, si \\(\\mathbf{v}=(v_1,\\ldots,v_n)'\\), \\(\\mathbf{z}'=\\mathbf{v}'\\mathbf{X}\\) es también normal.\nLas esperanzas de \\(\\mathbf{y}\\), \\(\\mathbf{z}\\) son: \\(E(\\mathbf{y})=\\left(\\sum_{i=1}^nu_i\\right)\\boldsymbol{\\mu}\\), \\(E(\\mathbf{z})=\\left(\\sum_{i=1}^nv_i\\right)\\boldsymbol{\\mu}\\). Las covarianzas entre \\(\\mathbf{y}\\) y \\(\\mathbf{z}\\) son:\n\\[\\begin{align*}\nE\\left[(\\mathbf{y}-E(\\mathbf{y}))(\\mathbf{z}-E(\\mathbf{z}))'\\right] & =\\ \\sum_{i=1}^nu_iv_jE\\left[(\\mathbf{x}_i-\\boldsymbol{\\mu})(\\mathbf{x}_j-\\boldsymbol{\\mu})'\\right]\\\\\n& =\\ \\sum_{i=1}^nu_iv_iE\\left[(\\mathbf{x}_i-\\boldsymbol{\\mu})(\\mathbf{x}_i-\\boldsymbol{\\mu})'\\right]=\\mathbf{u}'\\mathbf{v\\Sigma}=\\mathbf{0},\n\\end{align*}\\]\nlo que prueba la independencia estocástica entre \\(\\mathbf{y}\\) y \\(\\mathbf{z}\\).\n\n\nTeorema 3.1 Sea \\(\\mathbf{X}(n\\times p)\\) una matriz de datos \\(N_p(\\boldsymbol{\\mu},\\mathbf{\\Sigma})\\) y sea \\(\\mathbf{C}(n\\times n)\\) una matriz simétrica.\n\n\\(\\mathbf{X}'\\mathbf{CX}\\) tiene la misma distribución que una suma ponderada de matrices \\(W_p(\\mathbf{\\Sigma}, 1)\\), donde los pesos son valores propios de \\(\\mathbf{C}\\).\n\\(\\mathbf{X}'\\mathbf{CX}\\) es Wishart \\(W_p(\\mathbf{\\Sigma}, r)\\) si y solo si \\(\\mathbf{C}\\) es idempotente y \\(rango(\\mathbf{C})=r\\).\n\n\n\nPrueba. Sea\n\\[\\mathbf{C}=\\sum_{i=1}^n\\lambda_i\\mathbf{u}_i\\mathbf{u}_i'\\]\nla descomposición espectral de \\(\\mathbf{C}\\), es decir, \\(\\mathbf{Cu}_i=\\lambda_i\\mathbf{u}_i\\). Entonces\n\\[\\mathbf{X}'\\mathbf{CX}=\\sum_{i=1}^n\\lambda_i\\mathbf{y}_i'\\mathbf{y}_i\\].\nPor Lema 3.1, las filas \\(\\mathbf{y}_i'\\) de la matriz\n\\[\\mathbf{Y}=\\left( \\begin{array}{c}\n\\mathbf{y}_1'\\\\\n\\vdots\\\\\n\\mathbf{y}_n'\n\\end{array} \\right)\n=\\left( \\begin{array}{c}\n\\mathbf{u}_1'\\mathbf{X}\\\\\n\\vdots\\\\\n\\mathbf{u}_n'\\mathbf{X}\n\\end{array} \\right),\\]\nson también independientes \\(N_p(\\mathbf{0},\\mathbf{\\Sigma})\\) y cada \\(\\mathbf{y}_i\\mathbf{y}_i'\\) es \\(Wp(\\mathbf{\\Sigma}, 1)\\).\nSi \\(\\mathbf{C}^2=\\mathbf{C}\\) entonces \\(\\mathbf{Cu}_i=\\lambda_i\\mathbf{u}_i\\) siendo \\(\\lambda_i=0\\) ó \\(1\\). Por lo tanto \\(r=tr(\\mathbf{C})\\)\n\\[\\mathbf{X}'\\mathbf{CX}=\\sum_{i=1}^r\\mathbf{y}_i\\mathbf{y}_i'\\sim W_p(\\mathbf{\\Sigma},r).\\]\nEl siguiente resultado se conoce como teorema de Craig, y junto con el teorema de Cochran, permite construir contrastes sobre vectores de medias.\n\n\nTeorema 3.2 Sea \\(\\mathbf{X}(n\\times p)\\) una matriz de datos \\(N_p(\\boldsymbol{\\mu},\\mathbf{\\Sigma})\\) y sean \\(\\mathbf{C}_1(n\\times n)\\),\\(\\mathbf{C}_2(n\\times n)\\) matrices simétrica. Entonces \\(\\mathbf{X}'\\mathbf{C}_1\\mathbf{X}\\) es independiente de \\(\\mathbf{X}'\\mathbf{C}_1\\mathbf{X}\\) si \\(\\mathbf{C}_1\\mathbf{C}_2=\\mathbf{0}\\).\n\n\nPrueba. \\[\\begin{align*}\n\\mathbf{C}_1=\\sum_{i=1}^n\\lambda_i(1)\\mathbf{u}_i\\mathbf{u}_i',\\quad \\mathbf{X}'\\mathbf{C}_1\\mathbf{X}=\\sum\\lambda_i(1)\\mathbf{y}_i\\mathbf{y}_i',\\\\\n\\mathbf{C}_2=\\sum_{j=1}^n\\lambda_j(2)\\mathbf{u}_j\\mathbf{u}_j',\\quad \\mathbf{X}'\\mathbf{C}_2\\mathbf{X}=\\sum\\lambda_j(2)\\mathbf{y}_j\\mathbf{y}_j',\n\\end{align*}\\]\nsiendo \\(\\mathbf{y}_i'=\\mathbf{u}_i'\\mathbf{X}\\), \\(\\mathbf{z}_j'=\\mathbf{v}_j'\\mathbf{X}\\). Por otra parte\n\\[\\mathbf{C}_1\\mathbf{C}_2=\\sum_{i=1}^n\\sum_{j=1}^n\\lambda_i(1)\\lambda_j(2)\\mathbf{u}_i\\mathbf{u}_i'\\mathbf{v}_j\\mathbf{v}_j'=\\mathbf{0}\\ \\ \\Rightarrow\\ \\ \\lambda_i(1)\\lambda_j(2)\\mathbf{u}_i'\\mathbf{v}_j=0,\\quad\\forall i,j.\\]\nSi \\(\\lambda_i(1)\\lambda_j(2)\\not=0\\), entonces por Lema 3.1, \\(\\mathbf{y}_i'(1\\times p)=\\mathbf{u}_i'\\mathbf{X}\\) es independiente de \\(\\mathbf{z}_j'(1\\times p)=\\mathbf{v}_j'\\mathbf{X}\\). Así \\(\\mathbf{X}'\\mathbf{C}_1\\mathbf{X}\\) es independiente de \\(\\mathbf{X}'\\mathbf{C}_1\\mathbf{X}\\).\n\nUna primera consecuencia del teorema anterior es la independencia entre vectores de medias y matrices de covarianzas muestrales. En el caso univariante \\(p = 1\\) es el llamado teorema de Fisher.\n\nTeorema 3.3 Sea \\(\\mathbf{X}(n\\times p)\\) una matriz de datos \\(N_p(\\boldsymbol{\\mu},\\mathbf{\\Sigma})\\). Entonces:\n\nLa media \\(\\overline{\\mathbf{x}}\\) es \\(N_p(\\boldsymbol{\\mu},\\mathbf{\\Sigma}/n)\\).\nLa matriz de covarianzas \\(\\mathbf{S}=\\mathbf{X}'\\mathbf{HX}/n\\) verifica que \\(n\\mathbf{S}\\sim W_p(\\Sigma,n-1)\\).\n\\(\\overline{\\mathbf{x}}\\) y \\(\\mathbf{S}\\) son estocásticamente independientes.\n\n\n\nPrueba. Consideremos\\(\\mathbf{C}_1=n^{-1}\\mathbf{11}'\\). Tenemos \\(rango(\\mathbf{C}_1)=1,\\mathbf{X}'\\mathbf{C}_1\\mathbf{X}=\\overline{\\mathbf{x}}\\overline{\\mathbf{x}}'\\). Consideramos también \\(\\mathbf{C}_2=\\mathbf{H}\\). Como \\(\\mathbf{C}_1\\mathbf{C}_2=\\mathbf{0}\\) deducimos que \\(\\overline{\\mathbf{x}}\\) es independiente de \\(\\mathbf{S}\\).\nPor otra parte, \\(\\mathbf{H1}=\\mathbf{0}\\) y \\(\\mathbf{H}\\) tiene el valor propio \\(1\\) con multiplicidad \\(n-1\\). Así \\(\\mathbf{u}_i\\), vector propio de valor 1, es ortogonal a \\(\\mathbf{1}\\), resultando que \\(\\mathbf{y}_i'=\\mathbf{u}_i'X\\) verifica que \\(E(\\mathbf{y}_i')=\\left(\\sum_{\\alpha-1}^nu_{i\\alpha}\\right)\\boldsymbol{\\mu}=(\\mathbf{u}_i'1)\\boldsymbol{\\mu}=0\\boldsymbol{\\mu}=\\mathbf{0}\\). Si \\(\\mathbf{u}_j\\) es otro vector propio, \\(\\mathbf{y}_i, \\mathbf{y}_j\\) son independientes (Lema 3.1). Tenemos que \\(n\\mathbf{S}=\\sum_{i=1}^{n-1}\\mathbf{y}_i\\mathbf{y}_i'\\), donde los \\(\\mathbf{y}_i\\mathbf{y}_i'\\) son \\(W_p(\\mathbf{\\Sigma},1)\\) independientes.\n\n\nTeorema 3.4 Sean \\(\\mathbf{X}_i\\), matrices de datos independientes de orden \\(n_i\\times p\\) con distribución \\(N_p(\\boldsymbol{\\mu}_i,\\mathbf{\\Sigma}), i=1,\\ldots,g, n=\\sum_{i=1}^gn_i\\). Si la hipótesis nula\n\\[H_0: \\boldsymbol{\\mu}_1=\\boldsymbol{\\mu}_2=\\cdots=\\boldsymbol{\\mu}_g\\]\nes cierta, entonces \\(\\mathbf{B}, \\mathbf{W}\\) son independientes con distribuciones Wishart:\n\\[\\mathbf{B}\\sim W_p(\\mathbf{\\Sigma},g-1),\\quad\\mathbf{W}\\sim W_p(\\mathbf{\\Sigma},n-g).\\]\n\n\nPrueba. Escribimos las matrices de datos como una única matriz\n\\[\\mathbf{X}=\\left[ \\begin{array}{c}\n\\mathbf{X}_1\\\\\n\\vdots\\\\\n\\mathbf{X}_g\n\\end{array} \\right].\\]\nSean\n\\[\\begin{align*}\n& \\mathbf{1}_1 & =\\ & (1,\\ldots,1,0,\\ldots,0),\\ldots,\\mathbf{1}_g=(0,\\ldots,0,1,\\ldots,1), \\\\\n& \\mathbf{1} & =\\ & \\sum_{i=1}^g\\mathbf{1}_i=(1,\\ldots,1,\\ldots,1,\\ldots,1),\n\\end{align*}\\]\ndonde \\(\\mathbf{1}_1\\) tiene \\(n_1\\) unos y el resto ceros, etc. Sean también\n\\[\\begin{align*}\n& \\mathbf{I}_i & =\\ & diag(\\mathbf{1}_i),\\quad \\mathbf{I}=\\sum_{i=1}^g\\mathbf{I}_i, \\\\\n& \\mathbf{H}_i & =\\ & \\mathbf{I}_i - n_i^{-1}\\mathbf{1}_i\\mathbf{1}_i' \\\\\n& \\mathbf{C}_1 & =\\ & \\sum_{i=1}^g\\mathbf{H}_i,\\quad \\mathbf{C}_2=\\sum_{i=1}^gn_i^{-1}\\mathbf{1}_i\\mathbf{1}_i'-n^{-1}\\mathbf{1}\\mathbf{1}'.\n\\end{align*}\\]\nEntonces\n\\[\\begin{align*}\n\\mathbf{C}_1^2 & =\\mathbf{C}_1, & \\mathbf{C}_2^2 & = \\mathbf{C}_2,\\qquad \\mathbf{C}_1\\mathbf{C}_2=\\mathbf{0}, \\\\\nrango(\\mathbf{C}_1) & =\\ n-g, & rango(\\mathbf{C}_2) & = g-1, \\\\\n\\mathbf{W} & = \\mathbf{X}'\\mathbf{C}_1\\mathbf{X}, & \\mathbf{B} & = \\mathbf{X}'\\mathbf{C}_2\\mathbf{X}.\n\\end{align*}\\]\nEl resultado es consecuencia de Teorema 3.1 y Teorema 3.2.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inferencia multivariante en poblaciones normales</span>"
    ]
  },
  {
    "objectID": "t3_inferencia.html#construcción-de-contrastes-de-hipótesis",
    "href": "t3_inferencia.html#construcción-de-contrastes-de-hipótesis",
    "title": "3  Inferencia multivariante en poblaciones normales",
    "section": "3.5 Construcción de contrastes de hipótesis",
    "text": "3.5 Construcción de contrastes de hipótesis\n\n3.5.1 Razón de verosimilitud\nSupongamos que la función de densidad de \\((X_1,\\ldots,X_p)\\) es \\(f(\\mathbf{x},\\boldsymbol{\\theta})\\), donde \\(\\mathbf{x}\\in\\mathbb{R}^p\\) y \\(\\boldsymbol{\\theta}\\in\\boldsymbol{\\Theta}\\), siendo \\(\\boldsymbol{\\Theta}\\) una región paramétrica de dimensión geométrica \\(r\\). Sea \\(\\boldsymbol{\\Theta}_0\\subset\\boldsymbol{\\Theta}\\) una subregión paramétrica de dimensión \\(s\\), y planteamos el test de hipótesis\n\\[H_0:\\boldsymbol{\\theta}\\in\\boldsymbol{\\Theta}_0\\quad\\text{vs}\\quad H_1:\\boldsymbol{\\theta}\\in\\boldsymbol{\\Theta}\\setminus\\boldsymbol{\\Theta}_0\\]\nSea \\(\\mathbf{x}_1,\\ldots,\\mathbf{x}_n\\) una muestra de valores independientes de \\(\\mathbf{X}\\), consideremos la función de verosimilitud\n\\[L(\\mathbf{x}_1,\\ldots,\\mathbf{x}_n;\\boldsymbol{\\theta})=\\prod_{i=1}^nf(\\mathbf{x}_i,\\boldsymbol{\\theta})\\]\ny sea \\(\\widehat{\\boldsymbol{\\theta}}\\) el estimador máximo verosímil de \\(\\boldsymbol{\\theta}\\in\\boldsymbol{\\Theta}\\): Consideremos análogamente \\(\\widehat{\\boldsymbol{\\theta}}_0\\), el estimador de máxima verosimilitud de \\(\\boldsymbol{\\theta}\\in\\boldsymbol{\\Theta}_0\\). Tenemos que \\(\\widehat{\\boldsymbol{\\theta}}\\) maximiza \\(L\\) sin restricciones y \\(\\widehat{\\boldsymbol{\\theta}}_0\\) maximiza \\(L\\) cuando se impone la condición de que pertenezca a \\(\\boldsymbol{\\Theta}_0\\). La razón de verosimilitud es el estadístico\n\\[\\lambda_R=\\frac{L\\left(\\mathbf{x}_1,\\ldots,\\mathbf{x}_n;\\widehat{\\boldsymbol{\\theta}}_0\\right)}{L\\left(\\mathbf{x}_1,\\ldots,\\mathbf{x}_n;\\widehat{\\boldsymbol{\\theta}}\\right)},\\]\nque satisface \\(0\\le\\lambda_R\\le1\\). Aceptamos la hipótesis \\(H_0\\) si \\(\\lambda_R\\) es próxima a 1 y aceptamos la alternativa \\(H_1\\) si \\(\\lambda_R\\) es significativamente próximo a 0.\nEl test basado en \\(\\lambda_R\\) tiene muchas aplicaciones en AM, pero en la mayoría de los casos su distribución es desconocida. Existe un importante resultado (atribuido a Wilks), que dice que la distribución de \\(-2\\) veces el logaritmo de \\(\\lambda_R\\) es ji-cuadrado con \\(r-s\\) g.l. cuando el tamaño de la muestra \\(n\\) es grande.\n\nTeorema 3.5 Bajo ciertas condiciones de regularidad, se verifica:\n\\[-2log\\lambda_R\\text{ es asintóticamente }\\chi^2_{r-s},\\]\ndonde \\(s=dim\\left(\\boldsymbol{\\Theta}_0\\right)&lt;r=dim\\left(\\boldsymbol{\\Theta}\\right)\\).\n\nEntonces rechazamos la hipótesis \\(H_0\\) cuando \\(-2log\\lambda_R\\) sea grande y significativo. Veamos dos ejemplos.\n\n3.5.1.1 Test de independencia\nSi \\((X_1,\\ldots,X_p)\\) es \\(N_p(\\boldsymbol{\\mu},\\mathbf{\\Sigma})\\), y queremos hacer un test sobre la independencia estocástica de las variables, entonces\n\\[\\begin{align*}\n\\boldsymbol{\\Theta}_0 &=\\{(\\boldsymbol{\\mu},\\mathbf{\\Sigma}_0)\\}, & s & =2p \\\\\n\\boldsymbol{\\Theta} &=\\{(\\boldsymbol{\\mu},\\mathbf{\\Sigma})\\}, & r & =p+p(p+1)/2,\n\\end{align*}\\]\ndonde \\(\\mathbf{\\Sigma}_0\\) es la diagonal. \\(\\boldsymbol{\\Theta}_0\\) contiene las \\(p\\) medias de las variables y las \\(p\\) varianzas. \\(\\mathbf{\\Sigma}\\) es cualquier matriz definida positiva. Se demuestra que\n\\[-2log\\lambda_R=-n\\,log|\\mathbf{R}|,\\]\ndonde \\(\\mathbf{R}\\) es la matriz de correlaciones. Es estadístico \\(-n\\,log|\\mathbf{R}|\\) es asintóticamente ji-cuadrado con\n\\[q=p+p(p+1)/2-2p=p(p-1)/2\\quad\\text{g.l.}\\]\nSi las variables son independientes, tendremos que \\(\\mathbf{R}\\approx\\mathbf{I}\\), \\(-n\\,log|\\mathbf{R}|\\approx0\\), y es probable que \\(\\chi_q^2=-n\\,log|\\mathbf{R}|\\) no sea significativo.\n\n\n3.5.1.2 Test de comparación de medias\nConsideremos ahora el test de comparación de medias planteado en la Sección 3.12.3. Ahora\n\\[\\begin{align*}\n\\boldsymbol{\\Theta}_0 &=\\{(\\boldsymbol{\\mu},\\mathbf{\\Sigma})\\}, & s & =p+p(p+1)/2 \\\\\n\\boldsymbol{\\Theta} &=\\{((\\boldsymbol{\\mu}_1,\\ldots,\\boldsymbol{\\mu}_g),\\mathbf{\\Sigma})\\}, & r & =gp+p(p+1)/2,\n\\end{align*}\\]\ndonde \\(\\mathbf{\\Sigma}\\) es matriz definida positiva y \\(\\boldsymbol{\\mu}\\) (vector) es la media común cuando \\(H_0\\) es cierta. Hay \\(gp+p(p+1)/2\\) parámetros bajo \\(H_1\\), y \\(p+p(p+1)/2\\) bajo \\(H_0\\). Se demuestra la relación\n\\[\\lambda_R=\\Lambda^{n/2},\\]\ndonde \\(\\Lambda=|\\mathbf{W}|/|\\mathbf{T}|\\) es la lambda de Wilks y \\(n=n_1+\\cdots+n_g\\). Por lo tanto \\(-n\\,log\\Lambda\\) es asintóticamente ji-cuadrado con \\(r-s=(g-1)p\\) g.l. cuando la hipótesis \\(H_0\\) es cierta.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inferencia multivariante en poblaciones normales</span>"
    ]
  },
  {
    "objectID": "t3_inferencia.html#ejemplos",
    "href": "t3_inferencia.html#ejemplos",
    "title": "3  Inferencia multivariante en poblaciones normales",
    "section": "3.6 Ejemplos",
    "text": "3.6 Ejemplos\n\nEjemplo 3.1 (Moscas) Se desean comparar dos especies de moscas de agua: Amerohelea fascinata, Amerohelea pseudofascinata. En relación a las variables \\(X_1=\\) long. antena, \\(X_2=\\) long. ala (en mm), para dos muestras de tamaños \\(n_1=9\\) y \\(n_2=6\\), se han obtenido las matrices de datos de la Tabla 3.2.\nVectores de medias (valores multiplicados por 100):\n\\[\\overline{\\mathbf{x}}=(141.33,180.44)',\\quad\\overline{\\mathbf{y}}=(122.67,192.67)'.\\]\nMatrices (no centradas) de covarianzas:\n\\[\\mathbf{S}_1=\\left( \\begin{array}{cc}\n87.11 & 71.85\\\\\n71.85 & 150.03\n\\end{array} \\right)\\quad\n\\mathbf{S}_2=\\left( \\begin{array}{cc}\n32.88 & 36.22 \\\\\n36.22 & 64.89\n\\end{array} \\right).\\]\nEstimación centrada de la matriz de covarianzas común:\n\\[\\widehat{\\mathbf{S}}=\\frac{1}{13}(9\\mathbf{S}_1+6\\mathbf{S}_2)=\\left( \\begin{array}{cc}\n75.49 & 66.46 \\\\\n66.46 & 133.81\n\\end{array} \\right).\\]\nDistancia de Mahalanobis entre las dos muestras:\n\\[D^2=(\\overline{\\mathbf{x}}-\\overline{\\mathbf{y}})'\\widehat{\\mathbf{S}}^{-1}(\\overline{\\mathbf{x}}-\\overline{\\mathbf{y}})=15.52.\\]\nEstadístico \\(T^2\\):\n\\[T^2=\\frac{6\\times9}{6+9}D^2=55.87\\]\nEstadístico \\(F\\):\n\\[\\frac{9+6-1-2}{2(9+6-2)}T^2=25.78\\sim F^2_{12}\\]\nDecisión: rechazamos la hipótesis de que las dos especies son iguales (nivel de significación \\(=0.001\\)).\n\n\n\n\nTabla 3.2: \\(X_1=\\) long. antena, \\(X_2=\\) long. ala (en mm), para dos muestras de tamaños \\(n_1=9\\) y \\(n_2=6\\)\n\n\n\n\n\nAmerohelea fascinata\n\nA. pseudofascinata\n\n\n\n\n\n\\(n_1=9\\)\n\n\\(n_2=6\\)\n\n\n\n\\(X_1\\)\n\\(X_2\\)\n\\(X_1\\)\n\\(X_2\\)\n\n\n1.38\n1.64\n1.14\n1.78\n\n\n1.40\n1.70\n1.20\n1.86\n\n\n1.24\n1.72\n1.18\n1.96\n\n\n1.36\n1.74\n1.30\n1.96\n\n\n1.38\n1.82\n1.26\n2.00\n\n\n1.48\n1.82\n1.28\n2.00\n\n\n1.54\n1.82\n\n\n\n\n1.38\n1.90\n\n\n\n\n1.56\n2.08\n\n\n\n\n\n\n\n\n\nEjemplo 3.2 (Flores) Comparación de las especies virginica, versicolor, setosa de flores del género Iris (datos de R. A. Fisher, Tabla 3.3), respecto a las variables que miden longitud y anchura de sépalos y pétalos:\n\\[\\begin{align*}\nX_1 = \\text{longitud de sépalo},\\quad X_2 = \\text{anchura de sépalo},\\\\\nX_3 = \\text{longitud de pétalo},\\quad X_4 = \\text{anchura de pétalo}.\n\\end{align*}\\]\nVectores de medias y tamaños muestrales:\n\\[\\begin{align*}\n& I. setosa & (5.006,3.428,1.462,0.246)\\quad n_1=50\\\\\n& I. versicolor & (5.936,2.770,4.260,1.326)\\quad n_2=50\\\\\n& I. virginica & (6.588,2.974,5.550,2.026)\\quad n_3=50\n\\end{align*}\\]\nMatriz dispersión entre grupos:\n\\[\\mathbf{B}=\\left( \\begin{array}{cccc}\n63.212 & -19.953 & 165.17 & 71.278\\\\\n& 11.345 & -57.23 & -22.932\\\\\n& & 436.73 & 186.69\\\\\n& & & 80.413\n\\end{array} \\right)\\]\nMatriz dispersión dentro grupos:\n\\[\\mathbf{W}=\\left( \\begin{array}{cccc}\n38.956 & 13.630 & 24.703 & 5.645\\\\\n& 16.962 & 8.148 & 4.808\\\\\n& & 27.322 & 6.284\\\\\n& & & 6.156\n\\end{array} \\right)\\]\nLambda de Wilks:\n\\[\\Lambda=\\frac{|\\mathbf{W}|}{|\\mathbf{W}|+|\\mathbf{B}|}=0.02344\\sim\\Lambda(4,147,2).\\] Transformando a una \\(F\\) aplicando Ecuación 3.9:\n\\[\\Lambda\\rightarrow F=198.95\\sim F_{288}^8.\\]\nDecisión: las diferencias entre las tres especies son muy significativas.\n\n\n\n\nTabla 3.3: Longitud y anchura de sépalos y pétalos de 3 especies del género Iris: Setosa, Versicolor, Virginica.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(X_1\\)\n\\(X_2\\)\n\\(X_3\\)\n\\(X_4\\)\n\\(X_1\\)\n\\(X_2\\)\n\\(X_3\\)\n\\(X_4\\)\n\\(X_1\\)\n\\(X_2\\)\n\\(X_3\\)\n\\(X_4\\)\n\n\n\n\n5.1\n3.5\n1.4\n0.2\n7.0\n3.2\n4.7\n1.4\n6.3\n3.3\n6.0\n2.5\n\n\n4.9\n3.0\n1.4\n0.2\n6.4\n3.2\n4.5\n1.5\n5.8\n2.7\n5.1\n1.9\n\n\n4.7\n3.2\n1.3\n0.2\n6.9\n3.1\n4.9\n1.5\n7.1\n3.0\n5.9\n2.1\n\n\n4.6\n3.1\n1.5\n0.2\n5.5\n2.3\n4.0\n1.3\n6.3\n2.9\n5.6\n1.8\n\n\n5.0\n3.6\n1.4\n0.2\n6.5\n2.8\n4.6\n1.5\n6.5\n3.0\n5.8\n2.2\n\n\n5.4\n3.9\n1.7\n0.4\n5.7\n2.8\n4.5\n1.3\n7.6\n3.0\n6.6\n2.1\n\n\n4.6\n3.4\n1.4\n0.3\n6.3\n3.3\n4.7\n1.6\n4.9\n2.5\n4.5\n1.7\n\n\n5.0\n3.4\n1.5\n0.2\n4.9\n2.4\n3.3\n1.0\n7.3\n2.9\n6.3\n1.8\n\n\n4.4\n2.9\n1.4\n0.2\n6.6\n2.9\n4.6\n1.3\n6.7\n2.5\n5.8\n1.8\n\n\n4.9\n3.1\n1.5\n0.1\n5.2\n2.7\n3.9\n1.4\n7.2\n3.6\n6.1\n2.5\n\n\n5.4\n3.7\n1.5\n0.2\n5.0\n2.0\n3.5\n1.0\n6.5\n3.2\n5.1\n2.0\n\n\n4.8\n3.4\n1.6\n0.2\n5.9\n3.0\n4.2\n1.5\n6.4\n2.7\n5.3\n1.9\n\n\n4.8\n3.0\n1.4\n0.1\n6.0\n2.2\n4.0\n1.0\n6.8\n3.0\n5.5\n2.1\n\n\n4.3\n3.0\n1.1\n0.1\n6.1\n2.9\n4.7\n1.4\n5.7\n2.5\n5.0\n2.0\n\n\n5.8\n4.0\n1.2\n0.2\n5.6\n2.9\n3.6\n1.3\n5.8\n2.8\n5.1\n2.4\n\n\n5.7\n4.4\n1.5\n0.4\n6.7\n3.1\n4.4\n1.4\n6.4\n3.2\n5.3\n2.3\n\n\n5.4\n3.9\n1.3\n0.4\n5.6\n3.0\n4.5\n1.5\n6.5\n3.0\n5.5\n1.8\n\n\n5.1\n3.5\n1.4\n0.3\n5.8\n2.7\n4.1\n1.0\n7.7\n3.8\n6.7\n2.2\n\n\n5.7\n3.8\n1.7\n0.3\n6.2\n2.2\n4.5\n1.5\n7.7\n2.6\n6.9\n2.3\n\n\n5.1\n3.8\n1.5\n0.3\n5.6\n2.5\n3.9\n1.1\n6.0\n2.2\n5.0\n1.5\n\n\n5.4\n3.4\n1.7\n0.2\n5.9\n3.2\n4.8\n1.8\n6.9\n3.2\n5.7\n2.3\n\n\n5.1\n3.7\n1.5\n0.4\n6.1\n2.8\n4.0\n1.3\n5.6\n2.8\n4.9\n2.0\n\n\n4.6\n3.6\n1.0\n0.2\n6.3\n2.5\n4.9\n1.5\n7.7\n2.8\n6.7\n2.0\n\n\n5.1\n3.3\n1.7\n0.5\n6.1\n2.8\n4.7\n1.2\n6.3\n2.7\n4.9\n1.8\n\n\n4.8\n3.4\n1.9\n0.2\n6.4\n2.9\n4.3\n1.3\n6.7\n3.3\n5.7\n2.1\n\n\n5.0\n3.0\n1.6\n0.2\n6.6\n3.0\n4.4\n1.4\n7.2\n3.2\n6.0\n1.8\n\n\n5.0\n3.4\n1.6\n0.4\n6.8\n2.8\n4.8\n1.4\n6.2\n2.8\n4.8\n1.8\n\n\n5.2\n3.5\n1.5\n0.2\n6.7\n3.0\n5.0\n1.7\n6.1\n3.0\n4.9\n1.8\n\n\n5.2\n3.4\n1.4\n0.2\n6.0\n2.9\n4.5\n1.5\n6.4\n2.8\n5.6\n2.1\n\n\n4.7\n3.2\n1.6\n0.2\n5.7\n2.6\n3.5\n1.0\n7.2\n3.0\n5.8\n1.6\n\n\n4.8\n3.1\n1.6\n0.2\n5.5\n2.4\n3.8\n1.1\n7.4\n2.8\n6.1\n1.9\n\n\n5.4\n3.4\n1.5\n0.4\n5.5\n2.4\n3.7\n1.0\n7.9\n3.8\n6.4\n2.0\n\n\n5.2\n4.1\n1.5\n0.1\n5.8\n2.7\n3.9\n1.2\n6.4\n2.8\n5.6\n2.2\n\n\n5.5\n4.2\n1.4\n0.2\n6.0\n2.7\n5.1\n1.6\n6.3\n2.8\n5.1\n1.5\n\n\n4.9\n3.1\n1.5\n0.2\n5.4\n3.0\n4.5\n1.5\n6.1\n2.6\n5.6\n1.4\n\n\n5.0\n3.2\n1.2\n0.2\n6.0\n3.4\n4.5\n1.6\n7.7\n3.0\n6.1\n2.3\n\n\n5.5\n3.5\n1.3\n0.2\n6.7\n3.1\n4.7\n1.5\n6.3\n3.4\n5.6\n2.4\n\n\n4.9\n3.6\n1.4\n0.1\n6.3\n2.3\n4.4\n1.3\n6.4\n3.1\n5.5\n1.8\n\n\n4.4\n3.0\n1.3\n0.2\n5.6\n3.0\n4.1\n1.3\n6.0\n3.0\n4.8\n1.8\n\n\n5.1\n3.4\n1.5\n0.2\n5.5\n2.5\n4.0\n1.3\n6.9\n3.1\n5.4\n2.1\n\n\n5.0\n3.5\n1.3\n0.3\n5.5\n2.6\n4.4\n1.2\n6.7\n3.1\n5.6\n2.4\n\n\n4.5\n2.3\n1.3\n0.3\n6.1\n3.0\n4.6\n1.4\n6.9\n3.1\n5.1\n2.3\n\n\n4.4\n3.2\n1.3\n0.2\n5.8\n2.6\n4.0\n1.2\n5.8\n2.7\n5.1\n1.9\n\n\n5.0\n3.5\n1.6\n0.6\n5.0\n2.3\n3.3\n1.0\n6.8\n3.2\n5.9\n2.3\n\n\n5.1\n3.8\n1.9\n0.4\n5.6\n2.7\n4.2\n1.3\n6.7\n3.3\n5.7\n2.5\n\n\n4.8\n3.0\n1.4\n0.3\n5.7\n3.0\n4.2\n1.2\n6.7\n3.0\n5.2\n2.3\n\n\n5.1\n3.8\n1.6\n0.2\n5.7\n2.9\n4.2\n1.3\n6.3\n2.5\n5.0\n1.9\n\n\n4.6\n3.2\n1.4\n0.2\n6.2\n2.9\n4.3\n1.3\n6.5\n3.0\n5.2\n2.0\n\n\n5.3\n3.7\n1.5\n0.2\n5.1\n2.5\n3.0\n1.1\n6.2\n3.4\n5.4\n2.3\n\n\n5.0\n3.3\n1.4\n0.2\n5.7\n2.8\n4.1\n1.3\n5.9\n3.0\n5.1\n1.8\n\n\n\n\n\n\n\nEjemplo 3.3 (Paradoja de Rao) Consideremos los siguientes datos (tamaños muestrales, medias, desviaciones típicas, matrices de covarianzas) de \\(p=2\\) variables \\(X\\) (longitud del fémur), \\(Y\\) (longitud del húmero), obtenidas sobre dos poblaciones (Anglo-indios, Indios).\n\n\n\n\n\nMedias\n\\(X\\)\n\\(Y\\)\n\n\n\n\n\\(n_1=27\\)\n460.4\n335.1\n\n\n\\(n_2=20\\)\n444.3\n323.2\n\n\nDiferencia\n16.1\n11.9\n\n\nDesv. típicas\n23.7\n18.2\n\n\n\n\n\n\n\n\n\n\nMatriz covarianzas\n\n\n\n\n\\[\\widehat{\\mathbf{S}}=\\left( \\begin{array}{cc}\n561.7 & 374.2\\\\\n374.2 & 331.24\n\\end{array}\\right)\\]\n\n\nCorrelación: \\(r=0.867\\)\n\n\n\n\n\nSuponiendo normalidad, los contrastes \\(t\\) de comparación de medias para cada variable por separado son:\n\\[\\begin{align*}\nVariable\\ X\\quad t=2.302\\quad(45 g.l.)\\quad(p=0.0259),\\\\\nVariable\\ Y\\quad t=2.215\\quad(45 g.l.)\\quad(p=0.0318).\n\\end{align*}\\]\nA un nivel de significación del 0,05 se concluye que hay diferencias significativas para cada variable por separado.\nUtilicemos ahora las dos variables conjuntamente. La distancia de Mahalanobis entre las dos poblaciones es \\(\\mathbf{d}'\\widehat{\\mathbf{S}}^{-1}\\mathbf{d}=0.4777\\), siendo \\(\\mathbf{d}=(16.1,11.9)\\). La \\(T^2\\) de Hotelling es\n\\[T^2=\\frac{27\\times20}{27+20}0.4777=5.488\\]\nque convertida a una \\(F\\) da:\n\\[F=\\frac{27+20-1-2}{2(27+20-2)}5.488=2.685\\quad(2\\text{ y }44\\text{ g.l.})\\quad(p=0.079).\\]\nEsta F no es significativa al nivel 0,05. Por lo tanto ambos contrastes univariantes resultan significativos, pero el test bivariante no, contradiciendo la creencia de que un test multivariante debería proporcionar mayor significación que un test univariante.\nInterpretemos geométricamente esta paradoja (conocida como paradoja de Rao). Con nivel de significación 0,05, y aplicando el test \\(T^2\\) de Hotelling, aceptaremos la hipótesis nula bivariante si el vector diferencia \\(d=(x\\ y)'\\) pertenece a la elipse\n\\[\\frac{n_1n_2}{n_1+n_2}\\mathbf{d}'\\left( \\begin{array}{cc}\n561.7 & 374.2\\\\\n374.2 & 331.24\n\\end{array}\\right)^{-1}\\mathbf{d}\\le3.2\\]\ndonde 3.2 es el punto crítico para una \\(F_{44}^2\\). Así pues no hay significación si \\(x,y\\) verifican la inecuación\n\\[0.040369x^2-0.09121xy+0.068456y^2\\le3.2.\\]\nAnálogamente, en el test univariante y para la primera variable \\(x\\), la diferencia \\(d=\\overline{x}_1-\\overline{x}_2\\) debe verificar\n\\[\\left|\\sqrt{\\frac{n_1n_2}{n_1+n_2}}\\left(\\frac{d}{s_1}\\right)\\right|\\le2,\\]\nsiendo 2 el valor crítico para una \\(t\\) con 45 g.l. Procederíamos de forma similar para la segunda variable \\(y\\). Obtenemos así las cuatro rectas\n\\[\\text{Variable }x:\\quad0.143x=\\pm2,\\qquad\\text{Variable }y:\\quad0.1862=\\pm2.\\]\nEn la Figura 3.2 podemos visualizar la paradoja. Los valores de la diferencia que están a la derecha de la recta vertical \\(\\mathbf{r}_x\\) son significativos para la variable \\(x\\): Análogamente los que están por encima de la recta horizontal \\(\\mathbf{r}_y\\) lo son para la \\(y\\): Por otra parte, todos los valores que están fuera de la elipse (región \\(\\mathbf{F}\\)) son significativos para las dos variables. Hay casos en que \\(x,y\\) por separado no son significativos, pero conjuntamente sí. No obstante, existe una pequeña región por encima de \\(\\mathbf{r}_y\\) y a la derecha de \\(\\mathbf{r}_x\\) que cae dentro de la elipse. Para los datos del ejemplo, se obtiene el punto señalado con el signo \\(\\textbf{+}\\), para el cual \\(x\\) e \\(y\\) son significativas pero no \\((x,y)\\): Así \\(x\\) e \\(y\\) son significativas si el punto se encuentra en el cuadrante \\(\\mathbf{A}\\). (Una simetría con respecto al origen nos permitiría considerar otras dos rectas y la región \\(\\mathbf{B}\\)).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigura 3.2: Un test de comparación de poblaciones bivariante puede resultar menos significativo que dos test univariantes con las variables marginales.\n\n\n\nPues bien, el test con \\(x\\) y el test con \\(y\\) por separado, son contrastes \\(t\\) distintos del test \\(T^2\\) empleado con \\((x,y)\\), equivalente a una \\(F\\). Tales contrastes no tienen por qué dar resultados compatibles. Las probabilidades de las regiones de rechazo son distintas. Además, la potencia del test con \\((x,y)\\) es superior, puesto que la probabilidad de la región \\(\\mathbf{F}\\) es mayor que las probabilidades sumadas de las regiones \\(\\mathbf{A}\\) y \\(\\mathbf{B}\\).\n\n\n\n\nFigura 3.1: Función de densidad de una distribución normal bivariante de medias 1 y 1, desviaciones típicas 2 y 2, coeficiente de correlación 0.8\n\n\n\nCuadras, Carles M. 2014. Nuevos Métodos de Análisis Multivariante. https://gc.scalahed.com/recursos/files/r161r/w24899w/Semana5/METODOS_S5.pdf.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inferencia multivariante en poblaciones normales</span>"
    ]
  },
  {
    "objectID": "t4_trd.html",
    "href": "t4_trd.html",
    "title": "4  Técnicas de reducción de Dimensionalidad",
    "section": "",
    "text": "4.1 Análisis de Componentes Principales\nEl análisis de componentes principales, PCA por sus siglas en inglés (Principal Component Analysis), es uno de los métodos más populares del análisis multivariado.\nEl PCA se utiliza cuando deseamos obtener una representación en menor dimensión para un conjunto de variables cuantitativas correlacionadas y queremos expresar la información importante como un conjunto de pocas variables nuevas llamadas componentes principales. Estas componentes se corresponden con una combinación lineal de las variables originales.\nDado que la información de un conjunto de datos se corresponde a la variación total que contiene, el objetivo del PCA es identificar direcciones (o componentes principales) a lo largo de las cuales la variación en los datos es máxima.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Técnicas de reducción de Dimensionalidad</span>"
    ]
  },
  {
    "objectID": "t4_trd.html#análisis-de-componentes-principales",
    "href": "t4_trd.html#análisis-de-componentes-principales",
    "title": "4  Técnicas de reducción de Dimensionalidad",
    "section": "",
    "text": "4.1.1 Matriz (tabla) de datos\nSupongamos que disponemos de una tabla de datos con los valores de \\(p\\)-variables en \\(n\\) elementos de una población arreglados en una matriz \\(\\mathbf{X}\\) de la siguiente forma:\n\n\n\n\n\n\n\n\n\n\n\n\nID\n\\(\\bf{x}_1\\)\n\\(\\bf{x}_2\\)\n\\(\\ldots\\)\n\\(\\bf{x}_p\\)\n\\(\\bf{v}_1\\)\n\\(\\bf{v}_2\\)\n\n\n\n\n\\(1\\)\n\\(x_{11}\\)\n\\(x_{12}\\)\n\\(\\ldots\\)\n\\(x_{1p}\\)\n\\(v_{11}\\)\n\\(v_{12}\\)\n\n\n\\(2\\)\n\\(x_{21}\\)\n\\(x_{22}\\)\n\\(\\ldots\\)\n\\(x_{2p}\\)\n\\(v_{21}\\)\n\\(v_{22}\\)\n\n\n\\(3\\)\n\\(x_{31}\\)\n\\(x_{32}\\)\n\\(\\ldots\\)\n\\(x_{3p}\\)\n\\(v_{31}\\)\n\\(v_{32}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\ddots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(n\\)\n\\(x_{n1}\\)\n\\(x_{n2}\\)\n\\(\\ldots\\)\n\\(x_{np}\\)\n\\(v_{n1}\\)\n\\(v_{n2}\\)\n\n\n\n\nDonde las variables \\(x_1,\\ldots, x_n\\) describen a los \\(n\\) individuos observados.\nLas variables \\(v_1\\), \\(v_2\\) son de perfil (o explicativas) y ayudan a interpretar la variabilidad de los datos.\n\nEl objetivo del análisis es la reducción de la dimensionalidad. Buscamos un espacio de variables más reducido y fácil de interpretar.\nEl problema es que si reducimos el número de variables es posible que “perdamos parte toda la variabilidad de los datos originales”.\nAsí la idea básica es consentir una pérdida de información para lograr una ganancia en la significación.\n\n\n\n\n\n\n\n4.1.2 Enfoque geométrico\nSupongamos que \\(p=2\\) y que la nube de puntos de nuestra matriz de datos es la de la siguiente figura:\n\n\n\n\n\n\n\n\n\nLa siguiente figura muestra las dos componentes principales, es decir, las direcciones de las proyecciones que tienen máxima variabilidad.\n\n\n\n\n\nSi proyectamos en la dirección de la primera componente obtendremos las proyecciones siguientes (en color azul):\n\n\n\n\n\nLo que significa que la varianza de los puntos azules es máxima; en el sentido de que cualquier otra dirección o recta, las proyecciones sobre ésta tendrán a lo más igual varianza.\nLos puntos azules representan las coordenadas que tienen los puntos de nuestra tabla de datos (centrada) tomando como eje de abcisas la primera componente \\(CP_1\\).\nSi proyectamos en la dirección de la “segunda componente”, obtendremos las proyecciones siguientes (en color verde):",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Técnicas de reducción de Dimensionalidad</span>"
    ]
  },
  {
    "objectID": "t4_trd.html#cálculo-de-las-componentes",
    "href": "t4_trd.html#cálculo-de-las-componentes",
    "title": "4  Técnicas de reducción de Dimensionalidad",
    "section": "4.2 Cálculo de las componentes",
    "text": "4.2 Cálculo de las componentes\n\n4.2.1 Cálculo de la primera componente\nLa primera componente principal se define como la combinación lineal de las variables originales que tiene varianza máxima. Los valores en esta primera componente de los \\(n\\) individuos se representarán por un vector \\(\\mathbf{z_1}\\) dado por \\[\\mathbf{z_1}=\\mathbf{Xa_1}.\\] Si las variables originales tienen media cero, \\(\\mathbf{z_1}\\) también tendrá media nula. Su varianza será\n\\[\\begin{equation} \\tag{1}\nVar(\\mathbf{z_1})=\\frac{1}{n}\\mathbf{z_1^t}\\mathbf{z_1}=\\frac{1}{n}\\mathbf{a_1^t X^t}\\mathbf{Xa_1}=\\mathbf{a_1^t S}\\mathbf{a_1}\n\\end{equation}\\]\ndonde \\(\\mathbf{S}\\) es la matriz de varianzas-covarianzas de las observaciones.\nEs obvio que podemos maximizar la varianza tanto como queramos aumentando el módulo del vector \\(\\mathbf{a_1}\\). Para que maximizar (1) tenga solución debemos imponer una restricción al módulo del vector \\(\\mathbf{a_1}\\), y, sin pérdida de generalidad, impondremos que \\(\\mathbf{a_1^t}\\mathbf{a_1}=1\\). Introducimos esta restricción mediante el multiplicador de Lagrange: \\[M=\\mathbf{a_1^t}S\\mathbf{a_1}-\\lambda(\\mathbf{a_1^t}\\mathbf{a_1}-1)\\] y maximizamos esta expresión derivando respecto a las componentes de \\(\\mathbf{a_1}\\) e igualando a cero. Entonces \\[\\frac{\\partial M}{\\partial \\mathbf{a_1}}=2\\mathbf{Sa_1}-2 \\lambda \\mathbf{a_1}=0,\\]\ncuya solución es:\n\\[\\begin{equation} \\tag{2}\n\\mathbf{Sa_1}= \\lambda \\mathbf{a_1},\n\\end{equation}\\]\nque implica que \\(\\mathbf{a_1}\\) es un vector propio de la matriz \\(\\mathbf{S}\\), y \\(\\lambda\\) su correspondiente valor propio.\nPara determinar qué valor propio de \\(\\mathbf{S}\\) es la solución de (2), multiplicamos por la izquierda por \\(\\mathbf{a_1^t}\\) a esta ecuación,\n\\[\\begin{equation}\n\\mathbf{a_1^t S a_1}= \\lambda \\mathbf{a_1^t a_1}= \\lambda\n\\end{equation}\\]\ny concluimos, por (1) que \\(\\lambda\\) es la varianza de \\(\\mathbf{z_1}\\). Como esta es la cantidad que queremos maximizar, \\(\\lambda\\) será el mayor valor propio de la matriz \\(\\mathbf{S}\\). Su vector asociado, \\(\\mathbf{a_1}\\), define los coeficientes de cada variable en la primera componente principal.\n\n\n4.2.2 Ejemplo:\nCalculad la primera componente principal con los logaritmos del fichero acciones.txt que podéis descargad en Aula Digital. Las observaciones corresponden a distintas acciones que cotizan en el mercado español y las variables son:\n\nV1 la rentabilidad efectiva por dividendos,\nV2 la proporción de beneficios que va a dividendos\nV3 el ratio entre precio por acción y beneficios.\n\n¿Cuál de las variables está mejor representada por la componente principal 1?\nSolución\nCargamos los datos\n\ndatos&lt;-read.table(\"datos/acciones.txt\")\nn=dim(datos)[1]\nn\n\n[1] 34\n\n\nLa tabla presenta tres medidas de rentabilidad de 34 acciones en bolsa. Vamos a reescribirlas para ganar interpretabilidad. Llamamos \\(d\\) a los dividendos por acción, \\(p\\) al precio de la acción, \\(B\\) al beneficio y \\(N\\) al número de acciones. Entonces:\n\nV1 es la rentabilidad efectiva por dividendos, es decir, dividendos repartidos por acción divididos por precio de la acción. \\(V1=\\frac{d}{p}\\).\nV2 es la proporción de beneficios que va a dividendos. \\(V2=\\frac{dN}{B}\\).\nV3 es el cociente entre precio y beneficio por acción. \\(V3=\\frac{p}{B/N}=\\frac{pN}{B}\\)\n\nExploramos los datos\n\n\n\n\n\n\n\n\n\nLas densidades indican un alejamiento de la distribución normal para las tres variables. Las dos primeras sugieren la presencia de dos grupos de datos distintos (acciones con comportamientos distintos), y la tercera tiene una densidad muy asimétrica, con al menos un valor atípico.\nPor la forma de cálculo de las variables, es lógico esperar alta correlación positiva entre V1 y v2. La correlación negativa baja entre V1 y v3, así como alta negativa entre V2 yv3.\nAhora, estimamos la matriz de varianzas-covarianzas de las variables originales V1, v2 y V3, con el estimador sesgado:\n\nS0=round(((n-1)/n)*cov(datos),2)\nS0\n\n       V1     V2     V3\nV1  28.24  97.49 -15.24\nV2  97.49 559.28 -18.00\nV3 -15.24 -18.00  21.90\n\n\nRecordad que las densidades de las tres variables han mostrado una clara falta de normalidad y entre ellas hay relaciones no lineales. En estas condiciones, la matriz de varianzas-covarianzas no es un buen resumen de las relaciones de dependencia existentes.\nPara tratar de resolver el problema anterior, podemos usar el logaritmo que es una de las transformaciones más utilizadas para datos positivos en los siguientes casos:\n\nLos datos describen el tamaño de las cosas (renta de países o familias habitantes en las principales ciudades del mundo, tamaño de empresas, consumo de energía en hogares, etc), son generalmente muy asimétricas, pero se convierten en aproximadamente simétricas al expresar la variable en logaritmos.\nCuando las diferencias relativas entre los valores de la variable son importantes, conviene expresar las variables en logaritmos, ya que las diferencias entre logaritmos equivalen a diferencias relativas en la escala original.\nLa variabilidad de las variables transformadas es independiente de las unidades de medida. Para mostrar esta propiedad, supongamos que tenemos una sola variable aleatoria \\(X\\) que transformamos con \\(Y = \\log X\\), la variable transformada tiene media \\(\\mu_Y\\) y varianza \\(\\sigma^2_Y\\). Si cambiamos las unidades de medida de \\(X\\) multiplicando por una constante, \\(Z = kX\\), entonces la variable \\(\\log Z\\) tiene media \\(Y + \\log k\\) y la misma varianza que la variable \\(\\log X\\). Por tanto, al tomar logaritmos en las variables, las varianzas pueden compararse aunque los datos tengan distintas dimensiones.\n\nDe acuerdo a los anterior, aplicamos una transformación logarítmica a nuestros datos, con lo cual, la matriz de varianzas-covarianzas de las variables transformadas, sería:\n\ndatos_l=log(datos)\nS=round(((n-1)/n)*cov(datos_l),2)\nS\n\n      V1    V2    V3\nV1  0.35  0.15 -0.19\nV2  0.15  0.13 -0.03\nV3 -0.19 -0.03  0.16\n\n\nObservad que los logaritmos modifican mucho los resultados. Los datos ahora son más homogéneos y la variable de mayor varianza pasa a ser la primera, el logaritmo de la rentabilidad efectiva, mientras que la menor es la segunda, el logaritmo de la proporción de beneficios que va a dividendos. La relación entre el logaritmo del ratio precio/beneficios y la rentabilidad efectiva es negativa. Las otras relaciones son débiles.\nCalculamos los valores propios de la matriz de varianzas covarianzas de los datos transformados que son las raíces de la ecuación\n\\[\n\\begin{equation}\n\\begin{split}\n|S-\\lambda I| & =  \\left| \\begin{pmatrix}0.35 & 0.15 & -0.19\\\\ 0.15 & 0.13 & -0.03 \\\\ -0.19 & -0.03 & 0.16\\end{pmatrix} -\\begin{pmatrix} \\lambda & 0 & 0\\\\ 0 & \\lambda & 0 \\\\ 0 & 0 & \\lambda\\end{pmatrix}  \\right| \\\\ \\\\\n& = 0.000382-0.0628\\lambda+0.64 \\lambda^2 -\\lambda^3 =0\n\\end{split}\n\\end{equation}\n\\]\nBuscamos las raíces de este polinomio son\n\nlibrary(polynom)\np=polynomial(coef=c(0.00038,-0.0628,0.64,-1))\nraices=round(solve(p),3)\n\nLas raíces son \\(\\lambda_1\\)=0.521, \\(\\lambda_2\\)=0.113 y \\(\\lambda_3\\)=0.006.\nEl vector propio asociado a \\(lambda_1\\) nos da los pesos de la primera componente principal. Para calcularlo manualmente, debemos resolver el sistema \\[S \\mathbf{a_1}= \\lambda_1 \\mathbf{a_1}\\] que conduce a\n\\[\n\\begin{equation}\n\\begin{split}\n\\begin{pmatrix}0.35 & 0.15 & -0.19\\\\ 0.15 & 0.13 & -0.03 \\\\ -0.19 & -0.03 & 0.16\\end{pmatrix} \\begin{pmatrix} a_{11} \\\\a_{12} \\\\ a_{131} \\end{pmatrix} &=0.521 \\cdot \\begin{pmatrix} a_{11} \\\\a_{12} \\\\ a_{131} \\end{pmatrix}\n\\end{split}\n\\end{equation}\n\\] \\[\n\\begin{equation}\n\\begin{split}\n\\begin{pmatrix}-0.171 a_{11}+0.15 a_{12} -0.19 a_{13} \\\\ 0.15 a_{11}-0.391 a_{12} -0.03 a_{13}\\\\ -0.19 a_{11}-0.03 a_{12} -0.361 a_{13}\\end{pmatrix}  &= \\begin{pmatrix} 0 \\\\0 \\\\ 0 \\end{pmatrix}\n\\end{split}\n\\end{equation}\n\\] Este sistema es compatible indeterminado. Para encontrar una de las infinitas soluciones tomemos la primera variable como parámetro, \\(x\\), y resolvemos el sistema en función de \\(x\\). La solución es,\n\\[\\{a_{11}=x,\\; a_{12}=0.427x,\\; a_{13}=-0.562x\\]\nEl valor de \\(x\\) lo obtenemos imponiendo que el vector tenga norma uno, con lo que resulta:\n\\[\\mathbf{a_1}=\\begin{pmatrix} -0.817 \\\\-0.349 \\\\ 0.459 \\end{pmatrix}\\]\nPor lo tanto, la primera componente es \\[Z_1=-0.817 \\log(d/p)-0.349 \\log(dN/p)+0.459 \\log(pN/B)\\]\nque indica que este primer componente depende básicamente de la rentabilidad por dividendos. Esta variable es la que mejor explica la variabilidad conjunta de las acciones.\n\n\n4.2.3 Cálculo de la segunda componente\nVamos a obtener el mejor plano de proyección de l matriz \\(\\mathbf{X}\\). Lo calcularemos estableciendo como función objetivo que la suma de las varianzas de \\(\\mathbf{z_1}=\\mathbf{Xa_1}\\) y \\(\\mathbf{z_2}=\\mathbf{Xa_2}\\) sean máximas, donde \\(\\mathbf{a_1}\\) y \\(\\mathbf{a_2}\\) son los vectores que definen el plano. La función objetivo será:\n\\[\\begin{equation} \\tag{3}\n\\phi=\\mathbf{a_1^t S}\\mathbf{a_1} + \\mathbf{a_2^t S}\\mathbf{a_2} - \\lambda_1 (\\mathbf{a_1^t}\\mathbf{a_1}-1) - \\lambda_2 (\\mathbf{a_2^t}\\mathbf{a_2}-1)\n\\end{equation}\\]\nque incorpora las restricciones de que las direcciones deben de tener módulo unitario. Derivando e igualando a cero: \\[\\frac{\\partial \\phi}{\\partial \\mathbf{a_1}}=2\\mathbf{Sa_1}-2 \\lambda_1 \\mathbf{a_1}=0\\]\n\\[\\frac{\\partial \\phi}{\\partial \\mathbf{a_2}}=2\\mathbf{Sa_2}-2 \\lambda_2 \\mathbf{a_1}=0\\]\nLa solución del sistema es: \\[\\begin{equation} \\tag{4}\n\\mathbf{Sa_1}= \\lambda \\mathbf{a_1},\n\\end{equation}\\] \\[\\begin{equation} \\tag{5}\n\\mathbf{Sa_2}= \\lambda \\mathbf{a_2},\n\\end{equation}\\]\nque indica que \\(\\mathbf{a_1}\\) y \\(\\mathbf{a_2}\\) deben ser vectores propios de \\(\\mathbf{S}\\).\nTomando los vectores propios de norma uno y sustituyendo en (3), se obtiene que, en el máximo, la función objetivo es \\[\\begin{equation} \\tag{6}\n\\phi=\\lambda_1+\\lambda_2\n\\end{equation}\\]\nes claro que \\(\\lambda_1\\) y \\(\\lambda_2\\) deben ser los dos valores propios mayores de la matriz \\(\\mathbf{S}\\) y \\(\\mathbf{a_1}\\) y \\(\\mathbf{a_2}\\) sus correspondientes vectores propios.\nObservad que la covarianza entre \\(\\mathbf{z_1}\\) y \\(\\mathbf{z_2}\\), dada por \\(\\mathbf{a_1^t S a_2}\\) es cero ya que =0, y las variables\\(\\mathbf{z_1}\\) y \\(\\mathbf{z_2}\\) estarán incorreladas.\nSe puede demostrarse que si en lugar de maximizar la suma de varianzas, que es la traza de la matriz de covarianzas de la proyección, se maximiza la varianza generalizada (el determinante de la matriz de covarianzas) se obtiene el mismo resultado.\n\n4.2.3.1 Generalización\nAnálogamente, el espacio de dimensión \\(r\\) que mejor representa a los puntos viene definido por los vectores propios asociados a los \\(r\\) mayores valores propios de \\(\\mathbf{S}\\). Estas direcciones se denominan direcciones principales de los datos y a las nuevas variables por ellas definidas componentes principales. En general, la matriz \\(\\mathbf{X}\\) (y por tanto la \\(\\mathbf{S}\\)) tiene rango \\(p\\), existiendo entonces tantas componentes principales como variables que se obtendrán calculando los valores propios o raíces características, \\(\\lambda_1, \\ldots, \\lambda_p\\), de la matriz de varianzas y covarianzas de las variables, \\(\\mathbf{S}\\) , mediante:\n\\[\\begin{equation} \\tag{7}\n|\\mathbf{S}-\\lambda\\mathbf{I}|=0\n\\end{equation}\\]\ny sus vectores asociados son: \\[\\begin{equation} \\tag{8}\n(\\mathbf{S}-\\lambda_i\\mathbf{I})\\mathbf{a_i}=0.\n\\end{equation}\\]\nLos términos \\(\\lambda_i\\) son reales, al ser la matriz \\(\\mathbf{S}\\) simétrica, y positivos, ya que \\(\\mathbf{S}\\) es definida positiva.\nPor ser \\(\\mathbf{S}\\) simétrica si \\(\\lambda_j\\) y \\(\\lambda_h\\) son dos raíces distintas sus vectores asociados son ortogonales.\nSi \\(\\mathbf{S}\\) fuese semidefinida positiva de rango \\(r &lt; p\\), lo que ocurriría si \\(p−r\\) variables fuesen combinación lineal de las demás, habría solamente \\(r\\) raíces características positivas y el resto serían ceros.\nLlamando \\(\\mathbf{Z}\\) a la matriz cuyas columnas son los valores de las \\(p\\) componentes en los \\(n\\) individuos, estas nuevas variables están relacionadas con las originales mediante:\n\\[\\begin{equation} \\tag{9}\n\\mathbf{Z}=\\mathbf{X}\\mathbf{A}\n\\end{equation}\\]\ndonde \\(\\mathbf{A^t A}=\\mathbf{I}\\). Calcular las componentes principales equivale a aplicar una transformación ortogonal \\(\\mathbf{A}\\) a las variables \\(\\mathbf{X}\\) (ejes originales) para obtener unas nuevas variables \\(\\mathbf{Z}\\) incorreladas entre sí. Esta operación puede interpretarse como elegir unos nuevos ejes coordenados, que coincidan con los “ejes naturales” de los datos.\n\n\n4.2.3.2 Ejemplo:\nVamos a realizar el análisis de componentes principales (PCA) sobre el conjunto de datos de las acciones del mercado español, esta vez utilizando a R para calcular los valores y vectores propios.\nRecordad que ya hemos calculado la matriz de varianzas-covarianzas muestral sesgada de los datos transformados, \\(S\\)=0.35, 0.15, -0.19, 0.15, 0.13, -0.03, -0.19, -0.03, 0.16.\n\nsol=eigen(S)\n\nLos valores propios son:\n\\[\\lambda_1=0.521,\\quad \\lambda_2=0.113,\\quad \\lambda_3 = 0.007.\\]\n\nLos vectores propios ortonormales correspondientes a los valores propios, son los que aparecen con el nombre: $vectors\n\nLas expresiones de las variables nuevas \\(CP_i\\) en función de los logaritmos de las originales son:\n\\[\\begin{array}{rl}\nZ_1 = & 0.817 \\cdot \\log V_1 + 0.349\\cdot \\log V_2 - 0.459 \\cdot \\log V_3, \\\\\nZ_2 = & 0.043 \\cdot \\log V_1 + 0.758\\cdot \\log V_2 + 0.651 \\cdot \\log V_3, \\\\\nZ_3 = & 0.575 \\cdot \\log V_1 - 0.552 \\cdot \\log V_2 + 0.604 \\cdot \\log V_3, \\\\\n\\end{array}\\]\nLa nueva matriz de datos respecto de las componentes principales será:\n\nX=matrix(c(datos_l$V1,datos_l$V2,datos_l$V3),nrow=34)\nZ=X %*% sol$vectors # Z=XA  con A la matriz de vectores propios\nhead(Z)\n\n         [,1]     [,2]       [,3]\n[1,] 1.003343 5.678343 0.28103506\n[2,] 1.680545 4.608193 0.10346897\n[3,] 1.487526 4.652723 0.15673341\n[4,] 1.257073 4.543832 0.05520223\n[5,] 1.866053 4.187286 0.12455276\n[6,] 1.637393 3.919281 0.21592221\n\n\nSi representamos gráficamente las dos primeras componentes, podemos observar que se puede distinguir entre los dos grupos de acciones.\n\nplot(Z[,1],Z[,2])\n\n\n\n\n\n\n\n\n¿Qué hubiese pasado si centramos los datos?\nAunque no es obligatorio centrar la matriz de datos para PCA, es una práctica común y recomendada, ya que ayuda a garantizar que las componentes principales reflejen de manera más precisa la estructura de variabilidad en los datos.\nSi no se centran los datos, la primera componente principal estaría influenciada por la ubicación de los datos en el espacio original, es decir, por la media de los datos. En nuestro ejemplo, esto está minimizado por el efecto de la transformación logarítmica.\nVeamos los resultados centrando los datos\n\ncolMeans(X)\n\n[1] 2.071865 4.178913 2.117252\n\nHn=diag(n)-1/n # matriz de centrado\ncX=Hn%*%X # matriz centrada\nround(cX,3)\n\n        [,1]   [,2]   [,3]\n [1,] -0.848  0.318  1.291\n [2,] -0.443 -0.159  0.175\n [3,] -0.568 -0.222  0.325\n [4,] -0.819 -0.329  0.299\n [5,] -0.297 -0.425 -0.171\n [6,] -0.443 -0.758 -0.186\n [7,] -0.546 -0.014  0.351\n [8,] -0.462 -0.247  0.145\n [9,] -0.909 -0.183  0.571\n[10,] -0.848 -0.357  0.463\n[11,] -0.200 -0.493 -0.469\n[12,] -0.590 -0.483  0.500\n[13,] -0.443 -0.220  0.281\n[14,] -0.314 -0.397 -0.038\n[15,] -0.546 -0.212  0.550\n[16,] -0.098  0.008 -0.063\n[17,] -0.098 -0.117 -0.076\n[18,] -0.590 -0.110  0.376\n[19,] -0.018  0.255  0.281\n[20,]  0.701  0.391 -0.325\n[21,]  0.744  0.426 -0.200\n[22,]  0.649  0.346 -0.469\n[23,]  0.790  0.425 -0.200\n[24,]  0.713  0.359 -0.309\n[25,]  0.616  0.426 -0.230\n[26,]  0.656  0.425 -0.342\n[27,]  0.688  0.426 -0.186\n[28,]  0.835  0.389 -0.377\n[29,]  0.694  0.426 -0.309\n[30,]  0.707  0.348 -0.309\n[31,]  0.200  0.294 -0.076\n[32,] -0.140 -0.197 -0.230\n[33,]  0.595  0.296 -0.469\n[34,]  0.629 -0.638 -0.572\n\n\n\nSc&lt;-(1/n)*t(cX)%*%Hn%*%cX # estimador sesgado de la matriz de covarianza\nround(Sc,3) # daría igual cov(cX)*(n-1)/n\n\n       [,1]   [,2]   [,3]\n[1,]  0.352  0.147 -0.188\n[2,]  0.147  0.131 -0.031\n[3,] -0.188 -0.031  0.158\n\nsolc&lt;-eigen(Sc)\n\n\nZc=cX %*% solc$vectors \nplot(Zc[,1],Zc[,2])",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Técnicas de reducción de Dimensionalidad</span>"
    ]
  },
  {
    "objectID": "t4_trd.html#propiedades-del-acp",
    "href": "t4_trd.html#propiedades-del-acp",
    "title": "4  Técnicas de reducción de Dimensionalidad",
    "section": "4.3 Propiedades del ACP",
    "text": "4.3 Propiedades del ACP\n\n4.3.1 ACP con la matriz de covarianzas\nLos componentes principales como nuevas variables tienen las propiedades siguientes:\n\nLas componentes principales reproducen la varianza total. \\[\\sum_{i=1}^p Var(\\mathbf{Z}_i)=\\sum_{i=1}^p \\lambda_i=tr(\\mathbf{S})=\\sum_{i=1}^p s_i^2\\].\nLos componentes principales tienen correlación cero entre sí (son incorrelados) por lo tanto su matriz de covarianzas es\n\n\\[\\mathbf{S}_{Z}=\\left(\\begin{array}{cccc}\n\\lambda_1& 0 &\\ldots &0\\\\\n0& \\lambda_{2}&\\ldots & 0\\\\\n\\vdots & \\vdots & & \\vdots\\\\\n0 & 0&\\ldots &\\lambda_{p}\n\\end{array}\n\\right)\\]\n\\(\\det(\\mathbf{S}_{Z})=\\prod_{i=1}^p \\lambda_i =\\det(\\mathbf{S})\\). Luego los componentes principales conservan la varianza generalizada.\n\nLa proporción de varianza explicada por la componente \\(j\\)-ésima es \\[\\frac{\\lambda_j}{\\sum_{i=1}^p \\lambda_i}.\\]\n\nAdemás al ser incorrelados la proporción de varianza explicada por los \\(k\\) primeros componentes es \\[\\frac{\\sum_{i=1}^k \\lambda_i}{\\sum_{i=1}^p \\lambda_i}.\\] \n\\(\\mbox{Cov}(\\tilde{\\mathbf{X}}_i, \\mathbf{Z}_j)=\\frac{\\sqrt{\\lambda_j} a_{j i}}{s_i}\\), donde \\(a_{j i}\\) es la \\(i\\)-ésima componente del vector propio \\(\\mathbf{a}_j\\).\n\n\n4.3.2 ACP normado o con matriz de correlaciones\nLas componentes principales se obtienen maximizando la varianza de la proyección. En términos de las variables originales esto supone maximizar:\n\\[M=\\displaystyle \\sum_{i=1}^p a_i^2s_i^2+ 2 \\displaystyle \\sum_{i=1}^p  \\displaystyle \\sum_{j=i+1}^p a_i a_j s_{ij}\\] con la restricción \\(\\mathbf{a^ta}=1\\). Si alguna de las variables, por ejemplo la primera, tiene una varianza \\(s^2_1\\), mayor que las demás, la manera de aumentar \\(M\\) es hacer tan grande como podamos la coordenada \\(a_1\\) asociada a esta variable. En el límite si una variable tiene una varianza mucho mayor que las demás el primer componente principal coincidirá muy aproximadamente con esta variable.\nCuando las variables tienen unidades distintas esta propiedad no es conveniente: si disminuimos la escala de medida de una variable cualquiera, de manera que aumenten en magnitud sus valores numéricos (pasamos por ejemplo de medir en km. a medir en metros), el peso de esa variable en el análisis aumentará, ya que en la ecuación anterior:\n\nsu varianza será mayor y aumentará su coeficiente en el componente, \\(a_1\\), ya que contribuye más a aumentar M;\nsus covarianzas con todas las variables aumentarán, con el consiguiente efecto de incrementar \\(a_i\\).\n\nEn resumen, cuando las escalas de medida de las variables son muy distintas, la maximización de \\(M\\) dependerá decisivamente de estas escalas de medida y las variables con valores más grandes tendrán más peso en el análisis.\nSi queremos evitar este problema, conviene estandarizar las variables antes de calcular los componentes, de manera que las magnitudes de los valores numéricos de las variables \\(X\\) sean similares.\nLa estandarización resuelve otro posible problema. Si las variabilidades de la \\(X\\) son muy distintas, las variables con mayor varianza van a influir más en la determinación de la primera componente. Este problema se evita al estandarizar las variables, ya que entonces las varianzas son la unidad, y las covarianzas son los coeficientes de correlación. La ecuación a maximizar se transforma en:\n\\[M'=1 + 2 \\displaystyle \\sum_{i=1}^p  \\displaystyle \\sum_{j=i+1}^p a_i a_j r_{ij}\\] siendo \\(r_{ij}\\) el coeficiente de correlación lineal entre las variables \\(i\\), \\(j\\). En consecuencia la solución depende de la correlaciones y no de las varianzas.\nLas componentes principales normados se obtiene calculando los vectores y valores propios de la matriz R, de coeficientes de correlación. Llamando \\(\\lambda_p^R\\) a las raíces características de esa matriz, que suponemos no singular, se verifica que: \\[{\\sum_{i=1}^p \\lambda_i^R}=traza(R)=p\\] Las propiedades de las componentes extraídos de \\(R\\) son:\n\nLa proporción de variación explicada por \\(\\lambda_p^R\\) será \\(\\frac{\\lambda_p^R}{p}\\).\nLas correlaciones entre cada componente \\(z_j\\) y las variables \\(X\\) originales vienen dadas directamente por \\(a^t_j \\sqrt{\\lambda_j}\\), siendo \\(\\mathbf{z_j}=\\mathbf{Xa_j}\\).\n\nEstas propiedades son consecuencia inmediata de los resultados de la sección anterior.\n\nCuando las variables X originales están en distintas unidades conviene aplicar el análisis de la matriz de correlaciones o análisis normado.\nCuando las variables tienen las mismas unidades, ambas alternativas son posibles.\nSi las diferencias entre las varianzas de las variables son informativas y queremos tenerlas en cuenta en el análisis no debemos estandarizar las variables: por ejemplo, supongamos dos índices con la misma base pero uno fluctua mucho y el otro es casi constante. Este hecho es informativo, y para tenerlo en cuenta en el análisis, no se deben estandarizar las variables, de manera que el índice de mayor variabilidad tenga más peso. Por el contrario, si las diferencias de variabilidad no son relevantes podemos eliminarlas con el análisis normado. En caso de duda, conviene realizar ambos análisis, y seleccionar aquel que conduzca a conclusiones más informativas.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Técnicas de reducción de Dimensionalidad</span>"
    ]
  },
  {
    "objectID": "t4_trd.html#análisis-de-componentes-principales-en-r",
    "href": "t4_trd.html#análisis-de-componentes-principales-en-r",
    "title": "4  Técnicas de reducción de Dimensionalidad",
    "section": "4.4 Análisis de Componentes Principales en R",
    "text": "4.4 Análisis de Componentes Principales en R\nVamos a explicar las funciones de R para hacer el PCA, para ello utilizaremos los famosos datos de las flores iris.\n\n\n\nhead(iris,2)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n\ntail(iris,2)\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n149          6.2         3.4          5.4         2.3 virginica\n150          5.9         3.0          5.1         1.8 virginica\n\n\nLo primero que hacemos es revisar si las variables están correlacionadas, requisito necesario para obtener una representación más simple de éstas.\n\ncov(iris[,1:4])\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length    0.6856935  -0.0424340    1.2743154   0.5162707\nSepal.Width    -0.0424340   0.1899794   -0.3296564  -0.1216394\nPetal.Length    1.2743154  -0.3296564    3.1162779   1.2956094\nPetal.Width     0.5162707  -0.1216394    1.2956094   0.5810063\n\ncor(iris[,1:4])\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length    1.0000000  -0.1175698    0.8717538   0.8179411\nSepal.Width    -0.1175698   1.0000000   -0.4284401  -0.3661259\nPetal.Length    0.8717538  -0.4284401    1.0000000   0.9628654\nPetal.Width     0.8179411  -0.3661259    0.9628654   1.0000000\n\n\nComparamos las funciones de R para hacer el PCA\n\n\n\n\n\nCalculamos las componentes con los datos escalados con la librería factoextra\n\nlibrary(ggplot2)\nlibrary(\"factoextra\")\niris.acp=prcomp(iris[,1:4], scale = TRUE)\n\nLos valores propios muestran el porcentaje de varianza explicada por cada componente principal.\n\nlambdas=get_eigenvalue(iris.acp)\nlambdas\n\n      eigenvalue variance.percent cumulative.variance.percent\nDim.1 2.91849782       72.9624454                    72.96245\nDim.2 0.91403047       22.8507618                    95.81321\nDim.3 0.14675688        3.6689219                    99.48213\nDim.4 0.02071484        0.5178709                   100.00000\n\n\nObservamos que las dos primeras componentes principales explican aproximadamente el 96% de la variación total. Puede ser razonable, trabajar con esas dos componentes para el análisis posterior de estos datos.\nUn método alternativo para determinar el número de componentes principales es observar el diagrama de valores propios ordenados de mayor a menor. El número de componentes se determina en el punto, más allá del cual los valores propios restantes son todos relativamente pequeños y de tamaño comparable.\n\nfviz_eig(iris.acp, addlabels = TRUE, ylim=c(0,100))\n\n\n\n\n\n\n\n\nLa representación de variables difiere de la gráfica de las observaciones: Las observaciones están representadas por sus proyecciones, pero las variables están representados por sus correlaciones.\n\nfviz_pca_var(iris.acp, col.var = \"contrib\",\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             repel = TRUE) \n\n\n\n\n\n\n\n\nEl gráfico anterior también se conoce como círculo de correlación variable. Muestra las relaciones entre todas las variables. Se puede interpretar de la siguiente manera:\n\nLas variables correlacionadas positivamente se agrupan.\nLas variables correlacionadas negativamente se colocan en lados opuestos del origen de la trama (cuadrantes opuestos).\nLa distancia entre variables y el origen mide la calidad de la representación de las variables, las que están alejadas del origen están bien representadas.\n\nLa calidad de representación de las variables se llama cos2 (coseno cuadrado, coordenadas cuadradas). Es posible crear un diagrama de barras de las variables cos2:\n\nvar &lt;- get_pca_var(iris.acp)\nfviz_cos2(iris.acp, choice = \"var\", axes = 1:2)\n\n\n\n\n\n\n\n\n\nUn cos2 alto indica una buena representación de la variable en el componente principal. En este caso, la variable se coloca cerca de la circunferencia del círculo de correlación.\nUn cos2 bajo indica que la variable no está perfectamente representada por los PC. En este caso, la variable está cerca del centro del círculo.\n\nPara ver como se relacionan las componentes principales con los datos originales, veamos los autovectores.\n\niris.acp$rotation\n\n                    PC1         PC2        PC3        PC4\nSepal.Length  0.5210659 -0.37741762  0.7195664  0.2612863\nSepal.Width  -0.2693474 -0.92329566 -0.2443818 -0.1235096\nPetal.Length  0.5804131 -0.02449161 -0.1421264 -0.8014492\nPetal.Width   0.5648565 -0.06694199 -0.6342727  0.5235971\n\n\nLa primera componente principal da aproximadamente el mismo peso a la longitud del sépalo, la longitud del pétalo y el ancho del pétalo, pero da peso de signo contrario al ancho del sépalo.\nLa segunda componente principal se refiere principalmente al sépalo.\nEl biplot es un gráfico que permite representar las variables originales y las observaciones transformadas en los ejes de componentes principales.\n\nCada flecha corresponde a una variable.\nNos fijamos primeramente en las direcciones de las flechas y su sentido.\nDos flechas que apunten al mismo lugar indica correlación alta.\nDos flechas con sentidos diferentes pero en la misma dirección indican una correlación negativa.\nCuando dos variables no están correladas en absoluto, se observan dos flechas apuntando en direcciones totalmente perpendiculares.\nEn cuanto a la diferencia en la longitud de las flechas, una menos larga informa que su variable está peor representada que una de largo mayor. Es una forma de medir la calidad de representación.\n\n\nfviz_pca_biplot(iris.acp, repel = TRUE,\n                col.var = \"#2E9FDF\", # color para las variables\n                col.ind = \"#696969\"  # color para las observaciones\n                )\n\n\n\n\n\n\n\n\nEn el gráfico anterior podemos observar lo siguiente:\n\nTodas las variables originales tienen influencia en las componentes principales (lo cual se evidencia en el tamaño de las flechas).\nLa flecha más larga corresponde al ancho del sépalo, ya que tiene una fuerte influencia (loading) sobre la segunda componente.\nLa dirección de esta última flecha indica que el “loading” del ancho del sépalo para la primera componente es negativo.\nLos “loadings” de las variables longitud del pétalo y ancho del pétalo con respecto a la segunda componente son muy bajos (las flechas son prácticamente horizontales).\nLa variable longitud del sépalo tiene loadings relativamente altos en las dos componentes principales.\n\nAcceso a los resultados del ACP\n\n# Resultados por Variables\nres.var=get_pca_var(iris.acp)\nres.var$contrib        # Contribuciones a las CP \n\n                 Dim.1       Dim.2     Dim.3     Dim.4\nSepal.Length 27.150969 14.24440565 51.777574  6.827052\nSepal.Width   7.254804 85.24748749  5.972245  1.525463\nPetal.Length 33.687936  0.05998389  2.019990 64.232089\nPetal.Width  31.906291  0.44812296 40.230191 27.415396\n\nres.var$cos2           # Calidad de la representación\n\n                 Dim.1       Dim.2       Dim.3        Dim.4\nSepal.Length 0.7924004 0.130198208 0.075987149 0.0014142127\nSepal.Width  0.2117313 0.779188012 0.008764681 0.0003159971\nPetal.Length 0.9831817 0.000548271 0.002964475 0.0133055723\nPetal.Width  0.9311844 0.004095980 0.059040571 0.0056790544\n\n\n\n# Resultados por observaciones\nres.obs=get_pca_ind(iris.acp)\nhead(res.obs$coord,3)  #Coordenadas\n\n      Dim.1      Dim.2      Dim.3      Dim.4\n1 -2.257141 -0.4784238  0.1272796 0.02408751\n2 -2.074013  0.6718827  0.2338255 0.10266284\n3 -2.356335  0.3407664 -0.0440539 0.02828231\n\nhead(res.obs$contrib,3)  #Contribuciones a las CP\n\n     Dim.1      Dim.2       Dim.3      Dim.4\n1 1.163769 0.16694510 0.073591567 0.01867287\n2 0.982590 0.32925696 0.248367113 0.33919842\n3 1.268304 0.08469576 0.008816151 0.02574286\n\nhead(res.obs$cos2,3)  # Calidad de la representación\n\n      Dim.1      Dim.2        Dim.3        Dim.4\n1 0.9539975 0.04286032 0.0030335249 0.0001086460\n2 0.8927725 0.09369248 0.0113475382 0.0021874817\n3 0.9790410 0.02047578 0.0003422122 0.0001410446\n\n\n¿Qué tan bien lo hace el ACP?\n\nlibrary(\"ggfortify\")\nautoplot(iris.acp, data = iris, colour = 'Species',\n         loadings = TRUE, loadings.colour = 'blue',\n         loadings.label = TRUE, loadings.label.size = 3)\n\n\n\n\n\n\n\n\n\n4.4.1 Ejercicio:\nConsidera los datos europa.dat que están disponibles en Aula Digital. Los datos corresponden a los porcentajes de población empleados en diferentes actividades económicas en Europa para el año 1979. Las variables consideradas son: Agricultura, Minas, Fábricas, Suministro Eléctrico, Construcción, Industrias de Servicio, Finanzas, Servicios Sociales y Personales y, Transporte y Comunicaciones. Utiliza el método de componentes principales para reducir el número de variables, y tratar de determinar grupos de países con comportamientos semejantes en la distribución de su fuerza de trabajo. En este caso, usa la matriz de covarianza para el cálculo de las componentes principales, ya que todos los datos están medidos en la misma escala (porcentaje de la población) y por las caractersticas de los datos, no parece una buena idea considerarlos todos de igual manera.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Técnicas de reducción de Dimensionalidad</span>"
    ]
  },
  {
    "objectID": "t4_trd.html#escalamiento-multidimensional-mds",
    "href": "t4_trd.html#escalamiento-multidimensional-mds",
    "title": "4  Técnicas de reducción de Dimensionalidad",
    "section": "4.5 Escalamiento multidimensional (MDS)",
    "text": "4.5 Escalamiento multidimensional (MDS)\nLas técnicas de escalado multidimensional son una generalización de la idea de componentes principales cuando en lugar de disponer de una matriz de observaciones por variables, como en componentes principales, se dispone de una matriz, \\(\\mathbf{D}\\), cuadrada \\(n \\times n\\) de distancias o disimilaridades entre los \\(n\\) elementos de un conjunto.\nEstas distancias pueden haberse obtenido a partir de ciertas variables, o pueden ser el resultado de una estimación directa, por ejemplo preguntando a un grupo de evaluadores por sus opiniones sobre las similaridades entre los elementos considerados.\nEl objetivo del \\(MDS\\) es representar las distancias observadas mediante unas variables con dimensión menor a \\(n\\), tales que las distancias euclideanas entre las coordenadas de los elementos respecto a estas variables sean iguales (o lo más próximas posibles) a las distancias o disimilaridades de la matriz partida. De esta manera, la representación gráfica en menor dimensión será una reproducción fiel de la estructura observada.\nEl escalado multidimensional comparte con componentes principales el objetivo de describir e interpretar los datos. Si existen muchos elementos, la matriz de similaridades será muy grande y la representación por unas pocas variables de los elementos nos permitirá entender su estructura: qué elementos tienen propiedades similares, si aparecen grupos entre los elementos, si hay elementos atípicos, etc.\nEl escalado multidimensional representa un enfoque complementario a componentes principales en el sentido siguiente.\n\nComponentes principales considera la matriz \\(p \\times p\\) de correlaciones (o covarianzas) entre variables, e investiga su estructura.\nEl escalado multidimensional considera la matriz \\(n \\times n\\) de correlaciones (o covarianzas) entre individuos, e investiga su estructura. Los métodos existentes se dividen en métricos, cuando la matriz inicial es propiamente de distancias, y no métricos, cuando la matriz es de similaridades. Los métodos métricos, utilizan las diferencias entre similitudes mientras que los no métricos parten de que si \\(A\\) es más similar a \\(B\\) que a \\(C\\), entonces \\(A\\) esta más cerca de \\(B\\) que de \\(C\\), pero las diferencias entre las similitudes \\(AB\\) y \\(AC\\) no tienen interpretación.\n\n\n4.5.1 Reconstrucción de las variables a partir de las distancias entre puntos\nDado un conjunto \\(p\\) de variables en \\(n\\) individuos representados en la matriz \\(\\mathbf{X}_{n \\times p}\\), podemos construir dos tipos de matrices cuadradas y semidefinidas positivas: la matriz de covarianzas muestral \\(\\mathbf{S}\\) (definida por \\(\\frac{1}{n}\\mathbf{X}^t \\mathbf{X}\\)), si las variables tienen media cero, y la matriz de productos cruzados \\(\\mathbf{Q} = \\mathbf{XX}^t\\).\nEl \\(MDS\\) puede verse como un análisis de la matriz \\(\\mathbf{Q}\\) y esta puede interpretarse como una matriz de similitudes entre las observaciones ya que sus términos, \\(q_{ij}\\), contienen el producto escalar de las observaciones de dos elementos dados: \\[q_{ij} = \\sum_{k=1}^p x_{ik}x_{jk} = \\mathbf{x}_i^t \\mathbf{x}_j \\;\\; \\;\\;  \\; (1)\\] En efecto, como \\(q_{ij}=|\\mathbf{x}_i||\\mathbf{x}_j|\\cos \\theta_{ij}\\), si los dos elementos tienen coordenadas similares, \\(\\cos \\theta_{ij}\\approx 1\\) y \\(q_{ij}\\) será grande. Por el contrario, si los dos elementos tienen valores distintos \\(\\cos \\theta_{ij}\\approx 0\\) y \\(q_{ij}\\) será pequeño.\nLa distancia euclideana al cuadrado entre dos elementos se define por:\n\\[d^2_{ij}=\\sum_{k=1}^p (x_{ik}-x_{jk})^2=\\sum_{k}x_{ik}^2+\\sum_{k}x_{jk}^2-2\\sum_{k}x_{ik}x_{jk}\\] y la podemos escribir en función de los términos de la matriz \\(\\mathbf{Q}\\),\n\\[d^2_{ij}=q_{ii}+q_{jj}-2q_{ij} \\;\\; \\;\\;  \\; (2)\\]\nPor tanto, dada la matriz \\(\\mathbf{X}\\) podemos construir la matriz \\(\\mathbf{Q}\\) y a partir de esta matriz es fácil obtener la matriz de distancias al cuadrado con ayuda de las expresiones (1) y (2).\nEl problema que se aborda en MDS es el inverso: dada una matriz de distancias al cuadrado, \\(\\mathbf{D}\\), con elementos \\(d^2_{ij}\\) se trata de reconstruir la matrix \\(\\mathbf{X}\\).\nLo primero que se plantea es obtener la matriz \\(\\mathbf{Q}\\) dada la matriz \\(\\mathbf{D}\\). Para ello, observemos, que sin pérdida de generalidad, siempre podemos suponer que las variables \\(X\\) tienen media cero. En efecto, las distancias entre los puntos, \\(d^2_{ij}\\) no varían si expresamos las variables en desviaciones a la media, ya que:\n\\[d^2_{ij}=\\sum_{k=1}^p (x_{ik}-x_{jk})^2=\\sum_{k}\\left[ (x_{ik}-\\bar{x}_k)-(x_{jk}-\\bar{x}_k) \\right]^2 \\;\\; \\;\\;  \\; (3)\\] y, por tanto, podemos suponer siempre que las variables que buscamos tienen media cero.\nPor ello, como resulta que \\(\\mathbf{X}^t \\mathbf{1} = 0\\) se debe verificar que \\(\\mathbf{Q}\\mathbf{1} = 0\\), es decir, la suma de todos los elementos de una fila de la matriz \\(\\mathbf{Q}\\) (y de una columna ya que la matriz es simétrica) debe de ser cero.\nLuego, sumamos en (2) por filas: \\[\\sum_{i=1}^n d^2_{ij}=\\sum_{i=1}^n q_{ii}+nq_{jj}=t+nq_{jj} \\;\\; \\;\\;  \\; (4)\\] donde \\(t=\\sum_{i=1}^n q_{ii}=traza(\\mathbf{Q})\\), y sabiendo que \\(\\sum_{i=1}^n q_{ij}=0\\).\nSumando (2) por columnas se tiene: \\[\\sum_{j=1}^n d^2_{ij}=n q_{ii}+t \\;\\; \\;\\;  \\; (5)\\] y sumando ahora (4) por filas de nuevo se tiene:\n\\[\\sum_{i=1}^n \\sum_{j=1}^n d^2_{ij}=2nt\\]\nSustituyendo (4) y (5) en (2), tenemos que\n\\[d^2_{ij}=\\frac{1}{n}\\sum_{j=1}^n d^2_{ij} - \\frac{t}{n} + \\frac{1}{n}\\sum_{i=1}^n d^2_{ij} - \\frac{t}{n} -2 q_{ij},  \\] y llamando \\(d^2_{i.}\\) y \\(d^2_{.j}\\) a las medias por filas y por columnas y utilizando (5), tenemos que\n\\[ d^2_{ij}= d^2_{i.}+ d^2_{.j}-d^2_{..}-2 q_{ij} \\;\\; \\;\\;  \\; (6) \\] donde \\(d^2_{..}=\\frac{1}{n^2}\\sum_{i=1}^n \\sum_{j=1}^n d^2_{ij}\\)\nFinalmente, despejando de (6) resulta que \\[q_{ij}=-\\frac{1}{2}\\left( d^2_{ij}-d^2_{i.}-d^2_{.j}+d^2_{..}\\right)\\] expresión que indica cómo construir la matriz \\(\\mathbf{Q}\\) a partir de la matriz de distancias \\(\\mathbf{D}\\).\n\n\n4.5.2 Coordenadas principales a partir de distancias\nSea \\(\\mathbf{D}\\) una matriz de distancias euclídeas al cuadrado entre \\(n\\) objetos. Las coordenadas principales son las coordenadas de dichos objetos en \\(r &lt; n\\) dimensiones, \\(Z_1 \\ldots Z_r\\) (columnas de una matriz \\(Z_{n \\times r}\\)), de manera que la matriz de distancias euclídeas al cuadrado entre las coordenadas de dichos objetos coincida con \\(\\mathbf{D}\\).\nPartimos de una matriz de distancias \\(\\mathbf{D}\\) obtenida de una matriz de coordenadas \\(Y\\) . Vamos a ver que a partir de \\(D\\) se puede obtener la matriz de productos escalares \\(Q = Y Y^t\\), y a partir de \\(Q\\) se obtiene otra matriz de coordenadas \\(Z\\), cuya matriz de distancias coincide con \\(D\\).\nVamos a suponer que las columnas de \\(Z\\) están centradas.\nSi realizamos la descomposición espectral de \\(\\mathbf{Q}\\), \\[\\mathbf{Q}=\\mathbf{V} \\mathbf{\\Lambda}\\mathbf{V}^t=\\left(\\mathbf{V} \\mathbf{\\Lambda}^{1/2} \\right) \\left( \\mathbf{\\Lambda}^{1/2} \\mathbf{V}^t \\right)\\] donde \\(\\mathbf{\\Lambda}\\) es una matriz diagonal con los valores propios no nulos de \\(\\mathbf{Q}\\) y \\(\\mathbf{V}\\) es una matriz \\(n \\times r\\) con los \\(r\\) vectores propios asociados a los valores propios de \\(Q\\) en sus columnas. La matriz de coordenadas principales es: \\[Z= \\mathbf{V} \\mathbf{\\Lambda}^{1/2}\\]\nEl procedimiento para obtener las coordenadas principales es el siguiente:\n1.- Formar la matriz de distancias al cuadrado, \\(\\mathbf{D}\\), cuyos elementos son los cuadrados de las distancias.\n2.- Construir la matriz \\(\\mathbf{Q}\\) de productos cruzados.\n3.- Obtener los valores y vectores propios de \\(\\mathbf{Q}\\). Tomar los \\(r\\) mayores si podemos suponer que los restantes \\(n-r\\) son próximos a cero.\n4.- Obtener las coordenadas de los puntos en las variables mediante \\(\\sqrt{{\\lambda_i}} \\mathbf{v}_i\\), donde \\(\\mathbf{\\lambda}_i\\) es el valor propio y \\(\\mathbf{v}_i\\) el vector propio.\nEl método puede aplicarse también cuando la matriz de partida \\(\\mathbf{Q}\\) es una matriz de similaridades cualquiera. Entonces \\(q_{ii}=1\\), \\(q_{ij}=q_{ji}\\) y \\(0 \\leq q_{ij} \\leq 1\\). De acuerdo a (2), la matriz de distancias asociadas será: \\[d^2_{ij}= q_{ii}+ q_{jj}-2 q_{ij}=2(1-q_{ij})\\] y puede comprobarse que \\(\\sqrt{2(1-q_{ij})}\\) es una distancia y verifica la desigualdad triangular al corresponder a la distancia euclideana para cierta configuración de puntos.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Técnicas de reducción de Dimensionalidad</span>"
    ]
  },
  {
    "objectID": "t4_trd.html#selección-del-número-de-dimensiones",
    "href": "t4_trd.html#selección-del-número-de-dimensiones",
    "title": "4  Técnicas de reducción de Dimensionalidad",
    "section": "4.6 Selección del número de dimensiones",
    "text": "4.6 Selección del número de dimensiones\nSe ha propuesto como medida de la precisión en la aproximación mediante los \\(r\\) valores propios positivos, los coeficientes:\n\\[m_{1,r}=\\frac{\\sum_{i=1}^r |\\lambda_i|}{\\sum_{i=1}^p |\\lambda_i|} \\cdot 100\\] ó\n\\[m_{2,r}=\\frac{\\sum_{i=1}^r \\lambda_i^2}{\\sum_{i=1}^p \\lambda_i^2} \\cdot 100\\]\n\n4.6.1 Ejemplo a mano\nSobre el siguiente conjunto de animales \\(\\varepsilon=\\{león, jirafa, vaca, oveja, gato, hombre \\}\\) se han medido las siguientes variables binarias:\n\n\\(X_1\\): ¿tiene cola?\n\\(X_2\\): ¿es salvaje?\n\\(X_3\\): ¿tiene el cuello largo?\n\\(X_4\\): ¿es animal de granja?\n\\(X_5\\): ¿es carnívoro?\n\\(X_6\\): ¿camina sobre cuatro patas?\n\nLa matriz de datos es :\n\\[\\mathbf{X}=\\begin{pmatrix}\n1 & 1 & 0 & 0 & 1 & 1\\\\\n1 & 1 & 1 & 0 & 0 & 1\\\\\n1 & 0 & 0 & 1 & 0 & 1\\\\\n1 & 0 & 0 & 1 & 0 & 1\\\\\n1 & 0 & 0 & 0 & 1 & 1\\\\\n0 & 0 & 0 & 0 & 1 & 0\\\\\n\\end{pmatrix} \\begin{matrix}\nleón\\\\\njirafa \\\\\nvaca \\\\\noveja \\\\\ngato \\\\\nhombre\n\\end{matrix}\\]\nA partir de esta matriz, construimos las coordenadas principales y realizamos una representación en 2 dimensiones. Calcularemos una medida de precisión de la representación que hemos realizado.\nEn este caso tenemos \\(\\mathbf{X}\\), si no fuese el caso habría que usar una medida de similiaridad como por ejemplo la de Sokal y Mikener (Ejercicio)\n\nX=matrix(c(1, 1, 0, 0, 1, 1, 1, 1, 1, 0,\n           0, 1,1, 0, 0, 1, 0, 1,1, 0, 0,\n           1, 0, 1,1, 0, 0, 0, 1, 1,0, 0,\n           0, 0, 1, 0), nrow=6, byrow=TRUE)\nX\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]    1    1    0    0    1    1\n[2,]    1    1    1    0    0    1\n[3,]    1    0    0    1    0    1\n[4,]    1    0    0    1    0    1\n[5,]    1    0    0    0    1    1\n[6,]    0    0    0    0    1    0\n\nQ=X %*% t(X)\nvalores=eigen(Q)$values\nvectores=eigen(Q)$vectors\ny1=sqrt(valores[1])*vectores[,1]\ny2=sqrt(valores[2])*vectores[,2]\n\nCalculamos una medida de precisión (variabilidad explicada) con 2 componentes y miramos la representación\n\n(m_12=(sum(abs(valores[1:2]))/sum(abs(valores)))*100)\n\n[1] 86.20502\n\n(m_22=(sum(valores[1:2]^2)/sum(valores^2))*100)\n\n[1] 97.63187",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Técnicas de reducción de Dimensionalidad</span>"
    ]
  },
  {
    "objectID": "t4_trd.html#mds-no-métrico",
    "href": "t4_trd.html#mds-no-métrico",
    "title": "4  Técnicas de reducción de Dimensionalidad",
    "section": "4.7 MDS no métrico",
    "text": "4.7 MDS no métrico\nPartiendo de la matriz de disimilaridades \\(\\mathbf{\\Lambda}=(\\delta_{ij})\\), el escalado no métrico consiste en encontrar unas coordenadas, cuyas distancias euclídeas al cuadrado mantengan el órden de las disimilaridades. Es decir, el escalado no métrico solo tiene en cuenta la información referente al órden entre las disimilaridades, y no su magnitud. El procedimiento es el siguiente:\n\nSe calculan unas coordenadas iniciales \\(Z^{(0)}\\), por ejemplo aplicando MDS métrico a \\(\\mathbf{\\Lambda}\\). Esto es, calcular \\(\\mathbf{Q}\\), realizar la descomposición \\(\\mathbf{Q}=\\mathbf{V} \\mathbf{\\Lambda}\\mathbf{V}^t\\) y tomar \\(Z^{(0)} =\\mathbf{V}_r \\mathbf{\\Lambda}_r^{1/2}\\), donde \\(\\mathbf{\\Lambda}_r\\) contiene los \\(r\\) valores propios mayores y sus \\(\\mathbf{V}_r\\) vectores propios asociados en columnas. Así, obtenemos \\(D^{(0)}=(d^{(0)}_{ij})\\) a partir de las coordenadas \\(Z^{(0)}\\).\nSe calculan disparidades \\(\\hat{d}_{ij}\\) que son una transformación de las distancias \\(d_{ij}\\) que mantienen la misma ordenación que las disimilaridades \\(\\delta_{ij}\\), es decir \\[\\hat{d}_{ij}=f(d_{ij})\\] donde \\(f\\) es una función monótona que verifica: Si \\(\\delta_{ij} \\leq \\delta_{kl}\\), entonces \\(\\hat{d}_{ij} \\leq \\hat{d}_{kl}\\).\n\nEjemplo: Consideremos la matriz de disimilaridades \\[\\mathbf{\\Lambda}=(\\delta_{ij})=\\begin{pmatrix}\n0 & 2.1 & 3 & 2.4\\\\\n  & 0  &  1.7 & 3.9 \\\\\n  &    &  0  & 3.2 \\\\\n  &    &     & 0\n\\end{pmatrix} \\]\nSupongamos que hemos obtenido una matriz de coordenadas inicial \\(Z^{(0)},\\) cuya matriz de distancias es \\[D^{(0)}=(d^{(0)}_{ij})=\\begin{pmatrix}\n0 & 1.6 & 4.5 & 5.7\\\\\n  & 0  &  3.3 & 4.3 \\\\\n  &    &  0  & 1.3 \\\\\n  &    &     & 0\n\\end{pmatrix}\\]\nObtener las disparidades:\nEscribimos las disimilaridades en órden creciente \\[\\delta_{23}=1.7, \\delta_{12}=2.1, \\delta_{14}=2.4, \\delta_{13}=3, \\delta_{34}=3.2, \\delta_{24}=3.9\\]\nAhora escribimos las distancias correspondientes \\[d_{23}= \\ldots, d_{12}= \\ldots, d_{14}= \\ldots, d_{13}= \\ldots, d_{34}= \\ldots, d_{24}= \\ldots\\]\nSi mantuviesen el mismo órden que las disimilaridades, estarían ordenadas de menor a mayor, y en ese caso, las disparidades serían iguales a las distancias. Entonces \\(Z^{(0)}\\) sería una solución válida.\nUna transformación monótona de estas distancias que preserva el órden de las disimilaridades se calcula de la siguiente forma: cuando existe una secuencia de distancias que están ordenadas al contrario de lo deseado, se reemplazan todas estas distancias por la media de las distancias de dicha secuencia. Así, las disparidades son:\n\\[\\begin{align*}\n\\hat{d}_{23} &= \\hat{d}_{12}=\\frac{1}{2} (d_{23}+d_{12}), \\\\\n\\hat{d}_{14} &= \\hat{d}_{13}=\\hat{d}_{34}=\\frac{1}{3} (d_{14}+d_{13}+d_{34}), \\\\\n\\hat{d}_{24} &= d_{24}\n\\end{align*}\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Técnicas de reducción de Dimensionalidad</span>"
    ]
  },
  {
    "objectID": "t4_trd.html#medidas-de-bondad-de-ajuste-de-la-solución-obtenida",
    "href": "t4_trd.html#medidas-de-bondad-de-ajuste-de-la-solución-obtenida",
    "title": "4  Técnicas de reducción de Dimensionalidad",
    "section": "4.8 Medidas de bondad de ajuste de la solución obtenida",
    "text": "4.8 Medidas de bondad de ajuste de la solución obtenida\nPara medir la bondad de ajuste de la solución obtenida:\n\nSTRESS: \\(S= \\left[\\frac{\\sum_{i&lt;j} (d_{ij}-\\hat{d}_{ij})^2}{\\sum_{i&lt;j}d^2_{ij}} \\right]^{1/2}\\)\n\n\\(S \\in (0, 0.01] \\implies\\) Solución muy buena;\n\\(S \\in (0.01, 0.05] \\implies\\) Solución buena;\n\\(S \\in (0.05, 0.1] \\implies\\) Solución aceptable.\n\nS-STRESS: \\(S= \\left[\\frac{\\sum_{i&lt;j} (d_{ij}-\\hat{d}_{ij})^2}{\\sum_{i&lt;j}d^4_{ij}} \\right]^{1/2}\\)\n\nEsta medida está entre 0 y 1, siendo valores cercanos a cero indicadores de un buen ajuste, y valores cercanos a 1 indicadores de un mal ajuste.\n\nRSQ: Es el coeficiente de correlación al cuadrado entre las distancias y las disparidades. El ajuste es aceptable para RSQ \\(\\geq 0.6\\).\n\nSi para las coordenadas actuales, estas medidas no son satisfactorias, entonces pasamos a la búsqueda de una nueva solución. Esta solución se busca minimizando una de las medidas de ajuste respecto a las coordenadas, generalmente se utiliza el STRESS o el S-STRESS.\nMinimización del STRESS\nSea \\(\\mathbf{z}=(\\mathbf{z}^t_1, \\ldots, \\mathbf{z}^t_n)\\) el vector formado por las \\(n\\) filas de \\(\\mathbf(Z)\\) (nuestras incógnitas). El problema es encontrar \\(\\mathbf{z}\\) que minimice \\[S= \\left[\\frac{\\sum_{i&lt;j} (d_{ij}(\\mathbf{z})-\\hat{d}_{ij})^2}{\\sum_{i&lt;j}d^2_{ij}(\\mathbf{z})} \\right]^{1/2}\\] donde \\[d^2_{ij}(\\mathbf{z})=d^2_e(\\mathbf{z}_i,\\mathbf{z}_j)=\\sum_{k=1}^p (z_{ik}-z_{jk})^2.\\] Este problema es no lineal, con lo cual es necesario recurrir a métodos de resolución numéricos.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Técnicas de reducción de Dimensionalidad</span>"
    ]
  },
  {
    "objectID": "t4_trd.html#escalamiento-multidimensional-en-r",
    "href": "t4_trd.html#escalamiento-multidimensional-en-r",
    "title": "4  Técnicas de reducción de Dimensionalidad",
    "section": "4.9 Escalamiento multidimensional en R",
    "text": "4.9 Escalamiento multidimensional en R\nLa función en R para llevar a cabo un MDS es cmdscale(d,k,eig=FALSE), donde\n\nd es un objeto clase distancia como la producida por dist o una matriz simétrica que contiene disimilaridades.\nk es la dimensión del espacio en cual queremos representar los datos.\neig es una variable lógica que indica si los valores propios deben ser presentados.\n\nConsideremos como ejemplo un conjunto de datos de la librería MVA que contiene las distancias por carretera entre 21 ciudades europeas en km.\nEn base a estas distancias, deseamos “reconstruir” el mapa.\n\nlibrary(MVA)\n\nLoading required package: HSAUR2\n\n\nLoading required package: tools\n\ndata(\"eurodist\")\nstr(eurodist)\n\n 'dist' num [1:210] 3313 2963 3175 3339 2762 ...\n - attr(*, \"Size\")= num 21\n - attr(*, \"Labels\")= chr [1:21] \"Athens\" \"Barcelona\" \"Brussels\" \"Calais\" ...\n\neur.mds&lt;-cmdscale(eurodist,k=2)\nplot(eur.mds, type=\"n\")\ntext(eur.mds, labels = rownames(eur.mds))\n\n\n\n\n\n\n\n\nHemos obtenido una forma similar a lo que esperaríamos de un mapa de Europa, pero con norte y sur intercambiados.\nUn ejemplo de cómo trabajar con tidyverse, ver el siguiente link",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Técnicas de reducción de Dimensionalidad</span>"
    ]
  },
  {
    "objectID": "t4_trd.html#práctica",
    "href": "t4_trd.html#práctica",
    "title": "4  Técnicas de reducción de Dimensionalidad",
    "section": "4.10 Práctica",
    "text": "4.10 Práctica\nEl metabolismo se caracteriza por reacciones químicas vinculadas entre sí, creando una compleja estructura de red. Una representación simplificada del metabolismo, que denominamos red metabólica abstracta, es un grafo en el que las vías metabólicas son nodos y existe una arista entre dos nodos si sus correspondientes vías comparten uno o más compuestos.\nPara explorar los potenciales y límites de una representación tan básica, hemos empleado tres tipos de kernels (distancias entre grafos):\n\nVH (Vertex histogram): solo tiene en cuenta si las etiquetas de los nodos de los grafos que se comparan son iguales o no.\nSP (Shortest-Path): compara los grafos en función de sus caminos más cortos. Intuitivamente, esto significa medir lo fácil o difícil que es conectar, a través de compuestos compartidos, parejas de caminos en los dos grafos.\nPM (Pyramid Match): mide la similitud de las características topológicas (por ejemplo, la conectividad) de los nodos con la misma etiqueta en los dos grafos comparados.\n\nLa práctica consiste en representar gráficamente (con solo 2 coordenadas principales) las matrices de similitud generadas por cada kernel coloreando los puntos de acuerdo al grupo de animales de acuerdo a su phylum.\nLos ficheros necesarios para realizar la práctica los podéis descargar de la página del curso en Aula Digital.\n\n\n\n\nBakker, Jonathan D. 2024. «Applied Multivariate Statistics in R». 2024. https://uw.pressbooks.pub/appliedmultivariatestatistics.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Técnicas de reducción de Dimensionalidad</span>"
    ]
  },
  {
    "objectID": "t5_clustering.html",
    "href": "t5_clustering.html",
    "title": "5  Agrupamiento (Clustering)",
    "section": "",
    "text": "5.1 Introducción\nEl término clustering se refiere a una amplia gama de técnicas no supervisadas que buscan identificar patrones o grupos (clusters) dentro de un conjunto de las observaciones. Estas particiones se establecen de manera que las observaciones dentro de un mismo grupo sean similares entre sí y diferentes de las observaciones en otros grupos.\nDado el amplio espectro de aplicaciones del clustering en diversos campos, como la genómica y el marketing, se han desarrollado numerosas variantes y adaptaciones de sus métodos y algoritmos. Estas variantes pueden agruparse en las siguientes categorías principales:\nAunque existen métodos que combinan ambos tipos de clustering, en la asignatura se incluye únicamente los listados anteriormente porque se hace énfasis en la comprensión del algoritmo que se está utilizando y en la interpretación de los resultados en el contexto del problema.\nDado un conjunto de objetos, queremos clasificarlos en grupos (clusters) basándonos en sus semejanzas y diferencias.\nAlgunas aplicaciones en biología:\nPrincipios básicos\n¿Cómo formalizamos estos principios intuitivos?\nPrincipalmente hay dos tipos diferentes de clustering :",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Agrupamiento (Clustering)</span>"
    ]
  },
  {
    "objectID": "t5_clustering.html#introducción",
    "href": "t5_clustering.html#introducción",
    "title": "5  Agrupamiento (Clustering)",
    "section": "",
    "text": "Clustering de Particionamiento: Algoritmos que requieren la especificación previa del número de clusters: K-means, K-medoids, y CLARA.\nClustering Jerárquico: Algoritmos que no requieren especificar el número de clusters: aglomerativos y divisivos.\n\n\n\n\n\nClasificación jerárquica de organismos (relacionada con la filogenia).\nAgrupamiento de genes con patrones de expresión similares.\nAgrupamiento de genes por similitud secuencial.\nAgrupamiento de proteínas por similitud estructural.\n\n\n\nHomogeneidad: Objetos dentro de un mismo clúster han de ser semejantes (próximos).\nSeparación: Objetos dentro de clústers diferentes han de ser lejanos.\n\n\n\n\n\nDe partición: Dividimos los objetos en un número prefijado de clusters; posiblemente probamos diversos número de grupos y nos quedamos con el mejor agrupamiento.\n\n\n\n\n\n\n\nJerárquico: Agrupamos sucesivamente aglomerativo o dividimos divisivo los objetos o grupos de objetos. Producimos un árbol de clasificación.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Agrupamiento (Clustering)</span>"
    ]
  },
  {
    "objectID": "t5_clustering.html#k-medias",
    "href": "t5_clustering.html#k-medias",
    "title": "5  Agrupamiento (Clustering)",
    "section": "5.2 \\(k\\)-medias",
    "text": "5.2 \\(k\\)-medias\n\n5.2.1 Introducción\nEl algoritmo de las \\(k\\)-medias (\\(k\\)-means) busca una partición del conjunto de objetos, representados como elementos de un espacio \\(\\mathbb{R}^n\\) en un número fijo \\(k\\) de clusters.\nEstos clusters se identifican por medio de sus puntos medios (means).\nRecordad que dado \\(\\mathbf{x}=(x_1,\\ldots,x_n)\\in \\mathbb{R}^n\\), \\[\n\\|\\mathbf{x}\\|^2=\\sum_{i=1}^n x_i^2\\in \\mathbb{R}\n\\] y que dados \\(\\mathbf{x},\\mathbf{y} \\in \\mathbb{R}^n\\), \\(\\|\\mathbf{x}-\\mathbf{y}\\|\\) es la distancia euclidiana entre \\(\\mathbf{x}\\) y \\(\\mathbf{y}\\).\nFijamos el número de clusters \\(k\\)\nDados puntos \\(\\mathbf{x}_1,\\ldots,\\mathbf{x}_p\\in \\mathbb{R}^n\\), el objetivo es encontrar \\(k\\) puntos \\(\\mathbf{c}_1,\\ldots,\\mathbf{c}_k\\in \\mathbb{R}^n\\) que minimicen \\[\nSS_C(\\mathbf{x}_1,\\ldots,\\mathbf{x}_p; k)=\\sum_{i=1}^p\\min_{j=1,\\ldots,k} \\|\\mathbf{x}_i-\\mathbf{c}_j\\|^2\n\\] Entonces cada \\(\\mathbf{c}_j\\) definirá el clúster formado por los \\(\\mathbf{x}_i\\) que están más cerca más cerca de él que de cualquier otro \\(\\mathbf{c}_l\\): \\[\nC_j=\\{\\mathbf{x}_i\\mid \\|\\mathbf{x}_i-\\mathbf{c}_j\\|&lt;\\|\\mathbf{x}_i-\\mathbf{c}_l\\|\\mbox{ para todo }l\\neq j\\}\n\\] y \\[\nSS_C(\\mathbf{x}_1,\\ldots,\\mathbf{x}_p; k)=\\sum_{j=1}^k\\sum_{\\mathbf{x}_i\\in C_j}  \\|\\mathbf{x}_i-\\mathbf{c}_j\\|^2\n\\]\n\n\n5.2.2 Algoritmo de Lloyd\n\nEscogemos \\(\\mathbf{c}_1,\\ldots,\\mathbf{c}_k\\) (como queramos).\nAsignamos cada punto \\(\\mathbf{x}_i\\) al cluster \\(C_j\\) definido por el centro \\(\\mathbf{c}_j\\) más cercano.\nSubstituimos cada centro \\(\\mathbf{c}_j\\) por el punto medio de su cluster \\(C_j\\): \\[\\mathbf{c}_j= \\Big(\\sum_{\\mathbf{x}_i\\in C_j} \\mathbf{x}_i\\Big)/|C_j|\\]\nRepetimos los pasos anteriores hasta que los clústers se estabilicen, o un número prefijado de iteraciones.\n\nEl resultado depende de los \\(\\mathbf{c}_1,\\ldots,\\mathbf{c}_k\\) iniciales.\nEste algoritmo no tiene porque dar un clustering óptimo. Conviene repetirlo varias veces con diferentes valores iniciales.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLimitaciones de \\(k\\)-means:\n\nNo hay un método eficiente y universal para escoger los centros de partida\nNo se puede garantizar un óptimo global\nNo se puede determinar de manera efectiva el número \\(k\\) a priori\nNo es invariante por cambios de escala (conviene estandardizar datos)\nSensible a outliers\nSolo aplicable dentro \\(\\mathbb{R}^n\\) con distancia euclidiana\nEncuentra clusters esféricos\n\n\n\n5.2.3 Método del codo y test F\nEl \\(SS_C\\) óptimo disminuye con \\(k\\) siguiendo una función más o menos cóncava. Si podemos detectar un \\(k\\) a partir del cual \\(SS_C\\) disminuya mucho más lentamente que su anterior, este será el \\(k\\) recomendable.\n\n\n\n\n\n\n\n\n\n\\(k=3\\) es el más recomendable\nSe calcula\n\\[F_k=\\frac{SS_C(k)-SS_C(k+1)}{\\frac{SS_C(k+1)}{p-k-1}}\\] Se escoge como p-valor \\[P(F_{n,n(p-k-1)}&gt;F_k)\\] con \\(F_{n,n(p-k-1)}\\) una F de Fisher con \\(n\\) y \\(n(p-k-1)\\) grados de libertad, y escogemos el \\(k\\) con p-valor más pequeño Cabe decir que es un método muy utilizado, pero no demasiado justificable.\nEn el ejemplo del gráfico anterior\n\n\n\n\\(k\\)\n2\n3\n4\n5\n6\n7\n8\n\n\n\n\n\\(SS_C(k)\\)\n8.264\n5.344\n3.254\n2.428\n1.925\n1.677\n1.27\n\n\n\\(F_k\\)\n8.2\n9\n4.42\n3.14\n1.63\n3.2\n\n\n\np-valor\n0.0014\n0.001\n0.02\n0.06\n0.229\n0.06\n\n\n\n\n\\(k=3\\) vuelve a ser el más recomendable\nSi el conjunto de puntos es muy grande, todos los \\(p\\)-valores son cercanos a 0 y este método no es útil.\n\n\n5.2.4 \\(k\\)-medias con R\nLa instrucción básica para ejecutar un \\(k\\)-means amb R es\n\nkmeans(x,centres,iter.max=...)\n\ncon:\n\nx, una matriz con los puntos \\(\\mathbf{x}_i\\) como filas\ncentres, una matriz con los centros \\(\\mathbf{c}_i\\) de partida como filas, o el número \\(k\\)\niter.max, el número máximo de iteraciones\n\nEsta instrucción no sigue exactamente nuestro algoritmo, si queréis que ejecute el algoritmo explicado tenéis que poner, además, algorithm=\"Lloyd\".\n\ndades=matrix(c(0.8,1.3,0.8,1.8,1.0,0.9,1.1,\n0.1,1.1,1.6,1.4,0.6,1.5,0.1,2,2.1,1.5,2.3,1.8,\n1.8,2.3,0.5,0.3,2.2,1,2.5,2,0.5,2,1.5,2.5,1,\n0.5,0.5,1,2), \nnrow=18,byrow=TRUE) \ncent=matrix(c(0.5,0,0.5,1.5,0.5,3),\nnrow=3,byrow=TRUE)\nkmeans(dades,cent,algorithm=\"Lloyd\")\n\nK-means clustering with 3 clusters of sizes 8, 5, 5\n\nCluster means:\n    [,1]  [,2]\n1 1.5375 0.525\n2 1.3000 1.600\n3 1.1600 2.220\n\nClustering vector:\n [1] 2 2 1 1 2 1 1 3 3 2 1 3 3 1 2 1 1 3\n\nWithin cluster sum of squares by cluster:\n[1] 4.03375 1.46000 1.76000\n (between_SS / total_SS =  57.9 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\nComponentes de la list kmeans:\n\ncluster: asignaciones de elementos a clusters\n\n\nkm=kmeans(dades,cent,algorithm=\"Lloyd\")\nkm$cluster\n\n [1] 2 2 1 1 2 1 1 3 3 2 1 3 3 1 2 1 1 3\n\n\n\ncenters: los centros de los clusters\n\n\nkm$centers\n\n    [,1]  [,2]\n1 1.5375 0.525\n2 1.3000 1.600\n3 1.1600 2.220\n\n\n\ntotss: suma de los cuadrados de las distancias de los puntos a su centre\n\n\nkm$totss\n\n[1] 17.20944\n\n\n\nwithinss: vector de las sumas, para cada cluster, de los cuadrados de las distancias de sus puntos a su centro\n\n\nkm$withinss\n\n[1] 4.03375 1.46000 1.76000\n\n\n\ntot.withinss: suma de withinss, \\(SS_C\\)\n\n\nkm$tot.withinss  \n\n[1] 7.25375\n\n\n\nbetweenss: diferencia totss - tot.withinss\n\n\nkm$betweenss   \n\n[1] 9.955694\n\n\n\nNos interesa betweenss/totss, que mide la fracción de la variabilidad de los datos que explican los clusters. Cuanto más grande mejor, y viene dado por kmeans\n\n\n# (between_SS / total_SS =  57.9 %)\n9.955694/17.20944  #betweenss/totss\n\n[1] 0.5785019\n\n\n\nkm.rand=kmeans(dades,3,algorithm=\"Lloyd\")\nkm.rand\n\nK-means clustering with 3 clusters of sizes 6, 4, 8\n\nCluster means:\n    [,1]      [,2]\n1 1.8000 0.4666667\n2 1.8250 1.9250000\n3 0.8125 1.6000000\n\nClustering vector:\n [1] 3 3 3 1 3 1 1 2 2 2 1 3 3 1 2 1 3 3\n\nWithin cluster sum of squares by cluster:\n[1] 2.093333 0.535000 3.708750\n (between_SS / total_SS =  63.2 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n# (between_SS / total_SS =  67.8 %)\nkm.rand$tot.withinss\n\n[1] 6.337083\n\n\n\nkm2=kmeans(dades,3)  #5a repetición ;-) \nkm2\n\nK-means clustering with 3 clusters of sizes 7, 7, 4\n\nCluster means:\n       [,1]      [,2]\n1 0.9285714 1.9571429\n2 1.4000000 0.4571429\n3 2.0750000 1.6000000\n\nClustering vector:\n [1] 1 1 2 2 1 2 2 3 1 3 2 1 1 2 3 3 2 1\n\nWithin cluster sum of squares by cluster:\n[1] 1.851429 2.717143 0.927500\n (between_SS / total_SS =  68.1 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n# (between_SS / total_SS =  68.9 %)\nkm2$tot.withinss\n\n[1] 5.496071\n\n\n\nkmeans1 &lt;- ggplot(data=dades, aes(x, y)) + geom_point(color=km$cluster) + theme_minimal()\nkmeans2 &lt;- ggplot(data=dades, aes(x, y)) + geom_point(color=km2$cluster) + theme_minimal()\n\n\n\n\n\n\n\n\n\n\nEl nuestro\n\n\n\n\n\n\n\nEl óptimo\n\n\n\n\n\n\n\n5.2.5 Ejemplo\nEl paquete factoextra creado por Alboukadel Kassambara contiene funciones que facilitan en gran medida la visualización y evaluación de los resultados de clustering.\nEl set de datos USArrests contiene información sobre el número de delitos (asaltos, asesinatos y secuestros) junto con el porcentaje de población urbana para cada uno de los 50 estados de USA. Se pretende estudiar si existe una agrupación subyacente de los estados empleando K-means-clustering.\nSi se emplea K-means-clustering con distancia euclídea hay que asegurarse de que las variables empleadas son de tipo continuo, ya que trabaja con la media de cada una de ellas.\n\ndata(\"USArrests\")\nstr(USArrests)\n\n'data.frame':   50 obs. of  4 variables:\n $ Murder  : num  13.2 10 8.1 8.8 9 7.9 3.3 5.9 15.4 17.4 ...\n $ Assault : int  236 263 294 190 276 204 110 238 335 211 ...\n $ UrbanPop: int  58 48 80 50 91 78 77 72 80 60 ...\n $ Rape    : num  21.2 44.5 31 19.5 40.6 38.7 11.1 15.8 31.9 25.8 ...\n\n\nComo la magnitud de los valores difiere notablemente entre variables, se procede a escalarlas antes de aplicar el clustering.\n\ndatos &lt;- scale(USArrests)\n\nUna forma sencilla de estimar el número \\(K\\) óptimo de clusters cuando no se dispone de información adicional en la que basarse es aplicar el algoritmo para un rango de valores de \\(K\\), identificando aquel a partir del cual la reducción en la suma total de varianza intra-cluster deja de ser sustancial (en los siguientes apartados se detallan otras opciones). La función fviz_nbclust() automatiza este proceso.\n\nlibrary(factoextra)\nfviz_nbclust(x = datos, FUNcluster = kmeans, method = \"wss\",\n             diss = dist(datos, method = \"euclidean\")) +\n  geom_vline(xintercept = 4, linetype = 2)\n\n\n\n\n\n\n\n\nEn este caso, a partir de 4 clusters la reducción en la suma total de cuadrados internos parece estabilizarse, indicando que \\(K=4\\) es una buena opción.\n\nset.seed(123)\nkm_clusters &lt;- kmeans(x = datos, centers = 4, nstart = 25)\n\nEl paquete factoextra también permite obtener visualizaciones de las agrupaciones resultantes. Si el número de variables (dimensionalidad) es mayor de 2, automáticamente realiza un PCA y representa las dos primeras componentes principales.\n\nfviz_cluster(object = km_clusters, data = datos, show.clust.cent = TRUE,\n             ellipse.type = \"euclid\", star.plot = TRUE, repel = TRUE) +\n  theme_bw() + theme(legend.position = \"none\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Agrupamiento (Clustering)</span>"
    ]
  },
  {
    "objectID": "t5_clustering.html#k-medoides",
    "href": "t5_clustering.html#k-medoides",
    "title": "5  Agrupamiento (Clustering)",
    "section": "5.3 \\(k\\)-medoides",
    "text": "5.3 \\(k\\)-medoides\nEste capítulo es un extracto del libro Clustering y heatmaps: aprendizaje no supervisado por Joaquín Amat Rodrigo, que está disponible en su GitHub. (véase Amat 2017)\n\n5.3.1 Introducción\nK-medoids es un método de clustering muy similar a K-means en cuanto a que ambos agrupan las observaciones en \\(K\\) clusters, donde \\(K\\) es un valor preestablecido por el analista. La diferencia es que en K-medoids cada cluster está representado por una observación presente en el cluster (medoid), mientras que en K-means cada cluster está representado por su centroide, que se corresponde con el promedio de todas las observaciones del cluster pero con ninguna en particular.\nUna definición más exacta del término medoid es: elemento dentro de un cluster cuya distancia (diferencia) promedio entre él y todos los demás elementos del mismo cluster es lo menor posible. Se corresponde con el elemento más central del cluster y por lo tanto puede considerarse como el más representativo. El hecho de utilizar medoids en lugar de centroides hace de K-medoids un método más robusto que K-means, viéndose menos afectado por outliers o ruido. A modo de idea intuitiva puede considerarse como la analogía entre media y mediana.\nEl algoritmo más empleado para aplicar K-medoids se conoce como PAM (Partitioning Around Medoids) y sigue los siguientes pasos:\n\nSeleccionar \\(K\\) observaciones aleatorias como medoids iniciales. También es posible identificarlas de forma específica.\nCalcular la matriz de distancia entre todas las observaciones si esta no se ha calculado anteriormente.\nAsignar cada observación a su medoid más cercano.\nPara cada uno de los clusters creados, comprobar si seleccionando otra observación como medoid se consigue reducir la distancia promedio del cluster, si esto ocurre seleccionar la observación que consigue una mayor reducción como nuevo medoid.\nSi al menos un medoid ha cambiado en el paso 4, volver al paso 3, de lo contrario se termina el proceso.\n\nPor lo general, el método de K-medoids se utiliza cuando se conoce o se sospecha de la presencia de outliers. Si esto ocurre, es recomendable utilizar como medida de similitud la distancia de Manhattan, ya que es menos sensible a outliers que la euclídea.\n\n\n5.3.2 Ventajas y desventajas\n\nK-medoids es un método de clustering más robusto que K-means, por lo es más adecuado cuando el set de datos contiene outliers o ruido.\nAl igual que K-means, necesita que se especifique de antemano el número de clusters que se van a crear. Esto puede ser complicado de determinar si no se dispone de información adicional sobre los datos.\nPara sets de datos grandes necesita muchos recursos computacionales. En tal situación se recomienda aplicar el método CLARA.\n\n\n\n5.3.3 Ejemplo\nEl proceso a seguir en R para aplicar el método de K-medoids es igual al seguido en K-means, pero en este caso empleando la función pam() del paquete cluster.\n\ndata(\"USArrests\")\nstr(USArrests)\n\n'data.frame':   50 obs. of  4 variables:\n $ Murder  : num  13.2 10 8.1 8.8 9 7.9 3.3 5.9 15.4 17.4 ...\n $ Assault : int  236 263 294 190 276 204 110 238 335 211 ...\n $ UrbanPop: int  58 48 80 50 91 78 77 72 80 60 ...\n $ Rape    : num  21.2 44.5 31 19.5 40.6 38.7 11.1 15.8 31.9 25.8 ...\n\n\nComo la magnitud de los valores difiere notablemente entre variables, se procede a escalarlas antes de aplicar el clustering.\n\ndatos &lt;- scale(USArrests)\n\nUna forma sencilla de estimar el número \\(K\\) óptimo de clusters cuando no se dispone de información adicional en la que basarse es aplicar el algoritmo para un rango de valores de \\(K\\), identificando aquel a partir del cual la reducción en la suma total de varianza intra-cluster deja de ser sustancial. La función fviz_nbclust() automatiza este proceso. En este caso, dado que se sospecha de la presencia de outliers, se emplea la distancia de Manhattan como medida de similitud.\n\nlibrary(cluster)\nlibrary(factoextra)\nfviz_nbclust(x = datos, FUNcluster = pam, method = \"wss\",\n             diss = dist(datos, method = \"manhattan\"))\n\n\n\n\n\n\n\n\nA partir de 4 clusters la reducción en la suma total de cuadrados internos parece estabilizarse, indicando que \\(K = 4\\) es una buena opción.\n\nset.seed(123)\npam_clusters &lt;- pam(x = datos, k = 4, metric = \"manhattan\")\npam_clusters\n\nMedoids:\n         ID     Murder    Assault   UrbanPop         Rape\nAlabama   1  1.2425641  0.7828393 -0.5209066 -0.003416473\nMichigan 22  0.9900104  1.0108275  0.5844655  1.480613993\nOklahoma 36 -0.2727580 -0.2371077  0.1699510 -0.131534211\nIowa     15 -1.2829727 -1.3770485 -0.5899924 -1.060387812\nClustering vector:\n       Alabama         Alaska        Arizona       Arkansas     California \n             1              2              2              3              2 \n      Colorado    Connecticut       Delaware        Florida        Georgia \n             2              4              3              2              1 \n        Hawaii          Idaho       Illinois        Indiana           Iowa \n             3              4              2              3              4 \n        Kansas       Kentucky      Louisiana          Maine       Maryland \n             3              3              1              4              2 \n Massachusetts       Michigan      Minnesota    Mississippi       Missouri \n             3              2              4              1              3 \n       Montana       Nebraska         Nevada  New Hampshire     New Jersey \n             3              3              2              4              3 \n    New Mexico       New York North Carolina   North Dakota           Ohio \n             2              2              1              4              3 \n      Oklahoma         Oregon   Pennsylvania   Rhode Island South Carolina \n             3              3              3              3              1 \n  South Dakota      Tennessee          Texas           Utah        Vermont \n             4              1              2              3              4 \n      Virginia     Washington  West Virginia      Wisconsin        Wyoming \n             3              3              4              4              3 \nObjective function:\n   build     swap \n1.730682 1.712075 \n\nAvailable components:\n [1] \"medoids\"    \"id.med\"     \"clustering\" \"objective\"  \"isolation\" \n [6] \"clusinfo\"   \"silinfo\"    \"diss\"       \"call\"       \"data\"      \n\n\nEl objeto devuelto por pam() contiene entre otra información: las observaciones que finalmente se han seleccionado como medoids y el cluster al que se ha asignado cada observación.\n\nfviz_cluster(object = pam_clusters, data = datos, ellipse.type = \"t\", repel = TRUE) +\n  theme_bw() + theme(legend.position = \"none\")\n\n\n\n\n\n\n\n# Como en k-medoids no hay centroides, no se muestran en la representación ni\n# tampoco las distancias desde este al resto de observaciones\n\nLa función fviz_cluster() no permite resaltar las observaciones que actúan como medoids, sin embargo, al tratarse de un objeto ggplot2 es sencillo conseguirlo.\n\n# Como hay más de 2 variables, se están representando las 2 primeras componentes\n# de un PCA. Se tienen que calcular el PCA y extraer las proyecciones almacenadas\n# en el elemento x\nmedoids &lt;- prcomp(datos)$x\n# Se seleccionan únicamente las proyecciones de las observaciones que son medoids\nmedoids &lt;- medoids[rownames(pam_clusters$medoids), c(\"PC1\", \"PC2\")]\nmedoids &lt;- as.data.frame(medoids)\n# Se emplean los mismos nombres que en el objeto ggplot\ncolnames(medoids) &lt;- c(\"x\", \"y\")\n# Creación del gráfico\nfviz_cluster(object = pam_clusters, data = datos, ellipse.type = \"t\", repel = TRUE) +\n  theme_bw() +\n# Se resaltan las observaciones que actúan como medoids\ngeom_point(data = medoids, color = \"firebrick\", size = 2) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n5.3.4 Clara\nUna de las limitaciones del método K-medoids-clustering es que su algoritmo requiere mucha memoria RAM, lo que impide que se pueda aplicar cuando el set de datos contiene varios miles de observaciones. CLARA (Clustering Large Applications) es un método que combina la idea de K-medoids con el resampling para que pueda aplicarse a grandes volúmenes de datos.\nEn lugar de intentar encontrar los medoids empleando todos los datos a la vez, CLARA selecciona una muestra aleatoria de un tamaño determinado y le aplica el algoritmo de PAM (K-medoids) para encontrar los clusters óptimos acorde a esa muestra. Utilizando esos medoids se agrupan las observaciones de todo el set de datos. La calidad de los medoids resultantes se cuantifica con la suma total de las distancias entre cada observación del set de datos y su correspondiente medoid (suma total de distancias intra-clusters). CLARA repite este proceso un número predeterminado de veces con el objetivo de reducir el sesgo del muestreo. Por último, se seleccionan como clusters finales los obtenidos con los medoids que han minimizado la suma total de distancias.\nPara más información sobre CLARA y su algoritmo, se recomienda ver la referencia (véase Amat 2017).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Agrupamiento (Clustering)</span>"
    ]
  },
  {
    "objectID": "t5_clustering.html#jerarquico",
    "href": "t5_clustering.html#jerarquico",
    "title": "5  Agrupamiento (Clustering)",
    "section": "5.4 Jerarquico",
    "text": "5.4 Jerarquico\n\n5.4.1 Introducción\nLos métodos jerárquicos parten de una matriz \\(D\\) de semejanzas o de distancias entre los objetos. Si tenemos \\(p\\) objetos, necesitamos una matriz\n\\[D=\\left( \\begin{array}{cccc}\nd_{11} & d_{12} & \\cdots & d_{1p} \\\\\nd_{21} & d_{22} & \\cdots & d_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nd_{p1} & d_{p2} & \\cdots & d_{pp}\n\\end{array} \\right),\\]\ndonde cada \\(d_{ij}\\) es la distancia o la semejanza entre el objecto \\(i\\) y el objecto \\(j\\).\nEl primer problema es escoger la semejanza o la distancia a utilizar, según el significado que queramos que tenga el clustering. ¡Es una decisión muy importante!\nPartimos de \\(p\\) objectos, de los cuales hemos tomado \\(n\\) mediciones, y los organizamos en filas de una matriz\n\\[X=\\left( \\begin{array}{cccc}\nx_{11} & x_{12} & \\cdots & x_{1p} \\\\\nx_{21} & x_{22} & \\cdots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{p1} & x_{p2} & \\cdots & x_{pp}\n\\end{array} \\right),\\]\ndonde cada entrada es una frecuencia\nSean\n\\[x_{i\\bullet}=\\sum_{k=1}^n x_{ik},\\quad x_{\\bullet k}=\\sum_{i=1}^p x_{ik},\\quad x_{\\bullet\\bullet}=\\sum_{i=1}^p x_{i\\bullet}=\\sum_{k=1}^n x_{\\bullet k}\\] Se recomienda tomar como distancia\n\\[d_{ij}=\\sqrt{\\sum_{i=1}^n\\frac{x_{\\bullet\\bullet}}{x_{\\bullet k}}\\left(\\frac{x_{ik}}{x_{i\\bullet}}-\\frac{x_{jk}}{x_{j\\bullet}}\\right)^2}\\]\nA 3 bosques se han escogido una area de la misma superficie y se han contado els número de ejemplares de 5 plantes.\n\n\n\nBosque\nA\nB\nC\nD\nE\n\n\n\n\nX\n12\n3\n8\n0\n24\n\n\nY\n3\n22\n15\n8\n11\n\n\nZ\n0\n7\n12\n20\n6\n\n\n\nTabla con frecuencias marginales:\n\n\n\nBosque\nA\nB\nC\nD\nE\n\\(x_{i\\bullet}\\)\n\n\n\n\nX\n12\n3\n8\n0\n24\n47\n\n\nY\n3\n22\n15\n8\n11\n59\n\n\nZ\n0\n7\n12\n20\n6\n45\n\n\n\\(x_{\\bullet j}\\)\n15\n32\n35\n28\n41\n151\n\n\n\n\\[\\begin{array}{rl}\nd^2_{XY}=&\\frac{151}{15}\\left(\\frac{12}{47}-\\frac{3}{59}\\right)^2+\\frac{151}{32}\\left(\\frac{3}{47}-\\frac{22}{59}\\right)^2+\\frac{151}{35}\\left(\\frac{8}{47}-\\frac{15}{59}\\right)^2\\\\&+\\frac{151}{28}\\left(\\frac{0}{47}-\\frac{8}{59}\\right)^2+\\frac{151}{41}\\left(\\frac{24}{47}-\\frac{11}{59}\\right)^2=\\ldots\n\\end{array}\\]\n\\[D=\\left( \\begin{array}{ccc}\n0 & 1.178 & 1.525 \\\\\n& 0 & 0.880 \\\\\n& & 0\n\\end{array} \\right)\\]\nExisten dos tipos de métodos de clustering jerárquico:\n\nLos algoritmos aglomerativos comienzan con la partición más fina posible (cada objeto constituye un cluster) y los van agrupando.\nLos algoritmos de división comienzan con la partición más gruesa posible (todos los objetos constituyen un cluster) y van dividiendo los clusters en clusters más pequeños.\n\nLos algoritmos aglomerativos son más populares, porque en general requieren menos tiempo de cálculo\n\n\n5.4.2 Ejemplo\n[AÑADIR TEXTO]\n\n\n5.4.3 Jerarquico aglomerativo\n\n5.4.3.1 Algoritmo\n\nPartimos de \\(p\\) objetos, y de la matriz \\(p\\times p\\) de distancias entre ellos\nFormamos un cluster con cada objeto\nEncontramos dos clusters con la distancia mínima \\(C_1\\) y \\(C_2\\)\nUnimos \\(C_1\\) y \\(C_2\\) en un cluster nuevo \\(C_1+C_2\\)\nEliminamos \\(C_1\\) y \\(C_2\\) de la lista de clusters\nRecalculamos la distancia de \\(C_1+C_2\\) a los otros clusters\nRepetimos (3)–(6) hasta que solo queda un único cluster\n\n\n5.4.3.1.1 Enlaces\nEl cálculo de la distancia entre clusters se puede hacer de diversas maneras, dando lugar a resultados diferentes:\n\nPara el enlace simple: \\(d(C,C')=\\min\\{d(a,b)\\mid a\\in C,b\\in C'\\}\\). En este caso \\[d(C,C_1+ C_2)=\\min\\{d(C,C_1),d(C,C_2)\\}.\\] Este método tiende a construir clusters grandes: clusters que tendrían que ser diferentes pero que tienen dos individuos cercanos se unen en un único cluster.\nPara el enlace completo: \\(d(C,C')=\\max\\{d(a,b)\\mid a\\in C,b\\in C'\\}\\). En este caso \\[d(C,C_1+ C_2)=\\max\\{d(C,C_1),d(C,C_2)\\}.\\] Este método se va al otro extremo, y tiende a agrupar clusters solo cuando todos los puntos son cercanos.\nPara el enlace medio: \\(d(C,C')=\\frac{\\sum_{a\\in C, b\\in C'} d(a,b)}{|C|\\cdot |C'|}\\). En este caso, \\[d(C,C_1+C_2)=\\frac{|C_1|}{|C_1|+|C_2|}d(C,C_1)+\\frac{|C_2|}{|C_1|+|C_2|}d(C,C_2).\\] Este método suele ser una solución intermedia entre el enlace simple y el completo. Es muy utilizado en la reconstrucción de árboles filogenéticos a partir de matrices de distancies (método UPGMA, Unweighted Pair Group Method Using Arithmetic averages)\nPara el método de Ward: seguimos un principio muy diferente.  Se define la heterogeneidad de un cluster \\(C\\) como \\[I_C = \\frac{1}{n_C} \\sum_{x_i\\in C} d^2(x_i,\\mathbf{c}_C),\\] donde \\(\\mathbf{c}_C\\) representa el punto medio del cluster \\(C\\) respecto de la distancia utilizada. Si \\(d\\) es la distancia euclidiana, \\(I_C\\) es la varianza del cluster \\(C\\).  Cuando dos clusters se unen, \\[I_{C_1+C_2}=I_{C_1}+I_{C_2}+\\frac{n_{C_1}\\cdot n_{C_2}}{n_{C_1}+ n_{C_2}} d^2 (C_1,C_2).\\] El método de Ward une los clusters de manera que el aumento de la suma de las heterogeneidades sea mínima, y el resultado es que los grupos son (globalmente) lo más homogéneos posible.\n\nEn general, conocidas \\(d(C,C_1\\)), \\(d(C,C_2)\\) y \\((C_1,C_2),\\) hay una fórmula genérica para calcular \\(d(C,C_1+ C_2)\\):\n\\[\\begin{array}{rl}\nd(C,C_1+C_2)= &\\delta_1 d(C,C_1)+\\delta_2 d(C,C_2)+\\delta_3 d(C_1,C_2) \\\\ & + \\delta_0 |d(C,C_1)-d(C,C_2)|,\n\\end{array}\\] donde los \\(\\delta_i\\) son parámetros a probar. Cada elección da un algoritmo diferente, con resultados posiblemente diferentes.\nSi le decimos \\(n_X\\) al número de elementos de un cluster \\(X\\):\n\n\n\n\n\n\n\n\n\n\nNom\n\\(\\delta_1\\)\n\\(\\delta_2\\)\n\\(\\delta_3\\)\n\\(\\delta_0\\)\n\n\n\n\nEnlace simple\n\\(1/2\\)\n\\(1/2\\)\n\\(0\\)\n\\(-1/2\\)\n\n\nEnlace completo\n\\(1/2\\)\n\\(1/2\\)\n\\(0\\)\n\\(1/2\\)\n\n\nEnlace medio\n\\(\\frac{n_{C_1}}{n_{C_1} + n_{C_2}}\\)\n\\(\\frac{n_{C_2}}{n_{C_1}+ n_{C_2}}\\)\n\\(0\\)\n\\(0\\)\n\n\nCentroide\n\\(\\frac{n_{C_1}}{n_{C_1} + n_{C_2}}\\)\n\\(\\frac{n_{C_2}}{n_{C_1} + n_{C_2}}\\)\n\\(-\\frac{n_{C_1} n_{C_2}}{(n_{C_1} + n_{C_2})^2}\\)\n\\(0\\)\n\n\nMediana\n\\(1/2\\)\n\\(1/2\\)\n\\(-1/4\\)\n\\(0\\)\n\n\nWard\n\\(\\frac{n_{C} + n_{C_1}}{n_C + n_{C_1} + n_{C_2}}\\)\n\\(\\frac{n_C + n_{C_2}}{n_C + n_{C_1}+ n_{C_2}}\\)\n\\(-\\frac{n_C}{n_C + n_{C_1} + n_{C_2}}\\)\n\\(0\\)\n\n\n\n\n\n\n5.4.3.2 Ejemplo\nA continuación, realizaremos un ejemplo para ver como funciona el algoritmo. \\(A,B,C,D,E,F,G\\) son diferentes tipos de plantas, \\(x,y\\) son genes y nuestros datos son la expresión del gen en condiciones de sequía.\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nEn este caso, utilizaremos la distancia euclidiana.\n\n\n\nMatriz de distancias\n\n\nUsaremos el enlace simple. Ahora, vamos a ilustrar los pasos que sigue el algoritmo uno a uno, y resaltaremos la distancia mínima en rojo a cada paso.\n\n\n\n\n\n\n Substituimos \\(\\{\\mathbf{A},\\mathbf{B}\\}\\) por \\(\\mathbf{H}\\) y calculamos \n\n\n\n\n\n\n\n\n\n\n\n\n Substituimos \\(\\{\\mathbf{F},\\mathbf{G}\\}\\) por \\(\\mathbf{I}\\) y calculamos \n\n\n\n\n\n\n\n\n\n\n\n\n Substituimos \\(\\{\\mathbf{H},\\mathbf{C}\\}\\) por \\(\\mathbf{J}\\) y calculamos \n\n\n\n\n\n\n\n\n\n\n\n\n Substituimos \\(\\{\\mathbf{J},\\mathbf{D}\\}\\) por \\(\\mathbf{K}\\) y calculamos \n\n\n\n\n\n\n\n\n\n\n\n\n Substituimos \\(\\{\\mathbf{K},\\mathbf{E}\\}\\) por \\(\\mathbf{L}\\) y calculamos \n\n\n\n\n\n\nFinalmente, unimos \\(\\mathbf{L}\\) y \\(\\mathbf{I}\\) en un solo cluster\n\n\n\n5.4.3.3 Limitaciones\nUtilizar este método tiene sus complicaciones y restricciones:\n\nLa distancia utilizada es muy importante\nNo hay teoría que avale cual método para calcular la distancia entre clusters es el mejor en cada caso\nRealmente, no define directamente clusters, pero podemos obtenerlos cortando a una altura del dendrograma\nSiempre agrupa de dos en dos, y a vegades toma decisiones aleatorias para conseguirlo\n\n\n\n5.4.3.4 Ejemplo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nload(url(\"https://www.causeweb.org/tshs/datasets/ultrarunning.RData\"))\ndatos &lt;- ultrarunning %&gt;% as_tibble() %&gt;% na.omit(); datos &lt;- datos[,c(1,4,6,7)] %&gt;% scale()\n\nAl aplicar un hierarchical clustering se tiene que escoger una medida de distancia (1-similitud) y un tipo de linkage. En este caso, se emplea la función hclust() indicando la distancia euclídea como medida de similitud y se comparan los linkages complete, single, average y ward.\n\nmatriz_distancias &lt;- dist(x = datos, method = \"euclidean\")\nh_cluster_completo &lt;- hclust(d = matriz_distancias, method = \"complete\")\nh_cluster_single &lt;- hclust(d = matriz_distancias, method = \"single\")\nh_cluster_average &lt;- hclust(d = matriz_distancias, method = \"average\")\nh_cluster_ward &lt;- hclust(d = matriz_distancias, method = \"ward.D2\")\n\nLos objetos devueltos por hclust() pueden representarse en forma de dendrograma con la función plot() o con la función fviz_dend() del paquete factoextra.\nfviz_dend(x = h_cluster_completo, cex = 0.6)\nfviz_dend(x = h_cluster_single, cex = 0.6)\nfviz_dend(x = h_cluster_average, cex = 0.6)\nfviz_dend(x = h_cluster_ward, cex = 0.6)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEl conocer que existen 4 grupos en la población permite evaluar qué linkage consigue los mejores resultados. En este caso, los tres tipos identifican claramente 4 clusters, si bien esto no significa que en los 3 dendrogramas los clusters estén formados por exactamente las mismas observaciones.\nUna vez creado el dendrograma, hay que evaluar hasta qué punto su estructura refleja las distancias originales entre observaciones. Una forma de hacerlo es empleando el coeficiente de correlación entre las distancias cophenetic del dendrograma (altura de los nodos) y la matriz de distancias original. Cuanto más cercano es el valor a 1, mejor refleja el dendrograma la verdadera similitud entre las observaciones. Valores superiores a 0.75 suelen considerarse como buenos. Esta medida puede emplearse como criterio de ayuda para escoger entre los distintos métodos de linkage. En R, la función cophenetic() calcula las distancias cophenetic de un hierarchical clustering.\n\ncor(x = matriz_distancias, cophenetic(h_cluster_completo))\n\n[1] 0.5814256\n\ncor(x = matriz_distancias, cophenetic(h_cluster_single))\n\n[1] 0.6640045\n\ncor(x = matriz_distancias, cophenetic(h_cluster_average))\n\n[1] 0.7069825\n\ncor(x = matriz_distancias, cophenetic(h_cluster_ward))\n\n[1] 0.4163869\n\n\nEn este caso, el método de linkage average consigue representar ligeramente mejor la similitud entre observaciones.\n\nfviz_dend(x = h_cluster_average, k=8, cex = 0.4)\n\n\n\n\n\n\n\n\nUna forma visual de comprobar los errores en las asignaciones es indicando en el argumento labels el grupo real al que pertenece cada observación. Si la agrupación resultante coincide con los grupos reales, entonces, dentro de cada clusters las labels serán las mismas.\n\naverage_clusters=cutree(h_cluster_average, k = 8)\nperfil = percent_rank(ultrarunning$teique_sf) + percent_rank(ultrarunning$steu_b) + percent_rank(ultrarunning$stem_b)\nperfil &lt;- cut(perfil, 3, labels=c(\"Bajo\", \"Medio\", \"Alto\"))\ntable(average_clusters, perfil)\n\n                perfil\naverage_clusters Bajo Medio Alto\n               1    9    14   14\n               2   11    36   22\n               3    2     6    4\n               4    1     1    0\n               5    0     1    1\n               6    0     1    0\n               7    0     0    1\n               8    1     0    0\n\n\n\n\n\nEl nuestro\nEl óptimo\n\n\n\nAmat, Joaquin. 2017. «Clustering y Heatmaps». 2017. https://github.com/JoaquinAmatRodrigo/Estadistica-con-R/blob/master/PDF_format/37_Clustering_y_Heatmaps.pdf.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Agrupamiento (Clustering)</span>"
    ]
  },
  {
    "objectID": "t6_series_temporales.html",
    "href": "t6_series_temporales.html",
    "title": "6  Series temporales",
    "section": "",
    "text": "6.1 Introducción\nEl estudio de las series temporales es una rama fundamental en diversas disciplinas como la economía, la meteorología, la ingeniería, la medicina y muchas otras, ya que permite comprender la evolución y la variabilidad de una secuencia de observaciones registradas a lo largo del tiempo.\nLa identificación de tendencias, patrones cíclicos y comportamientos estacionales en series temporales proporciona valiosa información para construir modelos predictivos que ayuden en la toma de decisiones, la planificación estratégica y la anticipación de eventos futuros (véase Jonathan D. Cryer 2008) y (véase Peña 2010).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Series temporales</span>"
    ]
  },
  {
    "objectID": "t6_series_temporales.html#introducción",
    "href": "t6_series_temporales.html#introducción",
    "title": "6  Series temporales",
    "section": "",
    "text": "6.1.1 ¿Qué es una serie temporal?\nUna serie temporal es una sucesión de observaciones de una variable realizadas a intervalos regulares de tiempo.\nEl estudio de series temporales tiene por objeto analizar la evolución de una variable a través del tiempo.\nLa diferencia esencial con los análisis temporales es que las observaciones sucesivas no son independientes entre sí, y el análisis debe llevarse a cabo teniendo en cuenta el orden temporal de las observaciones. Por lo tanto, es muy importante conocer la periodicidad de los datos de las series que se están analizando.\nLa periodicidad puede ser:\n\nAnual: Se toma un dato cada año.\nMensual: Se toma un dato cada mes.\nSemanal: Se toma un dato cada semana.\nDiaria: Se toma un dato cada día.\n\nPor supuesto, existen muchos más tipos de periodicidad, como semestral o trimestral. El tipo de periodicidad va a ser algo importante en el análisis de la serie.\nEjemplos de series temporales\n\nSerie del IPC en España. Esta serie puede ser anual o mensual.\nSerie de Temperaturas en Mallorca. Esta serie suele ser mensual. Si fuera anual perderíamos mucha información, pues un invierno extremadamente frío puede compensarse con un verano muy cálido, de modo que la temperatura media del año sea templada.\nSerie de ventas de una empresa. Este tipo de series puede ser anual, mensual o semanal.\nDemanda de energía eléctrica. Esta serie suele obtenerse con periodicidad horaria.\nSeries de cotizaciones de bolsa. Este tipo de series se obtienen con la periodicidad que se quiera.\n\n\n\n6.1.2 ¿Cómo crear una serie de tiempo en R?\nEn R ts es la función genérica para que los datos tengan forma de serie temporal. Su sintaxis es la siguiente:\n\nts(data = NA, start = 1, end = numeric(), frequency = 1)\n\ndonde:\n\ndata: Vector, “data frame” o matriz de datos.\nstart: Referencia de la primera observacion, es un vector con dos valores numéricos, el primero relativo al año y el segundo relativo al trimestre y mes de inicio (1 para el primer trimestre y 1 para enero en series de datos mensuales).\nend: Referencia de la ultima observación.\nfrequency: Número de observaciones por año (4 en series trimestrales, 12 en series mensuales).\n\nUn ejemplo de elaboración de un objeto “ts” es el siguiente:\n\nts(1:10, frequency = 4, start = c(1959, 2)) \n\n     Qtr1 Qtr2 Qtr3 Qtr4\n1959         1    2    3\n1960    4    5    6    7\n1961    8    9   10     \n\n\nPara importar datos a R, usa la función ts de la siguiente manera. El inputData usado aquí es idealmente un vector numérico de clase “numeric” or “integer”.\n\nts(inputData, frequency = 4, start = c(1959, 2)) # datos trimestrales\n\nts(1:10, frequency = 12, start = 1990) # datos mensuales\n\nts(inputData, start=c(2009), end=c(2014), frequency=1) # datos anuales\n\nEjemplo\nConsideremos el gasto mensual por persona (en €) de los turistas con destino principal las Islas Baleares por país de residencia en el período junio 2017 a junio 2019. Fuente: IBESTAT.\n\ngastos=read.table(\"datos/gastos_diarios_IB.txt\",header=TRUE)\ncolnames(gastos) &lt;- c('Fecha', 'Uk','Alemania', 'Suiza')\nhead(gastos,12)\n\n     Fecha     Uk Alemania  Suiza\n1  2017-06 563.52   748.37  89.51\n2  2017-07 734.13   751.15 143.11\n3  2017-08 702.46   785.59  70.78\n4  2017-09 546.31   703.69  36.39\n5  2017-10 301.46   561.62  60.01\n6  2017-11  19.80    86.72  10.24\n7  2017-12  17.55    46.23     NA\n8  2018-01  16.70    51.89     NA\n9  2018-02  14.80    77.56     NA\n10 2018-03  50.43   219.53  13.35\n11 2018-04 165.01   363.07  37.12\n12 2018-05 414.05   578.02  57.94\n\ngastos.ts &lt;- ts(gastos[-1], start = c(2017,6), frequency = 12)\nhead(gastos.ts)\n\n         Uk Alemania  Suiza\n[1,] 563.52   748.37  89.51\n[2,] 734.13   751.15 143.11\n[3,] 702.46   785.59  70.78\n[4,] 546.31   703.69  36.39\n[5,] 301.46   561.62  60.01\n[6,]  19.80    86.72  10.24",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Series temporales</span>"
    ]
  },
  {
    "objectID": "t6_series_temporales.html#series-temporales-y-procesos-estocásticos",
    "href": "t6_series_temporales.html#series-temporales-y-procesos-estocásticos",
    "title": "6  Series temporales",
    "section": "6.2 Series temporales y procesos estocásticos",
    "text": "6.2 Series temporales y procesos estocásticos\nLos fenómenos dinámicos que observamos mediante series temporales pueden considerarse como una realización de un proceso aleatorio o estocástico.\nUn proceso estocástico (o aleatorio) es un conjunto de variables aleatorias \\(\\{X_t\\}\\) donde el índice \\(t\\) toma valores en un cierto conjunto \\(C\\). En el caso de las series temporales, este conjunto es ordenado y corresponde a los instantes temporales (días, meses, años, etc.).\nPara cada valor \\(t\\) del conjunto \\(C\\), la variable aleatoria, \\(X_t\\), toma un valor y, el conjunto de los valores observados en distintos instantes forman una serie temporal.\n\n\n\n\n\nPor tanto, una manera más formal de definir una serie temporal es como una sucesión de observaciones de una variable tomadas en varios instantes de tiempo. Estas observaciones provienen de una distribución que puede ser diferente en cada instante del tiempo.\n\n\n\n\n\nDependiendo de los valores que toma la serie:\n\nTiempo continuo: cuando el valor de la variable puede cambiar en cualquier momento (ej: velocidad del viento).\nTiempo discreto: cuando el valor de la variable puede cambiar en una serie de momentos determinados del tiempo.\nValores continuos: cada variable puede tomar cualquier valor comprendido en un rango (ej: temperatura).\nValores discretos: cada variable sólo puede tomar determinados valores discretos. (ej: activos cuyos precios oscilan de céntimos de € en céntimos de €).\n\nEjemplo:\nConsideremos un proceso estocástico que se conoce como “paseo aleatorio (ramdom walk)”, definido de la siguiente forma: \\[x_t=\\sum_{j=0}^t a_j,\\] donde \\(a_j \\mathop {\\sim}\\limits^{iid} N(0, \\sigma)\\) se conoce como ruido blanco Gaussiano.\nEjercicio: Dibujad con R, tres trayectorias de tamaño 300 de un paseo aleatorio con \\(\\sigma^2= 1\\). Agregad una leyenda al gráfico y las etiquetas de los ejes. Entregad el fichero y su compilación en Aula Digital.\nUn proceso estocástico queda caracterizado por el conjunto de todas las distribuciones finito-dimensionales.\nLa distribución n-dimensional de un proceso: Es la función de distribución de un conjunto de \\(n\\) variables del proceso \\((X_1,\\ldots, X_n)\\), es decir, \\[F(x_1,\\ldots, x_n) = P(x_1 \\leq x_1,\\ldots, X_n ≤ x_n), \\; \\; n \\in N \\]\nNo somos capaces de tratar cualquier tipo de serie temporal, ya que en cada instante tenemos una variable con distinta distribución de la que sólo observamos un dato.\nPara un proceso de tamaño \\(T\\), habría que estimar \\(T\\)-medias, \\(T\\)-varianzas, y \\(T(T-1)/2\\)-autocovarianzas. No disponemos de información suficiente.\nNecesitamos imponer condiciones a la serie\nUn proceso estocástico \\((Y_t)_{t\\in Z}\\) es:\n\nEstable en media: si \\(\\mu_t=\\mu=cte\\)\nEstable en varianza: si \\(\\sigma^2_t=\\sigma^2_y=cte\\)\n`Estable en autocovarianza: si \\(\\gamma_{t_1,t_1+h}=\\gamma_{t_2,t_2+h}=\\gamma_h\\) para cualquier par de instantes \\(t_1,t_2 \\in Z\\) y cualquier \\(h \\in Z\\).\nEstacionario en sentido débil: si es estable en media y en autocovarianza.\nEstacionario en sentido estricto: si las distribuciones marginales de todas las variables son idénticas y además la distribución finito-dimensional de cualquier conjunto de variables sólo depende de los retardos. Es decir, si \\[F_{t_1,\\ldots,t_k}(y_1,\\ldots,y_k)=F_{t_1+h,\\ldots,t_k+h}(y_1,\\ldots,y_k)\\] para cualquier \\(k \\in N\\), \\(t_1,\\ldots,t_k\\), \\(h \\in Z\\), y \\(y_1,\\ldots,y_k \\in R\\), donde \\(F_{t_1,\\ldots,t_k}\\) denota la distribución conjunta de \\(Y_{t_1},\\ldots, Y_{t_k}\\).\n\n\n6.2.1 Ejemplos para clasificar",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Series temporales</span>"
    ]
  },
  {
    "objectID": "t6_series_temporales.html#descripción-de-series-temporales",
    "href": "t6_series_temporales.html#descripción-de-series-temporales",
    "title": "6  Series temporales",
    "section": "6.3 Descripción de series temporales",
    "text": "6.3 Descripción de series temporales\nUna vez que has leído una serie de tiempo en R, el siguiente paso suele ser hacer un gráfico que muestre la evolución de la serie en el tiempo. Esto se puede hacer con la función plot.ts de R.\n\nplot.ts(gastos.ts)\n\n\n\n\n\n\n\n\nPero, en el caso en que tenemos varias series tomadas en el mismo instante de tiempo, podemos utilizar las siguientes instrucciones para visualizar mejor.\n\nplot(gastos.ts, plot.type=\"single\", \n     col = 1:ncol(gastos.ts), ylim=c(0,1000),\n     xlab=\"Fecha\", \n     ylab=\"Gasto mensual por persona en €\")\nlegend(\"topright\", colnames(gastos.ts), col=1:ncol(gastos.ts), lty=1, cex=.65)\n\n\n\n\n\n\n\n\nSi queremos ubicar dónde se producen los ciclos (movimientos de carácter periódico), podemos utilizar un gráfico de cajas y bigotes como el siguiente:\n\n\n\n\n\n\n\n\n\nOtras ideas para visualizar series temporales:\n\nggplot\nR-bloggers\n\nAdemás de identificar los ciclos, cuando visualizamos una(s) serie(s) temporal(es) se suele prestar atención a las siguientes características:\n\nTendencia: Se refiere a si la serie tiende a crecer o decrecer a largo plazo. Por ejemplo:\n\n\ndata(co2)\nplot.ts(co2)\n\n\n\n\n\n\n\n\nCuando una serie permanece más o menos constante, oscilando en torno a un valor, decimos que la serie no tiene tendencia.\n\n\nVariabilildad: Decimos que una serie es Homocedástica, si su variabilidad se mantiene constante a lo largo de la serie. Cuando la variabilidad de la serie aumenta o disminuye a lo largo del tiempo, decimos que la serie es Heterocedástica. La siguiente figura muestra una serie heterocedástica en la que la varianza va aumentando con el tiempo.\n\n\nUna serie puede tener tendencia y ser heterocedástica\n\n\nEstacionalidad: Corresponde a aquellos comportamientos de tipo regular y repetitivo que se dan a lo largo de un período de tiempo, generalmente igual o inferior a un año, y que son producidos por factores tales como las variaciones climatológicas, las vacaciones, las fiestas, etc.\n\nNo confundas estacionalidad con fluctuación cíclica, que refleja los movimientos de carácter periódico, pero no necesariamente regulares, a medio plazo en torno a la tendencia.\nVeamos si puedes identificar el tipo de comportamiento que presenta la siguiente serie\n\n\n\n\n\n\n\n\n\nHasta ahora hemos descrito el aspecto de la serie. Sin embargo, cuando se quiere analizar la serie es necesario identificar la estructura que la genera, es decir cómo influyen las observaciones del pasado en las observaciones del futuro.\n\n6.3.1 Función de autocorrelación simple\nLa función de autocovarianza de un proceso estocástico \\((Y_t)\\) es una función de \\(t\\) que proporciona las covarianzas entre las variables del proceso en cada par de instantes: \\[\\gamma_{t_1,t_2}=Cov(Y_{t_1},Y_{t_2})=E[(Y_{t_1}-\\mu_{t_1})(Y_{t_2}-\\mu_{t_2})], \\, \\, t_1,t_2 \\in I\\]\nLa función de autocorrelación de un proceso estocástico \\((Y_t)\\) es una función de dos instantes de tiempo que describe las correlaciones entre las variables en un par de instantes \\(t_1,t_2 \\in I:\\)\\(\\rho_{t_1,t_2}=Cor(Y_{t_1},Y_{t_2})=\\frac{\\gamma_{t_1,t_2}}{\\sigma_{t_1}\\sigma_{t_2}}, \\; \\;  t_1,t_2 \\in I\\)$\nLa función de autocorrelación simple es una serie que proporciona la estructura de dependencia lineal de la misma.\nSi los valores observados de la serie son: \\(y_1,y_2,\\ldots,y_{t-2},y_{t-1},y_{t}\\). Entonces, \\(y_{t+1}\\) representa el valor de la serie para próximo periodo, es decir un valor futuro.\nLuego, la función de autocorrelación proporciona el coeficiente de correlación entre las observaciones separadas un número determinado de periodos. Así la FAS (ACF en R), va a ser una sucesión de números:\n\n\\(\\rho_1\\) indica cómo influye una observación sobre la siguiente: \\(y_i \\rightarrow y_{i+1}\\).\n\\(\\rho_2\\) indica cómo influye una observación sobre la que está dos periodos en adelante: \\(y_i \\rightarrow y_{i+2}\\).\n\\(\\rho_3\\) indica cómo influye una observación sobre la que está tres periodos en adelante: \\(y_i \\rightarrow y_{i+3}\\).\nY así sucesivamente\n\nLos coeficientes de la FAS, \\(\\rho_1, \\cdots, \\rho_k,\\cdots\\) están acotados entre \\([-1,1]\\). Cuando un \\(\\rho_i\\) vale cero, quiere decir que no existe efecto entre una observación y la \\(i\\) posiciones posteriores.\nSi \\(\\rho_i\\) es próximo a 1 indica que hay mucha relación entre una observación y la \\(i\\) posiciones posteriores, y que esa relación es positiva.\nSi \\(\\rho_i\\) es próximo a -1 indica que hay mucha relación entre una observación y la \\(i\\) posiciones posteriores, y que esa relación es negativa.\nLa FAS proporciona cómo una observación influye sobre las posteriores.\nEjemplo:\nSerie del consumo mensual de gasolina en España 1/1966 - 8/1977\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEn el gráfico de la FAS anterior (ACF) se observa que los coeficientes (o barras) son significativos para retardos 1,2,3 y luego esto se repite para 11,12,13.\nLas bandas horizontales que se observan proporcionan los límites para considerar significativo un retardo.\nPero, la FAS tiene un problema, y es que si por ejemplo \\(\\rho_1\\) es distinto de cero, entonces: \\[y_1 \\rightarrow y_2  \\rightarrow \\cdots \\rightarrow y_{t-1} \\rightarrow y_{t} \\rightarrow y_{t+1} \\cdots \\] es decir existe una cadena de influencia separada por un retardo. Pero si \\(y_1 \\rightarrow y_2\\) y \\(y_2 \\rightarrow y_3\\), entonces \\(y_1 \\rightarrow y_3\\).\nPor tanto, la ACF en general, si \\(\\rho_1\\) es distinto de cero, encontrará que \\(\\rho_2, \\rho_3, \\cdots\\) serán distintos de cero.\nPero, es necesario distinguir la cadena de influencia general, a través de \\(\\rho_1\\) y las cadenas de influencia directa. Es decir cómo influye \\(y_1\\) sobre \\(y_3\\) directamente, es decir SIN PASAR A TRAVÉS DE \\(y_2\\).\n\n\n6.3.2 Función de autocorrelación parcial\nLa Función de autocorrelación parcial, que en R es la función PACF proporciona la relación directa que existe entre observaciones separadas por \\(k\\) retardos.\nEsta es una información muy valiosa sobre la estructura de la serie, ya que elimina el problema que presentaba la función de autocorrelación simple.\nEjemplo:\n\ny=cumsum(rnorm(300,0,1))\nplot.ts(y)\n\n\n\n\n\n\n\nacf(y)\n\n\n\n\n\n\n\npacf(y)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Series temporales</span>"
    ]
  },
  {
    "objectID": "t6_series_temporales.html#serie-estacionaria",
    "href": "t6_series_temporales.html#serie-estacionaria",
    "title": "6  Series temporales",
    "section": "6.4 Serie estacionaria",
    "text": "6.4 Serie estacionaria\nEn general, diremos que una serie es estacionaria cuando cumple las siguientes características:\n\nNo tiene tendencia\nEs homocedástica.\nNo tiene ciclos estacionales\nLa estructura de dependencia se mantiene constante, es decir si una observación influye sobre la posterior, ésto ocurre SIEMPRE. Esta condición es importante para modelizar la serie, pues si el fenómeno que genera la serie cambia, es imposible que podamos prever la evolución de la serie.\nLa influencia de las observaciones sobre las posteriores decrece con el tiempo.\n\n\n6.4.1 Transformaciones para conseguir estacionariedad\nCuando la serie no les estacionaria, es preciso transformarla.\nPara eliminar la tendencia se toman una o varias diferencias en la serie. Una serie se diferencia restando a cada observación la observación anterior: \\[w_t= y_t - y_{t-1}\\]\nEvidentemente la serie diferenciada \\(w\\) tiene una observación menos que la serie original, ya que la primera observación se pierde.\nSi el gráfico de una serie muestra tendencia, se diferencia la serie y se comprueba si ha perdido la tendencia. En caso de que no la haya perdido se diferencia una segunda vez. Es muy raro necesitar más de una o dos diferencias para eliminar la tendencia de una serie.\nNormalmente se utiliza la nomenclatura \\(\\Delta y_t= w_t\\) para representar la serie con una diferencia.\nEjemplo:\n\ngas.ts = ts(gas, start = c(1966,1), frequency = 12)\npar(mfrow=c(1,2))\nplot(gas.ts)\nacf(gas.ts)\n\n\n\n\n\n\n\n\nEn la ACF se observa una pauta que aparece cuando la serie tiene tendencia y por tanto no es estacionaria: las barras disminuyen muy lentamente.\n\nw = diff(x)\npar(mfrow=c(1,2))\nplot.ts(w)\nacf(w)\n\n\n\n\n\n\n\n\nLa serie ya no tiene tendencia, y su ACF decrece rápidamente.\nEstabilización de la varianza:\nPara estabilizar la variabilidad se suelen tomar logaritmos antes de aplicar diferencias en la serie\nPosteriormente, cuando vayamos a hacer la predicción de la serie habrá que deshacer las diferencias y aplicando antilogaritmos según convenga.\nEliminar una tendencia no constante\nEjemplo: Si tenemos una serie mensual, podemos usar una media móvil de 12 meses: \\[\\hat{\\mu_t}=\\frac{z_{t-5}+ \\cdots + z_{t+5}+ z_{t+6}}{12},\\]\nAplicando este método se obtiene una estimación del nivel de la serie en los instantes \\(t=6, \\ldots, T-6\\).\nPara obtener los valores del nivel en los extremos se ajusta una recta a los últimos valores y de esta manera se completa la serie de niveles.\nUna vez identificados los componentes determinísticos (tendencia y estacionalidad) y después de haberlos eliminado, persisten unos valores que son aleatorios.\nSe pretende estudiar qué tipo de comportamiento aleatorio presentan estos residuos, utilizando algún tipo de modelo probabilístico que los describa.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Series temporales</span>"
    ]
  },
  {
    "objectID": "t6_series_temporales.html#descomposición-de-una-serie-con-r",
    "href": "t6_series_temporales.html#descomposición-de-una-serie-con-r",
    "title": "6  Series temporales",
    "section": "6.5 Descomposición de una serie con R",
    "text": "6.5 Descomposición de una serie con R\nLos métodos de descomposición tratan de separar la serie en subseries correspondientes a la tendencia, la estacionalidad y el ruido (componente aleatorio).\nEl primer paso a seguir a la hora de descomponer una serie es determinar cómo se combinan sus componentes.\nCaso aditivo:\n\\[y_t= \\mu_t + S_t + a_t\\]\nCaso multiplicativo:\nEl efecto estacional tiende a aumentar al aumentar la tendencia. \\[y_t= \\mu_t \\times S_t \\times a_t\\]\n\n6.5.1 Conductas de series temporales\n\nEl procedimiento de construcción del modelo se realiza en tres etapas:\n\nSe estima el nivel de la serie observada con el modelo de tendencias deterministas que se fundamenta en modelar la dependencia temporal mediante las ideas de regresión.\n\nModelo:\nSuponemos que la serie se puede descomponer de la siguiente forma: \\[ y_t=\\mu_t+a_t, \\;\\;\\;  t=1,\\ldots,n\\]\n\n\\(\\mu_t=f(t,\\mathbf{\\beta})\\) es el nivel de la serie depende del tiempo y de un vector de parámetros, \\(\\mathbf{\\beta}\\) que se estiman a partir de los datos.\n\\(a_t\\), se denomina error aleatorio (o innovación) en el instante \\(t\\): recoge todos los demás efectos que actuan sobre la serie. Se supone que tiene una estructura estable a lo largo del tiempo: \\(a_t \\mathop {\\sim}\\limits^{iid} N(0, \\sigma)\\).\n\nHabitualmente \\(\\mathbf{\\beta}\\) se obtiene utilizando el método de mínios cuadrados; es decir, resolviendo el problema: \\[\\min_{\\mathbf{\\beta}}\\sum_{t=0}^n [y_t-f(t,\\mathbf{\\beta})]^2\\]\nDerivando la función \\(f(t,\\mathbf{\\beta})=\\sum_{t=0}^n [y_t-f(t,\\mathbf{\\beta})]^2\\) respecto de los elementos de \\(\\mathbf{\\beta}\\) e igualando a cero, se obtienen las ecuaciones normales. El estimador mínimo cuadrático \\(\\hat{\\mathbf{\\beta}}\\) es la solución a dichas ecuaciones.\nLa tendencia estimada es \\(\\hat{\\mu_t}=f(t,\\hat{\\mathbf{\\beta}})\\), donde \\(\\hat{\\mathbf{\\beta}}\\) es un estimador de \\(\\mathbf{\\beta}\\)\n\nA continuación se resta a la serie el nivel estimado para obtener una serie residual, que se denomina serie sin tendencia: \\[E_t=y_t - \\hat{\\mu}_t=S_t + a_t\\]\n\nUna forma sencilla de estimar el efecto de las \\(l\\) distintas estaciones es:\n\nCalcular la media de la serie sin tendencia: \\(\\bar{e}=n^{-1}\\sum_{t=1}^{n}e_t\\).\nCalcular las medias de cada estación: \\(\\bar{e}_j=m^{-1}\\sum_{k=1}^{m}e_{(k-1)l+j}, \\;\\; j=1,\\ldots,l\\).\nLas componentes estacionales se estiman mediante: \\[\\widehat{s}_j=\\overline{e}_j - \\overline{e}  \\;\\; j=1,\\ldots,l\\] y verifican que \\(\\sum_{j=1}^{l}\\widehat{s}_j=0\\)\n\nSe obtiene la serie de residuos restando la serie sin tendencia el factor estacional de cada observación: \\[ \\hat{a}_t=E_t- \\hat{S}_j\\].\nLa predicción de la serie se realiza sumando las estimaciones de la tendencia y el factor estacional que corresponde a cada observación.\nSi la variabilidad de los datos parece crecer con su nivel, podemos suponer un `Caso multiplicativo:.\nTomando logaritmos y renombrando las componentes se obtiene un modelo aditivo.\nEjemplo de descomposición de una serie:\n\ndatos=read.table(\"datos/precio_vestido.txt\",header=TRUE)\n# Ajustamos una tendencia lineal\nx=1:length(datos$vestido)\ntendencia=lm(datos$vestido ~ x)\n# Restamos esta recta a la serie\ne=datos$vestido-tendencia$fitted.values\ne_ts=ts(e,start = c(1993,1),frequency = 12)\nprecio_vestido=ts(datos$vestido,start = c(1993,1),frequency = 12)\n\n\npar(mfrow=c(1,2))\nts.plot(precio_vestido,xlab=\"Tiempo\",ylab=\"Precios vestidos\")\nts.plot(e_ts,xlab=\"Tiempo\",ylab=\"Serie sin tendencia\")\n\n\n\n\n\n\n\n\nEl comando cycle determina la unidad de tiempo a la que pertenece cada observación de la serie.\n\ncycle(e_ts)  \n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n1993   1   2   3   4   5   6   7   8   9  10  11  12\n1994   1   2   3   4   5   6   7   8   9  10  11  12\n1995   1   2   3   4   5   6   7   8   9  10  11  12\n1996   1   2   3   4   5   6   7   8   9  10  11  12\n1997   1   2   3   4   5   6   7   8   9  10  11  12\n1998   1   2   3   4   5   6   7   8   9  10  11  12\n1999   1   2   3   4   5   6   7   8   9  10  11  12\n2000   1   2   3   4   5   6   7   8   9  10  11  12\n2001   1   2   3   4   5   6   7   8   9  10  11  12\n\n\n\ne_barra=mean(e_ts) #media de la serie sin tendencia\ns=rep(0,12) #vector para los coeficientes estacionales\nfor(j in 1:12)\n{\n  indice_j=seq(j,108,by=12)\n  s[j]=mean(e_ts[indice_j])-e_barra\n}\n\n#Calculamos los residuos\ns_compl=rep(s,9) \na=e_ts-s_compl\n\n\nplot.ts(a,xlab=\"Tiempo\",ylab=\"Residuos\")\n\n\n\n\n\n\n\n\n\nserie=ts(datos$vestido,start=c(1993,1),frequency = 12)\nplot(decompose(serie))\n\n\n\n\n\n\n\n\nLos valores estimados de los componentes estacionales, de tendencia e irregulares ahora se almacenan en variables: descompose(serie)$trend, descompose(serie)$seasonal y descompose(serie)$random.\nPor ejemplo, podemos imprimir los valores estimados del componente estacional escribiendo:\n\ndecompose(serie)$seasonal\n\n              Jan          Feb          Mar          Apr          May\n1993  0.078689236 -0.092664931 -0.077039931  0.186501736  0.172960069\n1994  0.078689236 -0.092664931 -0.077039931  0.186501736  0.172960069\n1995  0.078689236 -0.092664931 -0.077039931  0.186501736  0.172960069\n1996  0.078689236 -0.092664931 -0.077039931  0.186501736  0.172960069\n1997  0.078689236 -0.092664931 -0.077039931  0.186501736  0.172960069\n1998  0.078689236 -0.092664931 -0.077039931  0.186501736  0.172960069\n1999  0.078689236 -0.092664931 -0.077039931  0.186501736  0.172960069\n2000  0.078689236 -0.092664931 -0.077039931  0.186501736  0.172960069\n2001  0.078689236 -0.092664931 -0.077039931  0.186501736  0.172960069\n              Jun          Jul          Aug          Sep          Oct\n1993  0.008897569 -0.152560764 -0.325477431 -0.348394097  0.016710069\n1994  0.008897569 -0.152560764 -0.325477431 -0.348394097  0.016710069\n1995  0.008897569 -0.152560764 -0.325477431 -0.348394097  0.016710069\n1996  0.008897569 -0.152560764 -0.325477431 -0.348394097  0.016710069\n1997  0.008897569 -0.152560764 -0.325477431 -0.348394097  0.016710069\n1998  0.008897569 -0.152560764 -0.325477431 -0.348394097  0.016710069\n1999  0.008897569 -0.152560764 -0.325477431 -0.348394097  0.016710069\n2000  0.008897569 -0.152560764 -0.325477431 -0.348394097  0.016710069\n2001  0.008897569 -0.152560764 -0.325477431 -0.348394097  0.016710069\n              Nov          Dec\n1993  0.307855903  0.224522569\n1994  0.307855903  0.224522569\n1995  0.307855903  0.224522569\n1996  0.307855903  0.224522569\n1997  0.307855903  0.224522569\n1998  0.307855903  0.224522569\n1999  0.307855903  0.224522569\n2000  0.307855903  0.224522569\n2001  0.307855903  0.224522569",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Series temporales</span>"
    ]
  },
  {
    "objectID": "t6_series_temporales.html#métodos-armónicos",
    "href": "t6_series_temporales.html#métodos-armónicos",
    "title": "6  Series temporales",
    "section": "6.6 Métodos armónicos",
    "text": "6.6 Métodos armónicos\nUn procedimiento alternativo para modelar la estacionalidad es representar la serie por una función armónica de su periodo \\(l\\).\nSuponiendo que hemos eliminado la tendencia, si la huviese, consideremos series que tienen sólo componente estacional con cliclo único:red, es decir: \\[y_t=S_t+a_t\\]\nLa estacionalidad se puede capturar con una función seno o coseno, de la siguiente forma:\n\\[S_t=A \\sin((2\\pi/l)t+\\theta),\\] donde:\n\n\\(l\\) es el periodo: número de observaciones hasta que la serie se repite.\n\\(A\\) es la amplitud de la oscilación.\n\\(\\theta\\) es el ángulo de desfase con relación al comienzo del ciclo.\n\\(f=1/l\\) se denomina frecuencia de la serie. Es la fracción de ciclo completado entre dos observaciones de la serie. Si \\(l&lt;1\\), entonces \\(f&gt;1\\) es el número de ciclos que pasan entre dos observaciones.\n\nEjemplo:\nUna serie trimestral (\\(l=4\\)), la frecuencia es \\(f=\\frac{1}{4}=0.25\\), indicando que entre dos observaciones, un trimestre, ha transcurrido 0.25 del periodo de la función o un 25% de un ciclo completo.\n\n\\(w=2 \\pi /l=2 \\pi f\\) es la frecuencia angular, o ángulo (en radianes) recorrido entre dos observaciones de la serie. Para \\(l=365 \\implies w=0.0172\\).\n\nPara capturar series de múltiples ciclos, usaremos la representación de Fourier: toda función periódica puede representarse como suma de funciones sinusoidales de distinta amplitud y frecuencia.\nDada una serie de longitud \\(n\\), se denomina periodos básicos o de Fourier a los que son fracciones exactas completas del tamaño muestral. Es decir, \\[s_j=\\frac{n}{j}, \\; \\; para \\; \\;  j=1,2,\\ldots n/2\\]\nPodemos obtener una representación general de una función periódica como suma de ondas asociadas a todas las frecuencias básicas, mediante: \\[y_t=\\mu+\\displaystyle\\sum_{j=1}^{n/2} A_j sen(w_j t) + \\displaystyle\\sum_{j=1}^{n/2} B_j cos(w_j t)\\]\nPara la ecuación anterior se usan los siguientes estimadores:\n\n\\(\\hat{\\mu}=\\frac{1}{n} \\displaystyle\\sum_{t=1}^{T} y_t\\)\n\\(\\hat{A}= \\frac{2}{n} \\displaystyle\\sum_{t=1}^{n} sen(w_jt) y_t\\)\n\\(\\hat{B}= \\frac{2}{n} \\displaystyle\\sum_{t=1}^{n} cos(w_jt) y_t\\)\n\nUn boxplot nos podría ayudar a ver cómo se comporta la serie por ciclos.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Series temporales</span>"
    ]
  },
  {
    "objectID": "t6_series_temporales.html#en-resumen",
    "href": "t6_series_temporales.html#en-resumen",
    "title": "6  Series temporales",
    "section": "6.7 En resumen…",
    "text": "6.7 En resumen…\nFinalizamos la discusión sobre el análisis descriptivo de una serie temporal. En general, deberíamos revisar:\n\nEl gráfico de la serie.\nTendencia: si la hubiera, estimarla (modelo de tendencia deteminista, suavizado o tendencia evolutiva).\nEstacionalidad: revisar el boxplot de la serie en función del ciclo y estimarla con los coeficientes estacionales (tal como lo hace R) o con funciones armónicas.\nLa predicción de la serie se realiza sumando las estimaciones de la tendencia y el factor estacional que corresponde a cada observación.\n\nEn el caso de que estemos trabajando con el caso aditivo: \\(y_t=\\mu_t + S_t +a_t\\).\nEn caso que el efecto estacional tienda a aumentar con la tendencia, usamos el caso multiplicativo: \\(z_t=\\mu_t \\times S_t  \\times a_t\\)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Series temporales</span>"
    ]
  },
  {
    "objectID": "t6_series_temporales.html#modelización-univariante-de-series-temporales",
    "href": "t6_series_temporales.html#modelización-univariante-de-series-temporales",
    "title": "6  Series temporales",
    "section": "6.8 Modelización univariante de series temporales",
    "text": "6.8 Modelización univariante de series temporales\nYa hemos discutido cómo hacer el análisis descriptivo de una serie temporal para establecer si tiene tendencia, si es homocedástica o detectar con los gráficos de las funciones de autocorrelación (simple y parcial) si presenta estructura que depende del rezago de la serie.\nAhora, la idea es proponer modelos que introduzcan las correlaciones que se generan entre relaciones lineales rezagadas y que funcionan bien para series estacionarias:blue}.\nLa representación formal de los procesos aleatorios que generan series reales se puede realizar mediante modelos lineales de series temporales. Para ello se considera la serie temporal ha sido generada por un proceso estocástico, en este parte vamos a describir los posibles modelos teóricos que permiten explicar el comportamiento de la misma y, por tanto, el de su proceso generador.\nLas estructuras estocásticas estacionarias lineales que se tratarán de asociar a una serie de datos se clasifican en tres tipos:\n\nModelos autoregresivos (AR) ,\nModelos de medias móviles (MA),\nModelos mixtos (ARMA)\n\nLuego, los modelos mixtos se pueden extender a modelos para series no estacionarias:blue}, popularizados por Box y Jenkins (1970), que conducen a los:\n\nModelos autorregresivos de media móvil integrados (ARIMA),\n\n\n6.8.1 Procesos autorregresivos (AR)\nLos procesos autorregresivos forman una familia de procesos tales que una observación depende de las observaciones anteriores. Se denominan procesos AR y se caracterizan por su orden.\nLos modelos autorregresivos se basan en la idea de que el valor actual de la serie, \\(y_t\\), puede explicarse como una función de \\(p\\) valores anteriores, \\(y_{t-1}, y_{t-2},\\ldots, y_{t-p}\\), donde \\(p\\) determina el número de pasos hacia el pasado necesarios para pronosticar el valor actual.\nPor ejemplo, diremos que una serie \\(y_t\\) sigue un proceso autoregresivo de primer orden, o un AR(1):blue} si ha sido generado por: \\[y_t=c+\\phi y_{t-1}+a_t,\\] donde \\(c\\) y \\(-1&lt;\\phi &lt; 1\\) son constantes a determinar y \\(a_t\\) es un proceso de ruido blanco con varianza \\(\\sigma^2\\).\nLa condición \\(-1&lt;\\phi &lt; 1\\) es necesaria para que el proceso sea estacionario. Veamos por qué…\nSupongamos que la serie comienza con \\(y_0=h\\), siendo \\(h\\) un valor cualquiera fijo. El siguiente valor será: \\(y_1= c + \\phi h + a_1\\). Luego: \\[\n\\begin{aligned}\ny_2&= c + \\phi y_1 + a_2 \\\\\n    &= c(1+\\phi)+\\phi^2 h + \\phi a_1+a_2\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\ny_3&= c + \\phi y_2 + a_3 \\\\\n    &= c+\\phi[c(1+\\phi)+ \\phi^2 h + \\phi a_1+a_2]+a_3 \\\\\n    &=c(1+\\phi+\\phi^2)+\\phi^3 h+\\phi^2 a_1+\\phi a_2+a_2\n\\end{aligned}\n\\] \\[\\vdots\\]\n\\[y_t= c \\sum_{i=0}^{t-1} \\phi^i + \\phi^t h+ \\sum_{i=0}^{t-1} \\phi^i a_{t-i}\\]\nLuego, como \\(E[a_t]=0\\), nos queda: \\(E[y_t]=c \\sum_{i=0}^{t-1} \\phi^i + \\phi^t h\\). Entonces, si \\(|\\phi|&lt;1\\), la serie geométrica \\(\\sum_{i=0}^{t-1} \\phi^i\\) converge a \\(\\frac{1}{1-\\phi}\\) y \\(\\phi^t\\) converge a cero.\nCon esta condición, depués de un periodo transcurrido inicial, cuando \\(t \\rightarrow \\infty\\), todas las variables \\(y_t\\) tendrán la misma esperanza, independientemente de las condiciones iniciales. \\[E[y_t]=\\frac{c}{1-\\phi}\\]\nObservemos también que en este proceso la innovación \\(a_t\\) está incorrelada con los valores previos del proceso, \\(y_{t-k}\\) para \\(k\\) positivo. En efecto:\nEl valor \\(y_{t-k}\\) depende de los valores de las innovaciones hasta ese instante, \\(a_1, \\ldots, a_{t-k}\\) pero no de sus valores futuros.\nComo la innovación es un proceso de ruido blanco, sus valores futuros están incorrelados con los pasados y, por tanto, con los valores previos de proceso, \\(y_{t-k}\\)\nACF y PACF del proceso AR(1)\nLa función de autocorrelación simple y la parcial del proceso AR(1) tienen un aspecto muy característico.\nSe puede demostrar que la ACF del proceso AR(1) tiene la expresión: \\[\\rho_k=\\phi^k,\\]\nes decir que la barra \\(k\\), de la ACF es igual al coeficiente \\(\\phi\\) elevado a la potencia \\(k\\). Esto implica que la ACF de un proceso AR(1) pueda tener el siguiente aspecto:\n\n\\(\\phi\\) positivo: La ACF será una función positiva y decreciente.\n\\(\\phi\\) negativo: La ACf será una función alternada, y tendrá barras pares positivas, y barras impares negativas.\n\nEn cuanto a la PACF, como indica la expresión AR(1), sólo existe influencia de primer orden, ya que si \\(y_t\\) depende de \\(y_{t-2}\\), es a través de z_{t-1}. Las PACF será por tanto:\n\n\\(\\phi\\) positivo: La PACF tendrá una única barra, la primera. Esta barra será positiva.\n\\(\\phi\\) negativo: La PACF tendrá una única barra y será negativa.\n\nEjemplos AR(1): blue;}\nVamos a simular dos AR(1), uno con \\(\\phi=0.8\\)\n\nAR&lt;-arima.sim(list(order=c(1,0,0),ar=+.8),n=100)\nplot(AR,main=(expression(AR(1)~~~~phi==0.8)))\n\n\n\n\n\n\n\nacf(AR,main=\"autocorrelacion simple de orden 1\")\n\n\n\n\n\n\n\npacf(AR,main=\"autocorrelacion parcial\")\n\n\n\n\n\n\n\n\nSimulamos un Ar(1) con \\(\\phi=-0.8\\)\n\nAR&lt;-arima.sim(list(order=c(1,0,0),ar=-.8),n=100)\nplot(AR,main=(expression(AR(1)~~~~phi==-0.8)))\n\n\n\n\n\n\n\nacf(AR,main=\"autocorrelacion simple de orden 1\")\n\n\n\n\n\n\n\npacf(AR,main=\"autocorrelacion parcial\")\n\n\n\n\n\n\n\n\nOperador de retardo\nEl proceso AR(1) puede escribirse utilizando la notación de operador de retardo, \\(B\\), definido por: \\[By_t=y_{t-1},\\] El operador de retardo es:\n\nLineal: \\(\\;\\;\\; Bay_{t}=aBy_t=ay_{t-1}\\)\nPuede aplicarse sucesivamente: \\(\\;\\;\\; B^ky_t=\\underbrace{B \\cdots B}_{k-veces}y_t=y_{t-k}\\)\n\nDefiniendo \\(\\tilde{y}_t= y_t-\\mu\\), como \\(B \\tilde{y}_t= \\tilde{y}_{t-1}\\), tenemos que: \\[(1-\\phi B)\\tilde{y}_t=a_t\\] Es decir, una serie centrada sigue un proceso AR(1) si al aplicarle el operador \\((1-\\phi B)\\) se obtiene un proceso de ruido blanco. Alternativamente, \\(1-\\phi B =0 \\implies B=1/\\phi\\). La condición de estacionaridad es entonces que la raíz del operador sea, en valor absoluto, mayor que uno.\nLa condición de estacionaridad es que este factor sea menor que la unidad en valor absoluto.\nAlternativamente, podemos hablar de la raíz de la ecuación del operador:\\(1-\\phi B =0 \\implies B=1/\\phi\\). La condición de estacionaridad es entonces que la raíz del operador sea, en valor absoluto, mayor que uno.\nCaracterísticas AR(1)\nEn resumen, las características de un AR(1)\n\nEsperanza:` Si \\(|\\phi|&lt;1\\), entonces \\(E(y_t)=\\mu=\\frac{c}{1-\\phi}\\). Usando que \\(c=\\mu (1-\\phi)\\), el proceso puede escribirse en desviaciones a su media \\(y_t-\\mu=\\phi(y_{t-1}-\\mu)+a_t,\\) y llamando \\(\\tilde{y}_t=y_t-\\mu\\), podemos escribir:\n\n\\[\\tilde{y}_t=\\phi \\tilde{y}_{t-1}+a_t\\]\n\nVarianza: se obtiene calculando \\(E(\\tilde{y}_t^2)\\), de donde \\(\\sigma_y^2=\\frac{\\sigma^2}{1-\\phi^2}\\)\nAutocovarianzas y correlación: se puede demostrar que:\n\n\\(\\gamma_k=\\phi \\gamma_{k-1}\\), para \\(k=1,2,\\ldots\\), donde \\(\\gamma_0=\\sigma^2_y\\)\n\\(\\rho_k=\\phi^k\\)\n\n\n\n\n6.8.2 AR(p)\nDiremos que una serie temporal \\(y_t\\) estacionaria sigue un proceso autoregresivo de orden \\(p\\) si: \\[\\tilde{y}_t=\\phi_1 \\tilde{y}_{t-1}+ \\cdots + \\phi_p \\tilde{y}_{t-p}+a_t,\\] donde \\(\\tilde{y}_t=y_t-\\mu\\), siendo \\(\\mu\\) la media del proceso \\(y_t\\) y \\(a_t\\) un proceso de ruido blanco.\nLos parámetros se determinan a partir de las ecuaciones de Yule-Walker\n\\[\\begin{matrix}\n\\rho_1 & = & \\phi_1+\\phi_2 \\rho_1 + \\cdots + \\phi_p \\rho_{p-1} \\\\\n\\rho_2 & = & \\phi_1 \\rho_1 + \\phi_2 + \\cdots + \\phi_p \\rho_{p-2} \\\\\n& \\vdots & \\\\\n\\rho_p & = & \\phi_1 \\rho_{p-1} + \\phi_2 \\rho_{p-2} + \\cdots + \\phi_p   \n\\end{matrix}\\]\nDefiniendo: \\[\\boldsymbol{\\phi}' = [\\phi_1, \\ldots, \\phi_p],\\] \\[\\boldsymbol{\\varrho}'=[\\rho_1, \\ldots, \\rho_p],\\]\n\\[R=\\begin{bmatrix}\n{1}&{\\rho_1}& \\cdots & \\rho_{p-1}\\\\\n{\\vdots}&{\\vdots} & & \\vdots \\\\\n\\rho_{p-1}& \\rho_{p-2}& \\cdots & 1\n\\end{bmatrix}\\] El sistema se escribe de manera matricialmente: \\[\\boldsymbol{\\varrho}=R \\boldsymbol{\\phi} \\] y los parámetros se determinan a partir de las autocorrelaciones mediante:\n\\[\\boldsymbol{\\phi}=R^{-1}\\boldsymbol{\\varrho}\\]\nEjemplo AR(p) en R:\n\nset.seed(7122021)\nx&lt;-rnorm(500)\nsim1&lt;-arima.sim(list(ar=c(-0.5)),n=500,innov=x)\nsim2&lt;-arima.sim(list(ar=c(.5,-0.5)),n=500,innov=x)\nsim3&lt;-arima.sim(list(ar=c(.2,-.4,.6)),n=500,innov=x)\nsim4&lt;-arima.sim(list(ar=c(.5,.4,-.6,.4)),n=500,innov=x)\n\n\n\n\n\n\n\n\n\n\n\npar(mfrow=c(2,2));acf(sim1); acf(sim2); acf(sim3); acf(sim4)\n\n\n\n\n\n\n\n\n\npar(mfrow=c(2,2));pacf(sim1); pacf(sim2); pacf(sim3); pacf(sim4)\n\n\n\n\n\n\n\n\n¿De qué orden será este AR?\n\n\n\n\n\n\n\n\n\n\n\n6.8.3 Ejemplo práctico\nUsaremos un conjunto de datos incorporado de R llamado AirPassengers. El conjunto de datos consta de el número total de pasajeros de aerolíneas internacionales, de 1949 a 1960 por meses.\n\ndata(\"AirPassengers\")\nis.ts(AirPassengers)\n\n[1] TRUE\n\nprint(AirPassengers)\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n1949 112 118 132 129 121 135 148 148 136 119 104 118\n1950 115 126 141 135 125 149 170 170 158 133 114 140\n1951 145 150 178 163 172 178 199 199 184 162 146 166\n1952 171 180 193 181 183 218 230 242 209 191 172 194\n1953 196 196 236 235 229 243 264 272 237 211 180 201\n1954 204 188 235 227 234 264 302 293 259 229 203 229\n1955 242 233 267 269 270 315 364 347 312 274 237 278\n1956 284 277 317 313 318 374 413 405 355 306 271 306\n1957 315 301 356 348 355 422 465 467 404 347 305 336\n1958 340 318 362 348 363 435 491 505 404 359 310 337\n1959 360 342 406 396 420 472 548 559 463 407 362 405\n1960 417 391 419 461 472 535 622 606 508 461 390 432\n\nsummary(AirPassengers)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  104.0   180.0   265.5   280.3   360.5   622.0 \n\nstart(AirPassengers)\n\n[1] 1949    1\n\nend(AirPassengers)\n\n[1] 1960   12\n\ntime(AirPassengers)\n\n          Jan      Feb      Mar      Apr      May      Jun      Jul      Aug\n1949 1949.000 1949.083 1949.167 1949.250 1949.333 1949.417 1949.500 1949.583\n1950 1950.000 1950.083 1950.167 1950.250 1950.333 1950.417 1950.500 1950.583\n1951 1951.000 1951.083 1951.167 1951.250 1951.333 1951.417 1951.500 1951.583\n1952 1952.000 1952.083 1952.167 1952.250 1952.333 1952.417 1952.500 1952.583\n1953 1953.000 1953.083 1953.167 1953.250 1953.333 1953.417 1953.500 1953.583\n1954 1954.000 1954.083 1954.167 1954.250 1954.333 1954.417 1954.500 1954.583\n1955 1955.000 1955.083 1955.167 1955.250 1955.333 1955.417 1955.500 1955.583\n1956 1956.000 1956.083 1956.167 1956.250 1956.333 1956.417 1956.500 1956.583\n1957 1957.000 1957.083 1957.167 1957.250 1957.333 1957.417 1957.500 1957.583\n1958 1958.000 1958.083 1958.167 1958.250 1958.333 1958.417 1958.500 1958.583\n1959 1959.000 1959.083 1959.167 1959.250 1959.333 1959.417 1959.500 1959.583\n1960 1960.000 1960.083 1960.167 1960.250 1960.333 1960.417 1960.500 1960.583\n          Sep      Oct      Nov      Dec\n1949 1949.667 1949.750 1949.833 1949.917\n1950 1950.667 1950.750 1950.833 1950.917\n1951 1951.667 1951.750 1951.833 1951.917\n1952 1952.667 1952.750 1952.833 1952.917\n1953 1953.667 1953.750 1953.833 1953.917\n1954 1954.667 1954.750 1954.833 1954.917\n1955 1955.667 1955.750 1955.833 1955.917\n1956 1956.667 1956.750 1956.833 1956.917\n1957 1957.667 1957.750 1957.833 1957.917\n1958 1958.667 1958.750 1958.833 1958.917\n1959 1959.667 1959.750 1959.833 1959.917\n1960 1960.667 1960.750 1960.833 1960.917\n\nfrequency(AirPassengers)\n\n[1] 12\n\n\n\nts.plot(AirPassengers, xlab=\"Año\", ylab=\"Número de pasajeros\", main=\"Totales mensuales de pasajeros \\n de aerolíneas internacionales, 1949-1960\")\n\n# Ajustamos una recta de regresión para la tendencia\nabline(reg=lm(AirPassengers~time(AirPassengers)),col=\"red\")\n\n\n\n\n\n\n\nacf(AirPassengers)\n\n\n\n\n\n\n\npacf(AirPassengers)\n\n\n\n\n\n\n\n\nVamos a ajustar un modelo AR\n\nAR &lt;- arima(AirPassengers, order = c(1,0,0))\nprint(AR)\n\n\nCall:\narima(x = AirPassengers, order = c(1, 0, 0))\n\nCoefficients:\n         ar1  intercept\n      0.9646   278.4649\ns.e.  0.0214    67.1141\n\nsigma^2 estimated as 1119:  log likelihood = -711.09,  aic = 1428.18\n\n#Dibujamos la serie y los valores ajustados\nts.plot(AirPassengers)\nAR_fit &lt;- AirPassengers - residuals(AR)\npoints(AR_fit, type = \"l\", col = 2, lty = 2)\n\n\n\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(forecast)\nAR %&gt;% checkresiduals(test = F)\n\n\n\n\n\n\n\n\nLos residuos muestran que aún hay estructura por capturar, por tanto, si hacemos predicciones con este modelo:\n\n#Usamos predict() para hacer una predicción de un paso adelante\npredict_AR &lt;- predict(AR)\n\n#Obtenemos el valor de la predicción\npredict_AR$pred[1]\n\n[1] 426.5698\n\n#Si queremos más predicciones\npredict(AR, n.ahead = 10)\n\n$pred\n          Jan      Feb      Mar      Apr      May      Jun      Jul      Aug\n1961 426.5698 421.3316 416.2787 411.4045 406.7027 402.1672 397.7921 393.5717\n          Sep      Oct\n1961 389.5006 385.5735\n\n$se\n          Jan      Feb      Mar      Apr      May      Jun      Jul      Aug\n1961 33.44577 46.47055 55.92922 63.47710 69.77093 75.15550 79.84042 83.96535\n          Sep      Oct\n1961 87.62943 90.90636\n\n#Dibujamos la serie y agregamos los pronósticos más los intervalos de predicción al 95%\nts.plot(AirPassengers, xlim = c(1949, 1961))\nAR_forecast &lt;- predict(AR, n.ahead = 10)$pred\nAR_forecast_se &lt;- predict(AR, n.ahead = 10)$se\npoints(AR_forecast, type = \"l\", col = \"red\")\npoints(AR_forecast - 2*AR_forecast_se, type = \"l\", col = \"green\", lty = 2, pch=3)\npoints(AR_forecast + 2*AR_forecast_se, type = \"l\", col = \"green\", lty = 2, pch=3)\n\n\n\n\n\n\n\n\nAjustamos un AR(2)\n\nAR2&lt;-arima(x=AirPassengers,order=c(2,0,0))\nprint(AR2)\n\n\nCall:\narima(x = AirPassengers, order = c(2, 0, 0))\n\nCoefficients:\n         ar1      ar2  intercept\n      1.2831  -0.3322   280.4696\ns.e.  0.0786   0.0792    49.4423\n\nsigma^2 estimated as 995.9:  log likelihood = -702.82,  aic = 1413.64\n\nAR2 %&gt;% checkresiduals(test = F)\n\n\n\n\n\n\n\nts.plot(AirPassengers, xlim = c(1949, 1961))\nAR_forecast &lt;- predict(AR2, n.ahead = 10)$pred\nAR_forecast_se &lt;- predict(AR2, n.ahead = 10)$se\npoints(AR_forecast, type = \"l\", col = \"red\")\npoints(AR_forecast - 2*AR_forecast_se, type = \"l\", col = \"green\", lty = 2, pch=3)\npoints(AR_forecast + 2*AR_forecast_se, type = \"l\", col = \"green\", lty = 2, pch=3)\n\n\n\n\n\n\n\n\n\n\n6.8.4 Modelos de Media Móvil (MA)\nLos procesos de media móvil, MA, sirven para representan procesos de memoria muy corta. Son función de un número finito, y generalmente pequeño, de las innovaciones pasadas.\nPor ejemplo, diremos que una serie \\(y_t\\) sigue un proceso de media móvil de primer orden, o un MA(1) si ha sido generado por: \\[\\tilde{y}_t=a_t - \\theta a_{t-1},\\] donde \\(\\tilde{y}_t=y_t-\\mu\\), siendo \\(\\mu\\) la media del proceso y \\(a_t\\) es un proceso de ruido blanco con varianza \\(\\sigma^2\\).\nObserva que MA como parte de los modelos que estamos estudiando se refiere a retardos de errores, mientras que en el análisis descriptivo, se refiere a una técnica de suavizado de datos.\nMA(1)\nEl proceso MA(1) puede escribirse con la notación de operadores: \\[\\tilde{z}_t=(1-\\theta B)a_t\\]\nEste proceso es la suma de dos procesos estacionarios, y por tanto, siempre será estacionario. A pesar de lo anterior, en las aplicaciones de MA, supondremos que \\(| \\theta|&lt;1\\), de manera que la innovación pasada tenga menos peso que la presente.\nPropiedades teóricas del MA(1) Se puede demostrar que:\n\n\\(E(y_t)=\\mu\\).\n\\(Var(y_t)=\\sigma_y^2 (1+\\theta^2)\\).\n\\(\\rho_1=\\frac{\\theta}{1+\\theta^2},\\)\n\\(\\rho_k=0\\) para \\(k&gt;1\\), por tanto, el ACF tendrá únicamente un valor distinto de cero en el primer retardo.\n\nEjemplo MA(1)\nConsidera el siguiente proceso: \\(y_t=10+0.7a_{t-1}+a_t,\\) donde \\(a_t \\mathop {\\sim}\\limits^{iid} N(0, 1)\\).\n\ny_c=arima.sim(n=150,list(ma=0.7))\ny=y_c+10\nplot(y,type=\"l\",main=\"\")\n\n\n\n\n\n\n\n\nDe acuerdo a los resultados teóricos, \\(\\rho_1=\\frac{0.7}{1+(0.7)^2}=0.4698\\) y \\(\\rho_k=0\\) para todo retardo mayor a 1.\n\npar(mfrow=c(1,2))\nacf(y,lag.max = 10) # 10 retardos.\npacf(y,lag.max=10)\n\n\n\n\n\n\n\n\n\n\n6.8.5 Proceso MA(q)\nPodemos escribir procesos cuyo valor actual depende no sólo de la última innovación, sino de las \\(q\\) últimas innovaciones: \\[\\tilde{y}_t=a_t- \\theta_1 a_{t-1}- \\theta_2 a_{t-2} - \\ldots - \\theta_q a_{t-q}\\]\nEn notación de retardos: \\[\\tilde{y}_t=(1-\\theta_1B-\\theta_2B^2- \\ldots \\theta_qB^q) a_t\\]\nPropiedades del MA(q)`\n\nEn general, tiene autocorrelaciones diferentes de cero para los primeros \\(q\\) retardos, es decir, lags &gt; \\(q\\) tienen \\(\\rho=0\\).\n\\(\\rho_k=\\frac{\\sum_{i=0}^q \\theta_i \\theta_{k+i}}{\\sum_{i=0}^q \\theta_i}\\), para \\(k=1,\\ldots,q\\). Además, \\(\\theta_0=-1\\).\n\nEjemplo MA(q) en R\n\nx&lt;-rnorm(500)\nsim1&lt;-arima.sim(list(ma=c(1)),n=500,innov=x)\nsim2&lt;-arima.sim(list(ma=c(1,-1)),n=500,innov=x)\nsim3&lt;-arima.sim(list(ma=c(1,1,1)),n=500,innov=x)\nsim4&lt;-arima.sim(list(ma=c(1/4,1/4,1/4,1/4)),n=500,innov=x)\n\n\n\n\n\n\n\n\n\n\n\npar(mfrow=c(2,2));acf(sim1); acf(sim2); acf(sim3); acf(sim4)\n\n\n\n\n\n\n\n\n\npar(mfrow=c(2,2));pacf(sim1); pacf(sim2); pacf(sim3); pacf(sim4)\n\n\n\n\n\n\n\n\nPara identificar si una serie se puede representar a través de un proceso \\(MA(q)\\) podemos observar los gráficos de la función de autocorrelación simple (ACF) y la función de autocorrelación parcial (PACF).\n\nLa ACF presentará los primeros \\(q\\) coeficientes no nulos.\nEn la PACF, muchos coeficientes serán no mulos`. Presenta una mezcla de decrecimientos geométricos y sinusoidales hacia 0.\n\n\n\n6.8.6 Ajuste de un MA a la serie AirPassengers\n\nMA &lt;- arima(AirPassengers, order = c(0,0,1))\nprint(MA)\n\n\nCall:\narima(x = AirPassengers, order = c(0, 0, 1))\n\nCoefficients:\n         ma1  intercept\n      0.9642   280.6464\ns.e.  0.0214    10.5788\n\nsigma^2 estimated as 4205:  log likelihood = -806.43,  aic = 1618.86\n\n#Diujamos la serie y los valores ajustados del MA\nts.plot(AirPassengers)\nMA_fit &lt;- AirPassengers - resid(MA)\npoints(MA_fit, type = \"l\", col = 2, lty = 2)\n\n\n\n\n\n\n\n\nVeamos qué tal sería la predicción\n\n#Predicción un paso adelante\npredict_MA &lt;- predict(MA)\npredict_MA$pred[1]\n\n[1] 425.1049\n\n#Predicción 10 pasos adelante\npredict(MA,n.ahead=10)\n\n$pred\n          Jan      Feb      Mar      Apr      May      Jun      Jul      Aug\n1961 425.1049 280.6464 280.6464 280.6464 280.6464 280.6464 280.6464 280.6464\n          Sep      Oct\n1961 280.6464 280.6464\n\n$se\n          Jan      Feb      Mar      Apr      May      Jun      Jul      Aug\n1961 64.84895 90.08403 90.08403 90.08403 90.08403 90.08403 90.08403 90.08403\n          Sep      Oct\n1961 90.08403 90.08403\n\n##Dibujamos la serie y agregamos los pronósticos más los intervalos de predicción al 95%\nts.plot(AirPassengers, xlim = c(1949, 1961))\nMA_forecasts &lt;- predict(MA, n.ahead = 10)$pred\nMA_forecast_se &lt;- predict(MA, n.ahead = 10)$se\npoints(MA_forecasts, type = \"l\", col = \"red\")\npoints(MA_forecasts - 2*MA_forecast_se, type = \"l\", col = \"green\", lty = 2)\npoints(MA_forecasts + 2*MA_forecast_se, type = \"l\", col = \"green\", lty = 2)\n\n\n\n\n\n\n\n\n\n\n6.8.7 ¿Qué es mejor para modelizar la serie de AirPassengers un AR o un MA?\nPara responder esta pregunta podemos utilizar o bien el criterio de información de Akaike (\\(AIC\\)) o el criterio de información bayesiano (\\(BIC\\)).\n\\[AIC=-2 \\, log(\\hat{\\sigma}^2_e)+2\\,k\\]\n\\[BIC=-2 \\,log(\\hat{\\sigma}^2_e)+k \\, log(n),\\] donde \\(n\\) es el número de observaciones, \\(k\\) es el número de parámetros, \\(\\hat{\\sigma}^2_e\\) es el estimador de la varianza del error.\nLa fórmula del primer término de \\(BIC\\) es la misma que la de \\(AIC\\), solo difiere en el segundo término. En el segundo término, se puede ver que el valor \\(BIC\\) está influenciado por el tamaño de muestra y parámetros en el modelo.\nLa idea principal es que estos indicadores penalizan los modelos con el número de parámetros estimados, para evitar el sobreajuste, y se prefieren valores más pequeños. Si todos los factores son iguales, un modelo que produce un \\(AIC\\) o \\(BIC\\) más bajo que otro modelo se considera un mejor ajuste.\n\nAIC(AR)\n\n[1] 1428.179\n\nAIC(MA)\n\n[1] 1618.863\n\nBIC(AR)\n\n[1] 1437.089\n\nBIC(MA)\n\n[1] 1627.772\n\n\nDado el valor más bajo de \\(AIC\\) y \\(BIC\\) en el modelo AR, deberíamos preferir a este que al MA para el análisis de series de tiempo de los datos de AirPassenger.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Series temporales</span>"
    ]
  },
  {
    "objectID": "t6_series_temporales.html#armapq",
    "href": "t6_series_temporales.html#armapq",
    "title": "6  Series temporales",
    "section": "6.9 ARMA(p,q)",
    "text": "6.9 ARMA(p,q)\nLa ecuación para los procesos ARMA(p,q) es: \\[(1-\\phi_1B-\\ldots - \\phi_pB^p)\\tilde{y}_t=(1-\\theta_1B- \\ldots \\theta_qB^q)a_t,\\] o en notación compacta: \\[\\Phi_p(B)\\tilde{y}_t=\\Theta_q(B)a_t\\] La ACF y la PACF de los procesos ARMA es el resultado de la superposición de sus propiedades AR y MA. Por tanto, no es fácil identicar la estructura. Tanto la ACF como la PACF presentan muchos coeficientes no nulos. Sin embargo:\n\nEn la ACF los coeficientes presentan decrecimiento hacia 0 desde \\(q\\).\nEn la PACF los coeficientes presentan decrecimiento hacia 0 desde \\(p\\).\n\nEjemplo ARMA(p,q) en R\n\nset.seed(1012)\nx&lt;-rnorm(200)\nsim1&lt;-arima.sim(list(ar=c(.1,.5,.2),ma=c(0.6,.4)),n=200,innov=x)\nsim2&lt;-arima.sim(list(ar=c(-0.4),ma=c(0.6,.1)),n=200,innov=x)\n\n\n\n\n\n\n\n\n\n\n\npar(mfrow=c(2,2));acf(sim1); pacf(sim1); acf(sim2); pacf(sim2)\n\n\n\n\n\n\n\n\nNo resulta nada sencillo en la práctica identificar un proceso ARMA a través de sus acf y pacf, ya que es fácil confundir dichas funciones con las de otros procesos.\nSe aconseja especificar y estimar inicialmente un modelo más sencillo, como por ejemplo un AR; posteriormente el análisis de los residuos obtenidos en dicha estimación pondrá de manifiesto la presencia de otras estructuras.\nSi, por ejemplo, detectas en las funciones de autocorrelación simple y parcial de los residuos obtenidos una estructura de MA será necesario incorporar dicha estructura especificando un modelo ARMA, el cual sin duda tendrá una mayor capacidad explicativa.\n¿Cómo harías esto en el ejemplo anterior?\n\nts.plot(sim1)\n\n\n\n\n\n\n\npacf(sim1)\n\n\n\n\n\n\n\nx&lt;-arima(sim1, order=c(3,0,0))\nprint(x)\n\n\nCall:\narima(x = sim1, order = c(3, 0, 0))\n\nCoefficients:\n         ar1     ar2      ar3  intercept\n      0.7661  0.3930  -0.2644    -0.2371\ns.e.  0.0678  0.0833   0.0686     0.6617\n\nsigma^2 estimated as 1.037:  log likelihood = -288.34,  aic = 586.69\n\ny&lt;-residuals(x)\nts.plot(y)\n\n\n\n\n\n\n\nacf(y)\n\n\n\n\n\n\n\ny %&gt;% checkresiduals(test = F)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Series temporales</span>"
    ]
  },
  {
    "objectID": "t6_series_temporales.html#procesos-integrados",
    "href": "t6_series_temporales.html#procesos-integrados",
    "title": "6  Series temporales",
    "section": "6.10 Procesos Integrados",
    "text": "6.10 Procesos Integrados\nLos procesos no estacionarios más frecuentes son los Procesos Integrados, que tienen la propiedad de que al diferenciarlos se obtienen procesos estacionarios.\nConsidera el proceso \\[w_t = \\nabla y_t = y_t − y_{t−1}\\]\nLos valores de esta nueva serie oscilan alrededor de una media constante y parecen corresponder a una serie estacionaria. En este caso, diremos que la serie es integrada de orden 1.\nAhora, ajustamos un ARMA a \\(w_t\\) y luego integramos: \\[y_t = w_t + y_{t−1}\\]\nA veces es necesario diferenciar más de 1 vez para obtener estacionaridad.\nPor ejemplo:\n\\(w_t = \\nabla y_t\\) , no es estacionario. Entonces, \\[z_t = \\nabla w_t = w_t − w_{t−1} = y_t − \\nabla y_{t−1} + y_{t−2} = \\nabla^2 y_t\\]\nSi \\(z_t\\) es estacionario integramos dos veces para modelar a \\(y_t\\) .\nProceso integrado de orden \\(h\\):\nEn general, diremos que un procesos es integrado de orden \\(h\\geq 0\\) y lo representamos por \\(I(h)\\) cuando, al diferenciarlo \\(h\\) veces se obtiene un proceso estacionario.\nEl modelo ARIMA(\\(p\\),\\(d\\),\\(q\\)) puede ser considerado como el modelo estocástico lineal general, del cual derivan el resto de procesos que hemos visto.\nAsí, si \\(p=d=0\\), estaremos ante un modelo ARIMA(0, 0, \\(q\\)) equivalente a un modelo MA(\\(q\\)). Si \\(q=0\\) tendríamos un modelo ARIMA(\\(p\\), \\(d\\), \\(0\\)) ó ARI(\\(p\\),\\(d\\)) (es decir, un modelo autorregresivo en el que se han tomado \\(d\\) diferencias para hacer estacionaria a la serie analizada).\n\n6.10.1 Ejemplo demostrativo con R\nVamos a trabajar de nuevo con datos simulados.\n\nset.seed(250)\nts=arima.sim(list(order = c(1,1,2), \n                  ma=c(0.32,0.47), \n                  ar=0.8), n = 50)+20\n\nA continuación, dibujamos la serie temporal original y sus acf y pacf.\n\nm &lt;- matrix(c(1, 3, 2, 3), ncol = 2)\nnf &lt;- layout(m)\nplot.ts(ts)\nacf(ts)\npacf(ts)\n\n\n\n\n\n\n\n\nLa serie original muestra una clara no estacionariedad; el acf muestra una disminución lenta y el pacf indica un AR de orden uno.\nVamos a diferenciar la serie para ver si logramos eliminar la tencia.\n\npar(mfrow=c(1,2))\ny=diff(ts)\nplot(y)\nacf(y)\n\n\n\n\n\n\n\n\nLos datos de la primera diferencia parecen estacionarios. El ACF se corta después de algunos rezagos.\nDado que se necesitó una diferenciación para obtener la estacionariedad, aquí \\(d\\) = 1\nEl siguiente paso es proponer los valores de \\(p\\), el orden de la parte AR. Vemos que el PACF de la serie original se corta en uno y el ACF de la serie diferenciada se corta después del rezago 2. Así que proponemos tres modelos para los datos diferenciados:\n\nARIMA(1,1,0)\nARIMA(0,1,2)\nARIMA(1,1,2)\n\nAjustamos los tres modelos ARIMA con las primeras 40 observaciones y dejamos las últimas 10 para pronosticar\n\n## Partición en training y test\ntrain=ts[1:40]\ntest=ts[41:50]\n\n## ajustamos los modelos\narimaModel_1=arima(train, order=c(1,1,0))\narimaModel_2=arima(train, order=c(0,1,2))\narimaModel_3=arima(train, order=c(1,1,2))\n\n## revisamos los parámetros ajustados\nprint(arimaModel_1);print(arimaModel_2);print(arimaModel_3)\n\n\nCall:\narima(x = train, order = c(1, 1, 0))\n\nCoefficients:\n         ar1\n      0.8012\ns.e.  0.0892\n\nsigma^2 estimated as 1.311:  log likelihood = -61.13,  aic = 126.26\n\n\n\nCall:\narima(x = train, order = c(0, 1, 2))\n\nCoefficients:\n         ma1    ma2\n      0.7408  1.000\ns.e.  0.2135  0.537\n\nsigma^2 estimated as 0.9673:  log likelihood = -57.78,  aic = 121.57\n\n\n\nCall:\narima(x = train, order = c(1, 1, 2))\n\nCoefficients:\n         ar1     ma1     ma2\n      0.5585  0.4438  0.7238\ns.e.  0.1581  0.1584  0.1305\n\nsigma^2 estimated as 0.8554:  log likelihood = -53.64,  aic = 115.29\n\n\nObserva que el signo de los coeficientes estimados por R es consistente con la fórmula de la simulación del ARIMA. Observa las varianzas de los términos de error de cada modelo y el AIC, de acuerdo a estos, el mejor modelo es el 3.\nVeamos las predicciones sobre las observaciones que no hemos utilizado para ajustar los parámetros\n\n#Modelo 1\nts.plot(ts)\nforecast1=predict(arimaModel_1, 10)$pred\nforecast1_se=predict(arimaModel_1, 10)$se\npoints(forecast1, type = \"l\", col = \"red\")\npoints(forecast1 - 2*forecast1_se, type = \"l\", col = \"green\", lty = 2)\npoints(forecast1 + 2*forecast1_se, type = \"l\", col = \"green\", lty = 2)\n\n\n\n\n\n\n\n#Modelo 2\nts.plot(ts)\nforecast2=predict(arimaModel_2, 10)$pred\nforecast2_se=predict(arimaModel_2, 10)$se\npoints(forecast2, type = \"l\", col = \"red\")\npoints(forecast2 - 2*forecast2_se, type = \"l\", col = \"green\", lty = 2)\npoints(forecast2 + 2*forecast2_se, type = \"l\", col = \"green\", lty = 2)\n\n\n\n\n\n\n\n#Modelo 3\nts.plot(ts)\nforecast3=predict(arimaModel_3, 10)$pred\nforecast3_se=predict(arimaModel_3, 10)$se\npoints(forecast3, type = \"l\", col = \"red\")\npoints(forecast3 - 2*forecast3_se, type = \"l\", col = \"green\", lty = 2)\npoints(forecast3 + 2*forecast3_se, type = \"l\", col = \"green\", lty = 2)\n\n\n\n\n\n\n\n\nVemos que el segundo modelo tiene una ejecución similar al tres, a pesar de no tener el mejor valor de probabilidad de AIC. Por tanto, necesitamos medidas de la precisión de las estimaciones:\n\naccuracy(forecast2, test)\n\n              ME     RMSE     MAE      MPE     MAPE\nTest set 4.97575 7.150113 4.97575 14.39955 14.39955\n\naccuracy(forecast3, test)\n\n              ME     RMSE     MAE      MPE     MAPE\nTest set 5.38043 7.474219 5.38043 15.73739 15.73739\n\n\nDonde cada medida es:\n\nME error medio\nRMSE error cuadrático medio\nMAE error absoluto medio\nMPE error porcentual medio\nMAPE error porcentual absoluto medio\n\nDepende de cada caso decidir, basándose en las medidas de precisión, si considera que el modelo es adecuado o no. Por ejemplo, el error porcentual medio de 14,4% no me parece grande, pero eso puede depender de cuáles son las series y cuánta previsibilidad podemos esperar de manera realista.\nPero cuidado! nos falta revisar los residuos !!!\n\narimaModel_2 %&gt;% checkresiduals(test = F)\n\n\n\n\n\n\n\narimaModel_3 %&gt;% checkresiduals(test = F)\n\n\n\n\n\n\n\n\nConclusión: Mejor es el modelo 3, tal como lo habíamos simulado ;-)\n\n\n6.10.2 Ejemplo con datos reales\nVamos a analizar un fichero de datos que contiene la edad en que murieron los reyes sucesivos de Inglaterra comenzando con William el conquistador, en total son 42 reyes.\nSegún otra fuente consultada, wikipedia, William fue el sexto rey y murió en 1837 a los 71 años, sin embargo, el fichero indica 60 años.\nLos datos se obtuvieron de un fichero publicado en el artículo “Interactive Data Anlaysis: A Practical Primer” de Don McNeil, publicado el 4 de mayo de 1977.\n\nedad_muerte=ts(kings)\nedad_muerte\n\nTime Series:\nStart = 1 \nEnd = 42 \nFrequency = 1 \n [1] 60 43 67 50 56 42 50 65 68 43 65 34 47 34 49 41 13 35 53 56 16 43 69 59 48\n[26] 59 86 55 68 51 33 49 67 77 81 67 71 81 68 70 77 56\n\n\n¿La serie es estacionaria?\n\nplot.ts(edad_muerte, xlab= \"Dinastía\", \n     ylab=\"Edad muerte del Rey\",\n     main=\"Longevidad de los Reyes Británicos \n     según la dinastía\")\n\n\n\n\n\n\n\n\nVamos a diferenciar la serie:\n\ndiff_o1 &lt;- diff(edad_muerte, differences=1)\nplot.ts(diff_o1)\n\n\n\n\n\n\n\n\nLa serie temporal de las primeras diferencias parece ser estacionaria en la media y la varianza, por lo que un modelo ARIMA(\\(p\\),1,\\(q\\)) es probablemente apropiado para la serie temporal de la edad de la muerte de los reyes de Inglaterra.\nAl tomar las series de tiempo de las primeras diferencias, hemos eliminado el componente de tendencia y nos quedamos con un componente irregular.\nAhora podemos examinar si hay correlaciones entre los términos sucesivos de este componente irregular. Si es así, esto podría ayudarnos a hacer un modelo predictivo para las edades de la muerte de los reyes.\n\npar(mfrow=c(1,2))\nacf(diff_o1, lag.max = 20) # si especificamos plot=FALSE podemos obtener los valores\npacf(diff_o1, lag.max = 20)\n\n\n\n\n\n\n\n\nLos modelos posibles son:\n\nARMA(3,0) es decir, un modelo autorregresivo de orden \\(p = 3\\), ya que el autocorrelograma parcial es cero después del retraso 3, y el autocorrelograma se reduce a cero (aunque quizás muy bruscamente para que este modelo sea apropiado).\nARMA(0,1) un modelo de media móvil de orden \\(q = 1\\), ya que el autocorrelograma es cero después del desfase 1 y el autocorrelograma parcial se reduce a cero.\nARMA(p,q) un modelo mixto con \\(p\\) y \\(q\\) mayores que 0, ya que el autocorrelograma y el correlograma parcial se reducen a cero (aunque el correlograma probablemente se reduce a cero abruptamente para que este modelo sea apropiado)\n\n\nmodelo1 = arima(diff_o1, order=c(3,0,0))\nmodelo2 = arima(diff_o1, order=c(0,0,1))\nmodelo3 = arima(diff_o1, order=c(3,0,1))\nAIC(modelo1,modelo2,modelo3)\n\n        df      AIC\nmodelo1  5 348.5243\nmodelo2  3 345.8136\nmodelo3  6 350.5172\n\nBIC(modelo1,modelo2,modelo3)\n\n        df      BIC\nmodelo1  5 357.0922\nmodelo2  3 350.9543\nmodelo3  6 360.7987\n\n\n\npar(mfrow=c(1,2))\nplot(modelo2$residuals)\nqqnorm(modelo2$residuals)\nqqline(modelo2$residuals,col=\"red\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodelo2$coef\n\n       ma1  intercept \n-0.7462804  0.3881635 \n\n\nUn modelo de MA (promedio móvil) se usa generalmente para modelar una serie de tiempo que muestra dependencias a corto plazo entre observaciones sucesivas.\nDe manera intuitiva, tiene sentido que se pueda usar un modelo de MA para describir el componente irregular en las series de tiempo de la muerte de los reyes ingleses, ya que podríamos esperar que la edad de un rey inglés en particular tenga algún efecto en las edades de muerte del próximo rey o dos, pero no mucho efecto en las edades en la muerte de los reyes que reinarán mucho más tiempo después de eso.\nDado que un modelo ARMA(0,1) (con \\(p = 0\\), \\(q = 1\\)) se considera el mejor modelo candidato para la serie temporal de las primeras diferencias de las edades de muerte de los reyes ingleses, la serie temporal original del las edades de muerte se pueden modelar con un ARIMA(0,1,1).\nVamos a usar a función auto.arima() de la librería forecast para confirmar nuestra selección.\n\nlibrary(forecast)\nauto.arima(edad_muerte)\n\nSeries: edad_muerte \nARIMA(0,1,1) \n\nCoefficients:\n          ma1\n      -0.7218\ns.e.   0.1208\n\nsigma^2 = 236.2:  log likelihood = -170.06\nAIC=344.13   AICc=344.44   BIC=347.56\n\n\nPodemos usar el modelo ARIMA para hacer pronósticos de valores futuros de la serie de tiempo, usando la función forecast.Arima() del paquete forecast.\nPor ejemplo, para pronosticar las edades al morir de los próximos cinco reyes ingleses, escribimos:\n\npronostico=forecast(arima(edad_muerte, order=c(0,1,1)), h=5)\npronostico\n\n   Point Forecast    Lo 80    Hi 80    Lo 95     Hi 95\n43       67.75063 48.29647 87.20479 37.99806  97.50319\n44       67.75063 47.55748 87.94377 36.86788  98.63338\n45       67.75063 46.84460 88.65665 35.77762  99.72363\n46       67.75063 46.15524 89.34601 34.72333 100.77792\n47       67.75063 45.48722 90.01404 33.70168 101.79958\n\n\n\nplot(pronostico)\n\n\n\n\n\n\n\n\n\n\n6.10.3 Ljung Box test\nEl test Ljung Box se utiliza para contrastar la ausencia de autocorrelación en una serie, hasta un retardo específico \\(k\\).\nEl test determina si los errores, luego de ajustar el modelo son independientes e idénticamente distribuidos (ruido blanco) o si existe alguna estructura más en ellos. En otras palabras, si las autocorrelaciones de los errores no son cero.\nUn \\(p\\)-valor significativo (&lt; 0.05) indica rechazar la hipótesis nula, es decir la serie presenta autocorrelación.\nEl estadístico del test para una serie de longitud \\(n\\) es: \\[Q(m)=n(n+2)\\sum_{j=1}^{m} \\frac{r_j^2}{n-j},\\] donde:\n\n\\(r_j\\) son las autocorrelaciones muestrales\n\\(m\\) es el tiempo de retardo.\n\nRechazamos \\(H_0\\) si \\(Q &gt; \\chi_{1-\\alpha,\\nu}^2\\), donde:\n\n\\(\\chi_{1-\\alpha,\\nu}^2\\) es el cuantil de la distribución ji-cuadrado para el nivel de significancia \\(\\alpha\\) y \\(\\nu\\) los grados de libertad.\n\nCuando se aplica el test Ljung Box a los residuos de un modelo ARIMA, los grados de libertad \\(\\nu\\) son \\(m-p-q\\), donde \\(p\\) y \\(q\\) son los parámetros del ARIMA(p,q) model. En R el test está implementaddo en la función Box.test.\n\nBox.test(residuals(modelo2),lag=2)\n\n\n    Box-Pierce test\n\ndata:  residuals(modelo2)\nX-squared = 0.83577, df = 2, p-value = 0.6584\n\n\n\n\n6.10.4 Procesos estacionales: SARIMAs\nOtra causa de no estacionaridad es la estacionalidad.\nPor ejemplo, en una serie mensual con estacionalidad anual, cada mes tiene una media distinta, con lo cual la media no es estable.\nEn muchos casos, un proceso estacional se convierte en estacionario al tomar un número \\(D\\) de diferencias estacionales.\nPara detectar un comportamiento estacional podemos analizar las ACF y PACF de la serie.\nSi al representar dichas funciones se aprecian valores muy altos, significativamente distintos de cero, para los retardos estacionales podremos concluir que la serie presenta un componente estacional y debemos exigir que el componente estacional se mantenga constante a lo largo del tiempo.\nAsí, si observamos que la ACF presenta un lento decaimiento en los valores correspondientes a los retardos estacionales y el valor del primer retardo estacional es próximo a uno tanto en ACF como PACF, es muy probable que el comportamiento estacional de la serie no presente un carácter estacionario, por lo que será necesario tomar diferencias de tipo estacional.\nLos ARIMAs estacionales tienen las siguientes características:\n\nContienen una componente ARIMA(\\(P\\),\\(D\\),\\(Q\\)) que modeliza la dependencia estacional.\nContienen otra componente ARIMA(\\(p\\),\\(d\\),\\(q\\)) que modeliza la dependencia regular, que es la dependencia asociada a observaciones consecutivas.\nEl proceso diferenciado \\(w_t=\\nabla^D \\nabla^d y_t\\) es un proceso estacionario que sigue el modelo ARMA estacional.\n\nEste proceso se denota como\n\\[\\textbf{ARIMA}\\quad\\underbrace{(p,d,q)}_{parte \\,\\, regular} \\times \\underbrace{(P,D,Q,)}_{parte \\,\\,estacional}\\]\nEjemplo de ajuste de un SARIMA\nConsideremos los datos del fichero papel que corresponden a las ventas mensuales de papel correspondiente a 10 años.\n\npapel=scan(\"datos/papel.txt\")\nplot(papel,ylab='Venta papel',xlab='Tiempo',type='o',pch=16)\n\n\n\n\n\n\n\n\n\ndpapel = diff(papel,1)\npar(mfrow=c(2,1))\nplot(dpapel,ylab='Valores',xlab='Tiempo',type='l',pch=16,main=\"Serie sin tendencia\")\nacf(dpapel,60,main=\"\")\n\n\n\n\n\n\n\n\n\npar(mfrow=c(2,1))\nd12dpapel = diff(dpapel,12)\nplot(d12dpapel,ylab='Serie',xlab='Tiempo',type='l',pch=16,\n     main=\"Serie sin tendencia y desestacionalizada\")\n\nacf(d12dpapel,60,main=\"Ventas de papel desestacionalizada y diferenciada\")\n\n\n\n\n\n\n\npacf(d12dpapel,60,main=\"\")\n\n\n\n\n\n\n\n\nLa PACF muestra decaimiento exponencial mientras que el acf muestra un pico bastante significativo y aislado en el retardo 1, entonces debe tratarse de un MA(1). Además de este pico, sigue apareciendo en el retardo 12 de ambas autocorrelaciones, aún hay un remanente de estacionalidad, lo que sugiere agregar un MA(1) estacional.\n\na=c(0,1,1)\n(modelo=arima(papel,a,seasonal=list(order=a,period=12)))\n\n\nCall:\narima(x = papel, order = a, seasonal = list(order = a, period = 12))\n\nCoefficients:\n          ma1     sma1\n      -0.8402  -0.6360\ns.e.   0.0611   0.0929\n\nsigma^2 estimated as 1809:  log likelihood = -556.91,  aic = 1119.83\n\n\n\npronostico=forecast(modelo, h=20)\nplot(pronostico)\n\n\n\n\n\n\n\n\n\ntsdiag(modelo)\n\n\n\n\n\n\n\n\n\n\n\n\nJonathan D. Cryer, Kung-Sik Chan. 2008. Time Series Analysis With Applications in R. Editado por Springer. 2º ed.\n\n\nPeña, Daniel. 2010. Análisis de Series Temporales. Editado por Alianza Editorial.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Series temporales</span>"
    ]
  },
  {
    "objectID": "referencias.html",
    "href": "referencias.html",
    "title": "Referencias",
    "section": "",
    "text": "Amat, Joaquin. 2017. “Clustering y Heatmaps.” 2017. https://github.com/JoaquinAmatRodrigo/Estadistica-con-R/blob/master/PDF_format/37_Clustering_y_Heatmaps.pdf.\n\n\nBakker, Jonathan D. 2024. “Applied Multivariate Statistics in\nr.” 2024. https://uw.pressbooks.pub/appliedmultivariatestatistics.\n\n\nCuadras, Carles M. 2014. Nuevos Métodos de Análisis\nMultivariante. https://gc.scalahed.com/recursos/files/r161r/w24899w/Semana5/METODOS_S5.pdf.\n\n\nHorton, Everitt P. 2011. An Introduction to Applied Multivariate\nAnalysis with r.\n\n\nJonathan D. Cryer, Kung-Sik Chan. 2008. Time Series Analysis with\nApplications in r. Edited by Springer. 2º ed.\n\n\nPeña, Daniel. 2010. Análisis de Series Temporales. Edited by\nAlianza Editorial.",
    "crumbs": [
      "Referencias"
    ]
  }
]